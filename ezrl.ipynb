{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EazyRL_v1.0.6 笔记\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 绪论\n",
    "> 9/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强化学习 RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 机器学习的一个分支\n",
    "- 研究智能主体在环境中如何采取行动以最大化累积奖励\n",
    "- 基本思想是通过智能体（Agent）与环境（Environment）的**交互**来**学习最优策略**，使得智能体能够采取一系列**动作**以**获取最大的长期奖励**\n",
    "- 目标是寻找最优策略\n",
    "\n",
    "![](./img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 产生 action 作用于 env\n",
    "\n",
    "env 根据 action 产生 reward 和 next_state\n",
    "\n",
    "agent 根据 reward 和 next_state 进行更新\n",
    "\n",
    "循环上述过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**与监督学习对比**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督学习：\n",
    "- 样本互不相关，是独立同分布的\n",
    "- 根据监督者给出的 label 确定正误\n",
    "- 通过反向传播更新模型参数，实现学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习：\n",
    "- 输入的是序列数据，数据之间有很强的关联\n",
    "- 不给出正确的动作，需要学习器不断尝试\n",
    "- 延迟奖励：奖励信号不是立即给出的，而是延迟一段时间\n",
    "- 动作会影响后面的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度强化学习：省去特征提取，实现端到端"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 序列决策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在与环境的交互过程中，智能体会获得很多观测\n",
    "\n",
    "每一个观测 - 采取一个动作 - 得到一个奖励\n",
    "\n",
    "**历史是观测 动作 奖励的序列**\n",
    "\n",
    "智能体采取**当前动作** 依赖于 **得到的历史**\n",
    "\n",
    "**状态是历史的函数**\n",
    "\n",
    "基本问题：**学习与规划**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体能够观察环境所有状态（上帝视角） - 完全可观测的环境 - 马尔可夫决策过程 MDP\n",
    "\n",
    "智能体只能看到部分的观测状态（有限信息） - 部分可观测的环境 - 部分可观测马尔可夫决策过程 POMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动作空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定环境中 有效动作的集合\n",
    "\n",
    "- 离散动作空间：动作数量有限，如围棋\n",
    "- 连续动作空间：动作是实值的向量，如智能驾驶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 智能体组成\n",
    "\n",
    "> 策略，价值函数，模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **策略**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入状态到动作的函数\n",
    "\n",
    "- 随机性策略：概率分布采样\n",
    "- 确定性策略：直接选择最有可能的动作（可能性最大的动作）\n",
    "\n",
    "通常采用随机性策略，避免出现局部最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **价值函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数值是对**未来奖励**的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**折扣因子**\n",
    "\n",
    "折扣因子γ（gamma） 反映了决策者对未来收益相对于当前收益的重视程度。\n",
    "\n",
    "折扣因子越大，表示决策者越看重未来的收益；\n",
    "\n",
    "折扣因子越小，表示决策者越倾向于即时满足，即更看重当前的收益。\n",
    "\n",
    "\n",
    "\n",
    "![](./img/2.png)\n",
    "\n",
    "pi策略 gamma折扣因子 r回报 s状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 函数**\n",
    "\n",
    "包含 状态 和 动作 两个变量，未来可以获得奖励的期望取决于当前的状态和当前的动作。\n",
    "\n",
    "![](./img/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 状态转移概率\n",
    "- 奖励函数：当前状态采取某个动作可以得到的奖励（价值函数是预测）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 智能体类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **基于价值 / 基于策略**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于价值：学习价值函数 （QL,Sarsa）\n",
    "- 基于策略：学习策略，通过状态给出动作概率 (PG)\n",
    "- 结合两者：演员（策略）- 评论员（价值）智能体\n",
    "\n",
    "基于价值迭代的方法只能应用在不连续的、离散的环境下，而基于策略迭代的方法可以应用在连续的环境中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **有模型 / 免模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是否需要对真实环境进行建模：\n",
    "\n",
    "- 有模型：学习状态转移来采取动作，如状态转移和奖励函数都是**已知**的，能知道某一状态采取某一决策后的奖励和状态（动规问题）\n",
    "- 免模型：学习价值函数和策略函数进行决策，没有环境转移函数和奖励函数。没有对真实环境进行建模，智能体只能在真实环境中通过一定的策略来执行动作，**等待**奖励和状态迁移，然后根据这些反馈信息来**更新**动作策略，这样反复迭代直到学习到最优策略。\n",
    "\n",
    "有模型同时在真实环境与虚拟世界学习\n",
    "\n",
    "免模型直接与真实环境交互来学习 一般需要大量数据样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索与利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单步强化学习任务 - K-臂赌博机\n",
    "\n",
    "- 仅探索：机会平均分配，最后计算期望\n",
    "- 仅利用：贪心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CartPole-v0 环境**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info, others = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态信息 observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.2427716, 3.7990518, 6.626667 , 4.7200627], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奖励 reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "游戏是否完成了 done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调试信息 info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`step()` 完成了一个完整的 S-A-R-S' 迭代过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看 gym 库中的环境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CartPole-v0',\n",
       " 'CartPole-v1',\n",
       " 'MountainCar-v0',\n",
       " 'MountainCarContinuous-v0',\n",
       " 'Pendulum-v1',\n",
       " 'Acrobot-v1',\n",
       " 'LunarLander-v2',\n",
       " 'LunarLanderContinuous-v2',\n",
       " 'BipedalWalker-v3',\n",
       " 'BipedalWalkerHardcore-v3',\n",
       " 'CarRacing-v2',\n",
       " 'Blackjack-v1',\n",
       " 'FrozenLake-v1',\n",
       " 'FrozenLake8x8-v1',\n",
       " 'CliffWalking-v0',\n",
       " 'Taxi-v3',\n",
       " 'Reacher-v2',\n",
       " 'Reacher-v4',\n",
       " 'Pusher-v2',\n",
       " 'Pusher-v4',\n",
       " 'InvertedPendulum-v2',\n",
       " 'InvertedPendulum-v4',\n",
       " 'InvertedDoublePendulum-v2',\n",
       " 'InvertedDoublePendulum-v4',\n",
       " 'HalfCheetah-v2',\n",
       " 'HalfCheetah-v3',\n",
       " 'HalfCheetah-v4',\n",
       " 'Hopper-v2',\n",
       " 'Hopper-v3',\n",
       " 'Hopper-v4',\n",
       " 'Swimmer-v2',\n",
       " 'Swimmer-v3',\n",
       " 'Swimmer-v4',\n",
       " 'Walker2d-v2',\n",
       " 'Walker2d-v3',\n",
       " 'Walker2d-v4',\n",
       " 'Ant-v2',\n",
       " 'Ant-v3',\n",
       " 'Ant-v4',\n",
       " 'Humanoid-v2',\n",
       " 'Humanoid-v3',\n",
       " 'Humanoid-v4',\n",
       " 'HumanoidStandup-v2',\n",
       " 'HumanoidStandup-v4']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym import envs\n",
    "\n",
    "specs = envs.registry.values() # 原书代码有误\n",
    "ids = [spec.id for spec in specs]\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 与 `gym` 库交互"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例：小车上山"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<MountainCarEnv<MountainCar-v0>>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观测空间：连续空间用 `gym.spaces.Box` 类 表示范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作空间：离散空间用 `gym.spaces.Discrete` 类 表示可取的数量，即动作数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现智能体，控制小车移动："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "    def decide(self, observation):\n",
    "        pos,vel = observation\n",
    "        lb = min( -0.09 * (pos + 0.25) ** 2 + 0.03,\n",
    "                0.3 * (pos + 0.9) ** 4 - 0.008 )\n",
    "        ub = -0.07 * (pos + 0.38) ** 2 + 0.07\n",
    "        if lb < vel < ub:\n",
    "            return  2\n",
    "        else: return 0\n",
    "    def learn(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SimpleAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与环境交互："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env,agent,render=False,train=False):\n",
    "    episode_reward = 0 # 回合总奖励\n",
    "    observation = env.reset() \n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.decide(observation)\n",
    "        next_obs,reward,done,_ = env.step(action) \n",
    "        episode_reward += reward\n",
    "        if train:\n",
    "            agent.learn(observation,action,reward,done)\n",
    "        if done:\n",
    "            break\n",
    "        observation = next_obs\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交互一回合：\n",
    "> !TODO notebook 中无法显示 gym 窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-105.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.seed(7) # 随机种子 v0.26 已经去掉了这个方法\n",
    "episode_reward= play(env,agent)\n",
    "episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估智能体的性能，需要计算出连续交互 100 回合的平均回合奖励："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-106.13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_rewards = [ play(env,agent) for _ in range(100) ]\n",
    "import numpy as np\n",
    "np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **总结**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('') # 取环境\n",
    "obs = env.reset()  # 初始化环境\n",
    "action = 1         # 随机动作\n",
    "next_obs,reward,done,info = env.step(action)   # 执行动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**习题**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 强化学习基本结构？**\n",
    "\n",
    "agent, environment, reward, value function, policy/action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 比监督学习训练过程困难的原因**\n",
    "\n",
    "延迟奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 基本特征**\n",
    "\n",
    "exploration, exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. 状态和观测？**\n",
    "\n",
    "状态：客观世界中事物的属性，是事物内部特征的集合，是全集\n",
    "\n",
    "观测：是状态的一种表现，是外界对状态的一种观察，是子集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. 智能体组成**\n",
    "\n",
    "policy, value function, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. 分类？**\n",
    "\n",
    "policy_based, value_based\n",
    "\n",
    "model_free, model_based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. 基于策略迭代和基于价值迭代的强化学习方法有什么区别**\n",
    "\n",
    "策略：通过策略给出概率，连续的环境\n",
    "\n",
    "价值：学习价值函数，离散的环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. 有模型学习和免模型学习有什么区别**\n",
    "\n",
    "有：奖励函数 / 价值函数 都是明确的，可以建立清晰的模型\n",
    "\n",
    "无：现实情况，无需模型。通过智能体与环境的交互来学习。需要较大的数据样本量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. 强化学习？**\n",
    "\n",
    "通过智能体与环境的不断交互，优化调整策略，以期达到最优奖励\n",
    "\n",
    "输入是一串序列，样本之间具有相关性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 马尔可夫决策过程\n",
    "\n",
    "> 9/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体与环境的交互过程 可以用 马尔可夫决策过程 表示，马尔可夫决策过程是强化学习的基本框架\n",
    "\n",
    "在马尔可夫决策过程中，它的环境是**全部可观测**的。\n",
    "\n",
    "但是很多时候环境里面有些量是不可观测的，但是这个部分观测的问题也可以转换成马尔可夫决策过程的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫过程 MP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **马尔可夫性质**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机过程 在某一时刻的状态 只与它前一个时刻的状态有关，而与它之前所有状态都无关的性质\n",
    "\n",
    "即 **未来的转移与过去的是独立的，无记忆性**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **马尔可夫链**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "马尔可夫过程：一组具有马尔可夫性质的随机变量序列\n",
    "\n",
    "马尔可夫链：离散时间的马尔可夫过程，状态是有限的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用状态转移矩阵来描述状态转移\n",
    "\n",
    "`a[i][j]` 表示 状态 `si` 到 状态 `sj` 的转移概率\n",
    "\n",
    "![](./img/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫奖励过程 MRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "马尔可夫链 + 奖励函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **回报和价值函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回报 return** 时刻t的回报是当前获得的所有奖励的和 (单个轨迹)\n",
    "\n",
    "**折扣回报 discounted return** 具有折扣因子的回报函数，折扣因子作为超参数，可以调整出不同偏好（长短期收益）下的 agent\n",
    "\n",
    "**价值函数** 状态价值函数：在某状态下，agent能获得的回报期望 v(pi,s) （所有轨迹计算后的期望）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **贝尔曼方程**\n",
    "\n",
    "> [推导](https://www.bilibili.com/video/BV1sd4y167NS?p=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单地说就是： 状态价值 = 即时奖励 + 未来奖励的折扣和， state value = immediate reward + sum( discount factor * next state value )\n",
    "\n",
    "![](./img/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展开来看，对于指定的策略，每个状态下的价值函数：\n",
    "\n",
    "![](./img/bellmaneq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多个状态的价值函数联立，就能得到状态转移概率矩阵：\n",
    "\n",
    "![](./img/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **计算状态价值**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学的方法需要求矩阵的逆，对于大型矩阵比较困难。可以通过迭代方法来求解。\n",
    "\n",
    "- 动态规划 bootstrapping方法\n",
    "- 蒙特卡洛\n",
    "- 时序差分学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫决策过程 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在奖励过程上添加了决策条件\n",
    "\n",
    "![](./img/7.png)\n",
    "\n",
    "在马尔可夫决策过程中，动作是由智能体决定的，智能体会采取动作来决定未来的状态转移"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q 函数 （动作价值函数）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即添加了 决策条件 后的状态价值函数，通过某一状态下采取某一动作，得出回报的期望\n",
    "\n",
    "![](./img/8.png)\n",
    "\n",
    "该状态下 每个决策的Q函数值 的加权平均就是 该状态的状态价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于 Q 函数的贝尔曼方程同理，只是增加了策略条件。整理得到的就是Q函数的**贝尔曼期望方程**：\n",
    "\n",
    "![](./img/9.png)\n",
    "\n",
    "即： 当前价值 = 期望(即时奖励 + 折扣 * 未来价值)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于某一个状态，它的当前价值是与它的未来价值线性相关的\n",
    "\n",
    "当前价值 = 即时奖励 + 折扣因子 * 未来价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **预测和控制**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测 就是评估决策的价值\n",
    "\n",
    "控制 就是搜索最优决策\n",
    "\n",
    "预测和控制是马尔可夫决策过程的两个重要方面\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过策略迭代和价值迭代来解决马尔可夫决策过程的控制问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **策略迭代**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包括：\n",
    "\n",
    "- **策略评估** 根据当前策略估算价值函数\n",
    "- **策略改进** 得到价值函数后 推算其Q函数，做贪心搜索\n",
    "\n",
    "两个步骤迭代进行，直到收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **价值迭代**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多次迭代贝尔曼最优方程，直到价值函数收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代算法过程：\n",
    "1. 初始化，所有状态的价值都设0\n",
    "2. 重复以下操作，直到所有状态的价值不再变化\n",
    "   - 计算每个状态下每个动作的Q值\n",
    "   - 利用Q值更新状态价值和动作条件（选取最大项）\n",
    "3. 收敛后，提取最优策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 折扣因子作用**\n",
    "\n",
    "超参数 调节智能体远视/近视 视野 避免无穷奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 解析解难求得**\n",
    "\n",
    "需要计算状态转移概率矩阵的逆 大型矩阵的逆很难求得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 贝尔曼方程求解**\n",
    "\n",
    "蒙特卡洛 动态规划 时序差分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 奖励过程与决策过程的区别**\n",
    "\n",
    "奖励过程只根据奖励信号来调整行为，而决策过程通过 agent 的介入具有了决策的自主性，agent 可以根据环境的变化做出相应的调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. 寻找最优策略？**\n",
    "\n",
    "策略迭代 / 价值迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 表格型方法\n",
    "\n",
    "> 9/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是查表\n",
    "\n",
    "- 蒙特卡洛\n",
    "- Q-learning\n",
    "- Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态、动作、状态转移概率和奖励 `(S、A、P 、R)`，这 4 个合集就构成了强化学习马尔可夫决策过程的四元组，后面也可能会再加上折扣因子构成五元组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习用价值函数 `V (S)` 来表示状态是好的还是坏的，\n",
    "\n",
    "用 Q 函数来判断在什么状态下采取什么动作能够取得最大奖励，即用Q 函数来表示状态-动作值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 免模型预测\n",
    "\n",
    "在无法获取环境模型的情况下，通过 **蒙特卡洛方法** 和 **时序差分方法** 估计某个策略的价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **蒙特卡洛策略评估**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采样大量轨迹、计算所有轨迹的真实回报、计算平均值\n",
    "\n",
    "局限性：只能用于 **有终止** 的马尔可夫决策过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**增量式蒙特卡洛方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获得 新的轨迹 后\n",
    "\n",
    "将原本蒙特卡洛的经验均值转换成增量均值\n",
    "\n",
    "用上一时刻的值和当前时刻的增量 更新现在的值\n",
    "\n",
    "![](./img/10.png)\n",
    "\n",
    "- $\\alpha$ 是一个超参数，用来控制更新速度\n",
    "- $G_t$ 代表当前时刻的回报，如 $G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma ^2 r_{t+3} + ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 增量式蒙特卡洛方法只需要一条轨迹就能更新轨迹上的所有状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **时序差分**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态价值迭代：下一个状态会影响上一个状态（例子：巴普洛夫的狗、多级条件反射）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 免模型的\n",
    "- 可以从不完整的回合中学习\n",
    "- 结合了自举的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于给定的策略 $\\pi$ ,一步一步地计算价值函数 $v_{\\pi}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一步时序差分 TD(0)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每往前走一步，就做一步自举，更新上一步的值：\n",
    "\n",
    "![](./img/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 免模型控制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\epsilon -greedy$ 探索**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有 $1-\\epsilon$ 概率按照Q函数来决定动作（取最大值）\n",
    "\n",
    "一般取 $\\epsilon = 0.1$ 在实现上会让 $\\epsilon$ 随时间递减\n",
    "\n",
    "> 最初的时候不知道那个动作较好，引入随机性实现探索；随着探索次数增加，随机性减小，就减少探索，选择最优动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sarsa 同策略时序差分控制**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用时序差分方法 估计 Q函数 (把时序的 V 改成 Q函数)\n",
    "\n",
    "![](./img/12.png)\n",
    "\n",
    "State Action Reward State' Action' SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码实现：\n",
    "- 根据 Q 表格选择动作 （$\\epsilon-greedy$ 策略）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_action(action_space, epsilon, pi, Q, s):\n",
    "    if np.random.rand() < epsilon: # explore\n",
    "        return np.random.choice(action_space,p=pi)\n",
    "    else : #  exploit\n",
    "        return np.nanargmax( Q[s,:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 获取 $s_t,a_t,r_{t+1},s_{t+1},a_{t+1}$ 更新 Q 表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(s,a,r,s_next,a_next,alpha,gamma,Q):\n",
    "    if s_next == 'terminal': # 结束状态\n",
    "        Q[s,a] = Q[s,a] + alpha * (r-Q[s,a])\n",
    "    else :\n",
    "        Q[s,a] = Q[s,a] + alpha * (r + gamma*Q[s_next,a_next]-Q[s,a]) # 时序差分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarsa 优化的是其实际执行的策略，而 Q-learning 优化的是其估计的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q学习 异策略时序差分控制**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 学习的下一个动作都是通过 argmax 函数来选择的\n",
    "\n",
    "![](./img/13.png)\n",
    "\n",
    "二者的更新公式是一样的，区别在于 Q 学习采取的动作是 argmax 选择的动作，Q 学习只需要接收 当前状态 s 和 动作 a 的 Q 值，然后根据 Q 值选择动作即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(s,a,r,s_next,gamma,alpha,Q):\n",
    "    Q[s,a] = Q[s,a] + alpha*(r + gamma*Q[s_next, np.nanargmax(Q[s_next,:]) ] - Q[s,a]) # 不考虑终点状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 学习不需要知道 a_next，其探索更加大胆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 学习解决悬崖寻路问题\n",
    "\n",
    "> 9/11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**环境：`CliffWalking-v0`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 每走一步 收到 -1 奖励\n",
    "- 掉入悬崖 返回起点并收到 -100 奖励\n",
    "- 超出边界 不会移动 但会收到 -1 奖励\n",
    "- 目标是以最少的步数到达终点\n",
    "\n",
    "![](./img/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化环境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import turtle\n",
    "import numpy as np\n",
    "\n",
    "# turtle tutorial : https://docs.python.org/3.3/library/turtle.html\n",
    "\n",
    "class CliffWalkingWapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.t = None\n",
    "        self.unit = 50\n",
    "        self.max_x = 12\n",
    "        self.max_y = 4\n",
    "\n",
    "    def draw_x_line(self, y, x0, x1, color='gray'):\n",
    "        assert x1 > x0\n",
    "        self.t.color(color)\n",
    "        self.t.setheading(0)\n",
    "        self.t.up()\n",
    "        self.t.goto(x0, y)\n",
    "        self.t.down()\n",
    "        self.t.forward(x1 - x0)\n",
    "\n",
    "    def draw_y_line(self, x, y0, y1, color='gray'):\n",
    "        assert y1 > y0\n",
    "        self.t.color(color)\n",
    "        self.t.setheading(90)\n",
    "        self.t.up()\n",
    "        self.t.goto(x, y0)\n",
    "        self.t.down()\n",
    "        self.t.forward(y1 - y0)\n",
    "\n",
    "    def draw_box(self, x, y, fillcolor='', line_color='gray'):\n",
    "        self.t.up()\n",
    "        self.t.goto(x * self.unit, y * self.unit)\n",
    "        self.t.color(line_color)\n",
    "        self.t.fillcolor(fillcolor)\n",
    "        self.t.setheading(90)\n",
    "        self.t.down()\n",
    "        self.t.begin_fill()\n",
    "        for i in range(4):\n",
    "            self.t.forward(self.unit)\n",
    "            self.t.right(90)\n",
    "        self.t.end_fill()\n",
    "\n",
    "    def move_player(self, x, y):\n",
    "        self.t.up()\n",
    "        self.t.setheading(90)\n",
    "        self.t.fillcolor('red')\n",
    "        self.t.goto((x + 0.5) * self.unit, (y + 0.5) * self.unit)\n",
    "\n",
    "    def render(self):\n",
    "        if self.t == None:\n",
    "            self.t = turtle.Turtle()\n",
    "            self.wn = turtle.Screen()\n",
    "            self.wn.setup(self.unit * self.max_x + 100,\n",
    "                          self.unit * self.max_y + 100)\n",
    "            self.wn.setworldcoordinates(0, 0, self.unit * self.max_x,\n",
    "                                        self.unit * self.max_y)\n",
    "            self.t.shape('circle')\n",
    "            self.t.width(2)\n",
    "            self.t.speed(0)\n",
    "            self.t.color('gray')\n",
    "            for _ in range(2):\n",
    "                self.t.forward(self.max_x * self.unit)\n",
    "                self.t.left(90)\n",
    "                self.t.forward(self.max_y * self.unit)\n",
    "                self.t.left(90)\n",
    "            for i in range(1, self.max_y):\n",
    "                self.draw_x_line(\n",
    "                    y=i * self.unit, x0=0, x1=self.max_x * self.unit)\n",
    "            for i in range(1, self.max_x):\n",
    "                self.draw_y_line(\n",
    "                    x=i * self.unit, y0=0, y1=self.max_y * self.unit)\n",
    "\n",
    "            for i in range(1, self.max_x - 1):\n",
    "                self.draw_box(i, 0, 'black')\n",
    "            self.draw_box(self.max_x - 1, 0, 'yellow')\n",
    "            self.t.shape('turtle')\n",
    "\n",
    "        x_pos = self.s % self.max_x\n",
    "        y_pos = self.max_y - 1 - int(self.s / self.max_x)\n",
    "        self.move_player(x_pos, y_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 4)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env_name = 'CliffWalking-v0'\n",
    "env = gym.make(env_name)\n",
    "env = CliffWalkingWapper(env)\n",
    "env.seed(1)\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "num_states, num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "a = env.action_space.sample()\n",
    "\n",
    "s,a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "环境初始化状态："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **强化学习基本过程**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 初始化 agent env\n",
    "2. agent.get_action(state)\n",
    "3. 执行 action , 返回 reward, new_state, done\n",
    "4. 学习，即进行策略更新\n",
    "5. 循环 2-4 直到算法收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**如何确定算法收敛：滑动平均奖励**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "滑动平均量 反映了奖励变化的趋势："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.9500000000000001,\n",
       " 0.8883333343267442,\n",
       " 0.8245000008940698,\n",
       " 0.7620500011026861,\n",
       " 0.7025116681557896,\n",
       " 0.6465462162645461,\n",
       " 0.5943915946380914,\n",
       " 0.5460635463681776,\n",
       " 0.5014571918803714,\n",
       " 0.4604023820541736,\n",
       " 0.4226954774304423,\n",
       " 0.3881182376662666,\n",
       " 0.35644927136180765,\n",
       " 0.3274710112399873,\n",
       " 0.30097391011598856,\n",
       " 0.2767588720674796,\n",
       " 0.2546385404576793,\n",
       " 0.23443784434586182,\n",
       " 0.21599405998578144,\n",
       " 0.19915655883780545,\n",
       " 0.18378635763494455,\n",
       " 0.16975554803939122,\n",
       " 0.15694666002629512,\n",
       " 0.14525199393425864,\n",
       " 0.13457294853026702,\n",
       " 0.12481935740853876,\n",
       " 0.11590885039876876,\n",
       " 0.10776624120811502,\n",
       " 0.10032295059448373,\n",
       " 0.09351646189051174,\n",
       " 0.08728981570146058,\n",
       " 0.08159113725192763,\n",
       " 0.07637320000827984,\n",
       " 0.07159302288056024,\n",
       " 0.06721149839097806,\n",
       " 0.06319305132002725,\n",
       " 0.05950532515499974,\n",
       " 0.05611889523703443,\n",
       " 0.05300700575058389,\n",
       " 0.05014532947490866,\n",
       " 0.047511748952718875,\n",
       " 0.04508615544413236,\n",
       " 0.042850267240178944,\n",
       " 0.04078746279219302,\n",
       " 0.03888262959694427,\n",
       " 0.037122026144345656,\n",
       " 0.0354931569253326,\n",
       " 0.03398465751751547,\n",
       " 0.03258619172106044,\n",
       " 0.031288356932072554,\n",
       " 0.03008259823358242,\n",
       " 0.028961130891169765,\n",
       " 0.027916869667702012,\n",
       " 0.026943364461540963,\n",
       " 0.026034742380928802,\n",
       " 0.025185654120819398,\n",
       " 0.024391226633349028,\n",
       " 0.023647019227408438,\n",
       " 0.022948984058257703,\n",
       " 0.022293429823121515,\n",
       " 0.021676990018547556,\n",
       " 0.021096592695648356,\n",
       " 0.02054943342608352,\n",
       " 0.020032951623369512,\n",
       " 0.01954480802133911,\n",
       " 0.019082864497887195,\n",
       " 0.018645166288870963,\n",
       " 0.018229925049297567,\n",
       " 0.017835503980922002,\n",
       " 0.017460404260820715,\n",
       " 0.017103252733975564,\n",
       " 0.01676279046407036,\n",
       " 0.01643786280173682,\n",
       " 0.01612740988718232,\n",
       " 0.015830458381951695,\n",
       " 0.015546113841248314,\n",
       " 0.01527355375589081,\n",
       " 0.01501202119694187,\n",
       " 0.014760819095874135,\n",
       " 0.014519305096719538,\n",
       " 0.014286886736739168,\n",
       " 0.014063017295290671,\n",
       " 0.013847191778412142,\n",
       " 0.013638943211815368,\n",
       " 0.013437839583976516,\n",
       " 0.013243480908653245,\n",
       " 0.013055496488017833,\n",
       " 0.012873542356344756,\n",
       " 0.012697299258726267,\n",
       " 0.012526470459387387,\n",
       " 0.012360779955433922,\n",
       " 0.01219997077609341,\n",
       " 0.012043803452031975,\n",
       " 0.011892054730871765,\n",
       " 0.011744515955495341,\n",
       " 0.011600992163313185,\n",
       " 0.011461301089339931,\n",
       " 0.011325271989566219,\n",
       " 0.011192744768257856,\n",
       " 0.011063569285657333,\n",
       " 0.010937604548650677,\n",
       " 0.010814717889235345,\n",
       " 0.01069478459767037,\n",
       " 0.010577687126650215,\n",
       " 0.010463314654457987,\n",
       " 0.010351562598671097,\n",
       " 0.0102423322716286,\n",
       " 0.010135530197822769,\n",
       " 0.010031068058345067,\n",
       " 0.009928862175226224,\n",
       " 0.00982883314047457,\n",
       " 0.009730905575342785,\n",
       " 0.009635008006800245,\n",
       " 0.009541072383829084,\n",
       " 0.00944903410775196,\n",
       " 0.009358831593865736,\n",
       " 0.009270406063176318,\n",
       " 0.009183701634356788,\n",
       " 0.009098664847716164,\n",
       " 0.00901524460930032,\n",
       " 0.00893339223371508,\n",
       " 0.008853061110137962,\n",
       " 0.008774206587993261,\n",
       " 0.008696785967191896,\n",
       " 0.008620758209950483,\n",
       " 0.008546083960825285,\n",
       " 0.008472725564742756,\n",
       " 0.008400646803830271,\n",
       " 0.008329812893394416,\n",
       " 0.008260190385169196,\n",
       " 0.008191747126805551,\n",
       " 0.008124452132279665,\n",
       " 0.008058275558392695,\n",
       " 0.007993188730186665,\n",
       " 0.007929163977554242,\n",
       " 0.007866174581319808,\n",
       " 0.007804194817844676,\n",
       " 0.0077431998129072616,\n",
       " 0.007683165549893633,\n",
       " 0.007624068830602874,\n",
       " 0.007565887286538044,\n",
       " 0.007508599249369645,\n",
       " 0.007452183774051139,\n",
       " 0.007396620566490653,\n",
       " 0.007341890011587766,\n",
       " 0.007287973105334366,\n",
       " 0.007234851486837677,\n",
       " 0.007182507280876469,\n",
       " 0.007130923235598414,\n",
       " 0.0070800825670509415,\n",
       " 0.007029969052089651,\n",
       " 0.00698056692573136,\n",
       " 0.006931860881904119,\n",
       " 0.006883836074122209,\n",
       " 0.006836478116093652,\n",
       " 0.0067897729891347085,\n",
       " 0.006743707098541307,\n",
       " 0.006698267184624952,\n",
       " 0.006653440475475683,\n",
       " 0.0066092144519197,\n",
       " 0.006565576961944137,\n",
       " 0.006522516183980973,\n",
       " 0.006480020640428666,\n",
       " 0.006438079163255516,\n",
       " 0.006396680863042674,\n",
       " 0.006355815194255115,\n",
       " 0.006315471781154872,\n",
       " 0.006275640587085844,\n",
       " 0.00623631183399948,\n",
       " 0.006197475976594024,\n",
       " 0.006159123725605965,\n",
       " 0.006121246022206211,\n",
       " 0.00608383406152278,\n",
       " 0.006046879220678953,\n",
       " 0.006010373133726013,\n",
       " 0.005974307572818183,\n",
       " 0.005938674574100718,\n",
       " 0.005903466318178546,\n",
       " 0.005868675255368684,\n",
       " 0.005834293939030648,\n",
       " 0.005800315108394457,\n",
       " 0.0057667316699735825,\n",
       " 0.00573353677396886,\n",
       " 0.005700723650201373,\n",
       " 0.0056682856932826755,\n",
       " 0.0056362164927018805,\n",
       " 0.005604509720205646,\n",
       " 0.005573159261270804,\n",
       " 0.005542159147165217,\n",
       " 0.005511503447236417,\n",
       " 0.005481186451368152,\n",
       " 0.005451202508228379,\n",
       " 0.00542154615908923,\n",
       " 0.005392212072000463,\n",
       " 0.005363194935979449,\n",
       " 0.005334489645178917,\n",
       " 0.005306091185241165,\n",
       " 0.005277994624149104,\n",
       " 0.005250195150558324,\n",
       " 0.005222688061729823,\n",
       " 0.005195468752669473,\n",
       " 0.005168532706353526,\n",
       " 0.005141875531497712,\n",
       " 0.005115492856851025,\n",
       " 0.00508938046889079,\n",
       " 0.005063534202917567,\n",
       " 0.005037950031305091,\n",
       " 0.005012623910094561,\n",
       " 0.004987552013458546,\n",
       " 0.004962730479057196,\n",
       " 0.004938155551387873,\n",
       " 0.004913823571101434,\n",
       " 0.004889730918820745,\n",
       " 0.004865874104275745,\n",
       " 0.004842249660260477,\n",
       " 0.004818854186892807,\n",
       " 0.00479568434488204,\n",
       " 0.004772736896035912,\n",
       " 0.004750008646584609,\n",
       " 0.004727496489304654,\n",
       " 0.004705197301732021,\n",
       " 0.004683108087384567,\n",
       " 0.004661225870031593,\n",
       " 0.004639547738234828,\n",
       " 0.004618070838869182,\n",
       " 0.00459679237129143,\n",
       " 0.004575709628658156,\n",
       " 0.004554819895413512,\n",
       " 0.004534120494726593,\n",
       " 0.004513608877751197,\n",
       " 0.0044932824711289695,\n",
       " 0.004473138772572878,\n",
       " 0.004453175343760075,\n",
       " 0.004433389710803231,\n",
       " 0.004413779554071486,\n",
       " 0.004394342506500298,\n",
       " 0.004375076344599319,\n",
       " 0.004355978741264336,\n",
       " 0.004337047555535429,\n",
       " 0.004318280581151897,\n",
       " 0.004299675646214593,\n",
       " 0.00428123070288203,\n",
       " 0.004262943675266222,\n",
       " 0.0042448125553696,\n",
       " 0.004226835349729834,\n",
       " 0.004209010124532442,\n",
       " 0.004191334906513745,\n",
       " 0.0041738078266041765,\n",
       " 0.004156427062942739,\n",
       " 0.0041391907430238925,\n",
       " 0.004122097088460392,\n",
       " 0.004105144312439203,\n",
       " 0.004088330667130208,\n",
       " 0.004071654486354043,\n",
       " 0.004055114037718639,\n",
       " 0.0040387076922219385,\n",
       " 0.00402243382078064,\n",
       " 0.0040062908175213865,\n",
       " 0.003990277120742834,\n",
       " 0.003974391161932322,\n",
       " 0.0039586314362961995,\n",
       " 0.003942996426538731,\n",
       " 0.003927484673961495,\n",
       " 0.003912094702754463,\n",
       " 0.003896825091556351,\n",
       " 0.003881674421443618,\n",
       " 0.003866641298969755,\n",
       " 0.0038517243769006456,\n",
       " 0.0038369223030272,\n",
       " 0.00382223376722898,\n",
       " 0.0038076574506992035,\n",
       " 0.0037931920733795107,\n",
       " 0.0037788363668020556,\n",
       " 0.003764589096213519,\n",
       " 0.0037504490339205915,\n",
       " 0.003736414958583974,\n",
       " 0.003722485701149103,\n",
       " 0.003708660069768486,\n",
       " 0.003694936921930186,\n",
       " 0.003681315112958138,\n",
       " 0.0036677935195116268,\n",
       " 0.0036543710607348148,\n",
       " 0.003641046624159062,\n",
       " 0.0036278191619964636,\n",
       " 0.0036146875915395197,\n",
       " 0.003601650891785352,\n",
       " 0.003588708027416046,\n",
       " 0.0035758579967962883,\n",
       " 0.0035630997820389736,\n",
       " 0.0035504324204795783,\n",
       " 0.0035378549293047096,\n",
       " 0.0035253663541331207,\n",
       " 0.003512965766172497,\n",
       " 0.003500652236377497,\n",
       " 0.0034884248587581214,\n",
       " 0.003476282701508048,\n",
       " 0.0034642249027185225,\n",
       " 0.003452250570256345,\n",
       " 0.0034403588546355066,\n",
       " 0.0034285488749032766,\n",
       " 0.0034168198149191337,\n",
       " 0.003405170847024351,\n",
       " 0.0033936011331938175,\n",
       " 0.003382109872638804,\n",
       " 0.003370696274800261,\n",
       " 0.0033593595350427867,\n",
       " 0.0033480989059114555,\n",
       " 0.003336913621564576,\n",
       " 0.003325802899612369,\n",
       " 0.0033147660126209696,\n",
       " 0.003303802236050705,\n",
       " 0.0032929108246836122,\n",
       " 0.0032820910845404614,\n",
       " 0.003271342297907688,\n",
       " 0.003260663772276954,\n",
       " 0.003250054814541563,\n",
       " 0.003239514731056295,\n",
       " 0.0032290428742573875,\n",
       " 0.003218638591488262,\n",
       " 0.0032083012022257385,\n",
       " 0.003198030093998957,\n",
       " 0.003187824599168821,\n",
       " 0.003177684116860143,\n",
       " 0.0031676080131529975,\n",
       " 0.003157595670953322,\n",
       " 0.0031476465115933975,\n",
       " 0.0031377598978569532,\n",
       " 0.003127935279532411,\n",
       " 0.0031181720450140287,\n",
       " 0.0031084696438566373,\n",
       " 0.003098827487527328,\n",
       " 0.0030892450463464835,\n",
       " 0.0030797217504701896,\n",
       " 0.003070257040472795,\n",
       " 0.00306085038958815,\n",
       " 0.0030515012538775256,\n",
       " 0.003042209120513003,\n",
       " 0.00303297345810026,\n",
       " 0.0030237937651013437,\n",
       " 0.0030146695202829038,\n",
       " 0.0030056002312518594,\n",
       " 0.0029965854082883094,\n",
       " 0.00298762454079515,\n",
       " 0.0029787171692349872,\n",
       " 0.00296986278689191,\n",
       " 0.002961060937721597,\n",
       " 0.002952311164718032,\n",
       " 0.0029436130100096705,\n",
       " 0.002934965991662929,\n",
       " 0.002926369675937584,\n",
       " 0.0029178236259013034,\n",
       " 0.002909327378465797,\n",
       " 0.002900880516851603,\n",
       " 0.002892482596108013,\n",
       " 0.002884133215779388,\n",
       " 0.0028758319456064618,\n",
       " 0.0028675783517897653,\n",
       " 0.0028593720439097753,\n",
       " 0.002851212624022794,\n",
       " 0.0028430996641302993,\n",
       " 0.0028350328023166857,\n",
       " 0.0028270116197256505,\n",
       " 0.002819035739386522,\n",
       " 0.002811104775459567,\n",
       " 0.0028032183341228957,\n",
       " 0.0027953760609368086,\n",
       " 0.002787577590339446,\n",
       " 0.0027798225467590074,\n",
       " 0.002772110568897806,\n",
       " 0.002764441285021796,\n",
       " 0.002756814360570336,\n",
       " 0.0027492294478727204,\n",
       " 0.002741686187459185,\n",
       " 0.002734184232523877,\n",
       " 0.002726723247658466,\n",
       " 0.0027193029077123895,\n",
       " 0.002711922873484012,\n",
       " 0.0027045828396918626,\n",
       " 0.0026972824617334233,\n",
       " 0.002690021406183364,\n",
       " 0.0026827993729588886,\n",
       " 0.0026756160454193506,\n",
       " 0.0026684711153051035,\n",
       " 0.0026613642586163382,\n",
       " 0.0026542951837532254,\n",
       " 0.0026472636049928544,\n",
       " 0.0026402691953354134,\n",
       " 0.0026333116837641344,\n",
       " 0.0026263907797977994,\n",
       " 0.0026195061754371823,\n",
       " 0.00261265759348298,\n",
       " 0.002605844761172756,\n",
       " 0.0025990673864541864,\n",
       " 0.0025923252064801827,\n",
       " 0.0025856179381222345,\n",
       " 0.0025789453032816186,\n",
       " 0.0025723070516694845,\n",
       " 0.0025657029114597375,\n",
       " 0.0025591326147258284,\n",
       " 0.0025525959203338625,\n",
       " 0.002546092541414142,\n",
       " 0.0025396222430670023,\n",
       " 0.0025331847673166178,\n",
       " 0.002526779881984745,\n",
       " 0.002520407308261771,\n",
       " 0.002514066818502109,\n",
       " 0.0025077581845416672,\n",
       " 0.0025014811544666333,\n",
       " 0.002495235478271512,\n",
       " 0.002489020930951358,\n",
       " 0.0024828372867186557,\n",
       " 0.0024766843190815765,\n",
       " 0.002470561777631347,\n",
       " 0.0024644694602831345,\n",
       " 0.002458407138594461,\n",
       " 0.0024523745836840323,\n",
       " 0.002446371566275619,\n",
       " 0.0024403979033036792,\n",
       " 0.002434453360160032,\n",
       " 0.002428537725697314,\n",
       " 0.002422650786599835,\n",
       " 0.002416792327600408,\n",
       " 0.0024109621549585655,\n",
       " 0.0024051600477417584,\n",
       " 0.0023993858303937563,\n",
       " 0.0023936392750647587,\n",
       " 0.00238792019997301,\n",
       " 0.0023822284182318655,\n",
       " 0.002376563715077216,\n",
       " 0.002370925897221364,\n",
       " 0.0023653147907053804,\n",
       " 0.002359730192400102,\n",
       " 0.0023541719194892802,\n",
       " 0.002348639784155228,\n",
       " 0.0023431335940789625,\n",
       " 0.002337653176173399,\n",
       " 0.002332198351377097,\n",
       " 0.0023267689585347597,\n",
       " 0.0023213647827574277,\n",
       " 0.002315985677211165,\n",
       " 0.002310631463179302,\n",
       " 0.00230530195653377,\n",
       " 0.0022999969915593094,\n",
       " 0.0022947163958291195,\n",
       " 0.0022894600141590816,\n",
       " 0.00228422766031736,\n",
       " 0.0022790191899783657,\n",
       " 0.0022738344266829413,\n",
       " 0.002268673211617844,\n",
       " 0.0022635353785680924,\n",
       " 0.002258420777940201,\n",
       " 0.002253329251816971,\n",
       " 0.002248260634789857,\n",
       " 0.002243214801274234,\n",
       " 0.002238191568394745,\n",
       " 0.0022331907948466346,\n",
       " 0.0022282123301725575,\n",
       " 0.002223256015677838,\n",
       " 0.0022183217085372706,\n",
       " 0.0022134092802081674,\n",
       " 0.002208518568435982,\n",
       " 0.0022036494271916226,\n",
       " 0.0021988017250489066,\n",
       " 0.0021939753204412047,\n",
       " 0.002189170062675487,\n",
       " 0.0021843858161280255,\n",
       " 0.0021796224587374655,\n",
       " 0.00217487983408246,\n",
       " 0.0021701578013837955,\n",
       " 0.0021654562572232445,\n",
       " 0.002160775038675209,\n",
       " 0.002156114022389716,\n",
       " 0.0021514730740687248,\n",
       " 0.0021468520728439985,\n",
       " 0.002142250909934124,\n",
       " 0.002137669417586637,\n",
       " 0.0021331074913904483,\n",
       " 0.002128565014092512,\n",
       " 0.0021240418568820242,\n",
       " 0.0021195378805461958,\n",
       " 0.0021150529830765816,\n",
       " 0.0021105870260994517,\n",
       " 0.0021061398850784497,\n",
       " 0.0021017114712139186,\n",
       " 0.0020973016347369746,\n",
       " 0.0020929102874217504,\n",
       " 0.0020885372800157733,\n",
       " 0.0020841825247579454,\n",
       " 0.0020798458960971508,\n",
       " 0.0020755273043204492,\n",
       " 0.0020712265988370015,\n",
       " 0.0020669436906811806,\n",
       " 0.002062678476500858,\n",
       " 0.002058430839996054,\n",
       " 0.0020542006532137226,\n",
       " 0.002049987824279186,\n",
       " 0.0020457922472221705,\n",
       " 0.0020416138266694294,\n",
       " 0.002037452453501977,\n",
       " 0.0020333080062296606,\n",
       " 0.0020291803987944083,\n",
       " 0.0020250695071778258,\n",
       " 0.0020209752663294875,\n",
       " 0.002016897547854817,\n",
       " 0.002012836259481761,\n",
       " 0.0020087912948824043,\n",
       " 0.0020047625583616262,\n",
       " 0.002000749963793817,\n",
       " 0.0019967534103828635,\n",
       " 0.0019927727841291994,\n",
       " 0.0019888080057162793,\n",
       " 0.0019848589804761486,\n",
       " 0.001980925611566115,\n",
       " 0.001977007811827757,\n",
       " 0.001973105479535429,\n",
       " 0.0019692185347775833,\n",
       " 0.00196534687070923,\n",
       " 0.001961490414452943,\n",
       " 0.0019576490654944416,\n",
       " 0.0019538227333708736,\n",
       " 0.001950011336665672,\n",
       " 0.0019462147904623945,\n",
       " 0.0019424330066947102,\n",
       " 0.0019386658944613897,\n",
       " 0.0019349133719513264,\n",
       " 0.0019311753538929922,\n",
       " 0.0019274517635420117,\n",
       " 0.0019237425085453824,\n",
       " 0.001920047505785403,\n",
       " 0.001916366668814609,\n",
       " 0.0019126999314718154,\n",
       " 0.0019090471992873586,\n",
       " 0.0019054083988800739,\n",
       " 0.0019017834525653806,\n",
       " 0.0018981722671440916,\n",
       " 0.0018945747820200037,\n",
       " 0.0018909909077319362,\n",
       " 0.00188742057540636,\n",
       " 0.0018838636997740336,\n",
       " 0.001880320215734172,\n",
       " 0.0018767900414130047,\n",
       " 0.0018732731031241377,\n",
       " 0.0018697693229082846,\n",
       " 0.0018662786306020878,\n",
       " 0.0018628009514169927,\n",
       " 0.0018593362180433007,\n",
       " 0.0018558843466192185,\n",
       " 0.0018524452733106641,\n",
       " 0.0018490189290254321,\n",
       " 0.0018456052399390048,\n",
       " 0.0018422041396093166,\n",
       " 0.0018388155565969761,\n",
       " 0.0018354394149649993,\n",
       " 0.0018320756580116322,\n",
       " 0.0018287242114222323,\n",
       " 0.0018253850083136207,\n",
       " 0.0018220579768494054,\n",
       " 0.0018187430523768323,\n",
       " 0.0018154401767084233,\n",
       " 0.0018121492741923837,\n",
       " 0.0018088702883836307,\n",
       " 0.0018056031568400572,\n",
       " 0.0018023478000807028,\n",
       " 0.001799104169855767,\n",
       " 0.0017958721994573656,\n",
       " 0.00179265181720687,\n",
       " 0.0017894429702350473,\n",
       " 0.0017862455993180551,\n",
       " 0.0017830596395129036,\n",
       " 0.0017798850207293695,\n",
       " 0.001776721691527784,\n",
       " 0.0017735695939709128,\n",
       " 0.0017704286642737134,\n",
       " 0.001767298845029647,\n",
       " 0.001764180072931297,\n",
       " 0.0017610722910019882,\n",
       " 0.0017579754479627127,\n",
       " 0.0017548894743792994,\n",
       " 0.0017518143194025265,\n",
       " 0.0017487499256265617,\n",
       " 0.0017456962413861564,\n",
       " 0.0017426532085410545,\n",
       " 0.0017396207631234935,\n",
       " 0.0017365988592040191,\n",
       " 0.0017335874321630581,\n",
       " 0.0017305864354845272,\n",
       " 0.0017275958156624189,\n",
       " 0.001724615524541326,\n",
       " 0.0017216454954983183,\n",
       " 0.0017186856802142907,\n",
       " 0.0017157360352020486,\n",
       " 0.0017127964980400523,\n",
       " 0.0017098670125489164,\n",
       " 0.001706947539808726,\n",
       " 0.001704038021508493,\n",
       " 0.001701138405168329,\n",
       " 0.0016982486435563335,\n",
       " 0.0016953686825222644,\n",
       " 0.0016924984849724359,\n",
       " 0.001689637982597934,\n",
       " 0.0016867871372038008,\n",
       " 0.0016839459027730417,\n",
       " 0.0016811142262488298,\n",
       " 0.0016782920598800207,\n",
       " 0.001675479360690584,\n",
       " 0.0016726760667190288,\n",
       " 0.0016698821454830767,\n",
       " 0.0016670975444656136,\n",
       " 0.0016643222164012363,\n",
       " 0.0016615561187510823,\n",
       " 0.0016587992015886428,\n",
       " 0.00165605141998006,\n",
       " 0.00165331272184333,\n",
       " 0.0016505830719461812,\n",
       " 0.0016478624269380367,\n",
       " 0.0016451507361618456,\n",
       " 0.001642447965667794,\n",
       " 0.0016397540616179852,\n",
       " 0.0016370689755583123,\n",
       " 0.0016343926755216108,\n",
       " 0.0016317251094543684,\n",
       " 0.0016290662421499565,\n",
       " 0.0016264160302808769,\n",
       " 0.0016237744232108493,\n",
       " 0.0016211413870087534,\n",
       " 0.0016185168794950488,\n",
       " 0.0016159008627081493,\n",
       " 0.001613293290841095,\n",
       " 0.0016106941226676215,\n",
       " 0.0016081033210840919,\n",
       " 0.0016055208410557002,\n",
       " 0.0016029466420511207,\n",
       " 0.001600380687592161,\n",
       " 0.0015978229332069148,\n",
       " 0.0015952733388706673,\n",
       " 0.0015927318802027074,\n",
       " 0.0015901985003357977,\n",
       " 0.0015876731713724895,\n",
       " 0.001585155856563547,\n",
       " 0.0015826465111931254,\n",
       " 0.0015801450950169644,\n",
       " 0.0015776515834567619,\n",
       " 0.001575165931108982,\n",
       " 0.001572688097110442,\n",
       " 0.0015702180446842775,\n",
       " 0.0015677557407313108,\n",
       " 0.0015653011554622818,\n",
       " 0.001562854250425324,\n",
       " 0.0015604149793722259,\n",
       " 0.001557983312321129,\n",
       " 0.0015555592106468283,\n",
       " 0.0015531426512281718,\n",
       " 0.0015507335899730586,\n",
       " 0.001548331987198597,\n",
       " 0.0015459378071901852,\n",
       " 0.0015435510294462135,\n",
       " 0.0015411716122321686,\n",
       " 0.0015387995179869892,\n",
       " 0.0015364347129057194,\n",
       " 0.0015340771665638994,\n",
       " 0.001531726851579515,\n",
       " 0.0015293837316672216,\n",
       " 0.0015270477625286768,\n",
       " 0.0015247189275784366,\n",
       " 0.0015223971886065368,\n",
       " 0.0015200825112240105,\n",
       " 0.0015177748644807866,\n",
       " 0.0015154742088802691,\n",
       " 0.0015131805205170546,\n",
       " 0.0015108937662347466,\n",
       " 0.001508613916192589,\n",
       " 0.0015063409318923694,\n",
       " 0.001504074790327228,\n",
       " 0.0015018154591494592,\n",
       " 0.0014995629092461281,\n",
       " 0.001497317102774061,\n",
       " 0.0014950780173159336,\n",
       " 0.001492845621054622,\n",
       " 0.0014906198853547148,\n",
       " 0.0014884007728028096,\n",
       " 0.001486188261368376,\n",
       " 0.0014839823195824044,\n",
       " 0.0014817829191227868,\n",
       " 0.0014795900228580935,\n",
       " 0.001477403620653102,\n",
       " 0.0014752236684615159,\n",
       " 0.0014730501382832,\n",
       " 0.0014708830049180333,\n",
       " 0.001468722245685906,\n",
       " 0.001466567816891656,\n",
       " 0.0014644197024927012,\n",
       " 0.0014622778764091856,\n",
       " 0.0014601423035277062,\n",
       " 0.0014580129638877334,\n",
       " 0.0014558898278832575,\n",
       " 0.0014537728688688704,\n",
       " 0.001451662062863704,\n",
       " 0.001449557376643446,\n",
       " 0.0014474587803062144,\n",
       " 0.0014453662585818478,\n",
       " 0.0014432797744441369,\n",
       " 0.001441199306211025,\n",
       " 0.0014391248343686616,\n",
       " 0.0014370563180715163,\n",
       " 0.0014349937438416767,\n",
       " 0.001432937087907489,\n",
       " 0.001430886317232933,\n",
       " 0.0014288414137271224,\n",
       " 0.0014268023378251951,\n",
       " 0.00142476907720184,\n",
       " 0.0014227416091227442,\n",
       " 0.0014207199014854947,\n",
       " 0.0014187039370394516,\n",
       " 0.0014166936886175067,\n",
       " 0.0014146891201277307,\n",
       " 0.0014126902223704516,\n",
       " 0.0014106969637828998,\n",
       " 0.0014087093276001156,\n",
       " 0.0014067272870921022,\n",
       " 0.0014047508182018632,\n",
       " 0.0014027798876365693,\n",
       " 0.0014008144887157385,\n",
       " 0.0013988545921438727,\n",
       " 0.0013969001715550238,\n",
       " 0.0013949512032198383,\n",
       " 0.0013930076657818979,\n",
       " 0.0013910695400204264,\n",
       " 0.001389136796995193,\n",
       " 0.0013872094223015222,\n",
       " 0.0013852873913336736,\n",
       " 0.001383370670304949,\n",
       " 0.001381459252090384,\n",
       " 0.0013795531069944467,\n",
       " 0.0013776522199326902,\n",
       " 0.0013757565656875802,\n",
       " 0.0013738661215633363,\n",
       " 0.0013719808671337555,\n",
       " 0.001370100772373723,\n",
       " 0.0013682258335437005,\n",
       " 0.001366356012353506,\n",
       " 0.0013644912976250407,\n",
       " 0.0013626316676564835,\n",
       " 0.001360777102916195,\n",
       " 0.0013589275858257002,\n",
       " 0.0013570830889228394,\n",
       " 0.0013552435874916683,\n",
       " 0.0013534090709293697,\n",
       " 0.001351579518051876,\n",
       " 0.0013497548981519939,\n",
       " 0.0013479351952347817,\n",
       " 0.001346120394904792,\n",
       " 0.0013443104842061218,\n",
       " 0.0013425054281953946,\n",
       " 0.0013407052187066355,\n",
       " 0.0013389098251074026,\n",
       " 0.0013371192431115631,\n",
       " 0.0013353334455785326,\n",
       " 0.0013335524080817852,\n",
       " 0.0013317761202789801,\n",
       " 0.0013300045495789462,\n",
       " 0.0013282376899326933,\n",
       " 0.0013264755242545986,\n",
       " 0.0013247180255260692,\n",
       " 0.0013229651810719037,\n",
       " 0.0013212169678428888,\n",
       " 0.001319473365094732,\n",
       " 0.001317734365799103,\n",
       " 0.0013159999403474394,\n",
       " 0.001314270073733568,\n",
       " 0.0013125447524519323,\n",
       " 0.0013108239527059996,\n",
       " 0.0013091076647204225,\n",
       " 0.001307395868055856,\n",
       " 0.00130568854431689,\n",
       " 0.001303985665306123,\n",
       " 0.001302287228928958,\n",
       " 0.0013005932100171936,\n",
       " 0.0012989035975609802,\n",
       " 0.0012972183700099213,\n",
       " 0.0012955375079686602,\n",
       " 0.0012938609939813757,\n",
       " 0.0012921888123378286,\n",
       " 0.001290520948898804,\n",
       " 0.0012888573909390078,\n",
       " 0.0012871981153641438,\n",
       " 0.0012855431013893454,\n",
       " 0.001283892341949764,\n",
       " 0.0012822458190455048,\n",
       " 0.0012806035164766617,\n",
       " 0.0012789654196633197,\n",
       " 0.0012773315038420226,\n",
       " 0.0012757017583671928,\n",
       " 0.0012740761620162772,\n",
       " 0.0012724547073305113,\n",
       " 0.0012708373759554741,\n",
       " 0.001269224151372187,\n",
       " 0.001267615018713569,\n",
       " 0.0012660099645992475,\n",
       " 0.0012644089653453552,\n",
       " 0.001262812011277943,\n",
       " 0.0012612190936904571,\n",
       " 0.001259630181463935,\n",
       " 0.0012580452698743745,\n",
       " 0.0012564643430286412,\n",
       " 0.0012548873866229143,\n",
       " 0.0012533143877837556,\n",
       " 0.0012517453232835378,\n",
       " 0.0012501801838589292,\n",
       " 0.001248618961172931,\n",
       " 0.0012470616244391792,\n",
       " 0.0012455081692330115,\n",
       " 0.0012439585799306995,\n",
       " 0.0012424128424708874,\n",
       " 0.0012408709441983547,\n",
       " 0.0012393328737234038,\n",
       " 0.0012377986091537739,\n",
       " 0.001236268142427963,\n",
       " 0.0012347414546490867,\n",
       " 0.001233218528809949,\n",
       " 0.0012316993612456063,\n",
       " 0.001230183925374544,\n",
       " 0.0012286722205564647,\n",
       " 0.001227164222932035,\n",
       " 0.0012256599226693878,\n",
       " 0.0012241593109198422,\n",
       " 0.001222662368078054,\n",
       " 0.0012211690764992126,\n",
       " 0.0012196794319445214,\n",
       " 0.0012181934189575304,\n",
       " 0.0012167110236274352,\n",
       " 0.0012152322334345116,\n",
       " 0.0012137570254694761,\n",
       " 0.0012122854023970371,\n",
       " 0.0012108173433323687,\n",
       " 0.0012093528294791268,\n",
       " 0.0012078918555621344,\n",
       " 0.001206434416833732,\n",
       " 0.0012049804857379616,\n",
       " 0.0012035300607575257,\n",
       " 0.001202083117243792,\n",
       " 0.0012006396562960556,\n",
       " 0.0011991996556206177,\n",
       " 0.001197763106794682,\n",
       " 0.0011963299905961994,\n",
       " 0.0011949003013668575,\n",
       " 0.0011934740223726438,\n",
       " 0.0011920511501944786,\n",
       " 0.0011906316701136583,\n",
       " 0.0011892155572418171,\n",
       " 0.0011878028124624907,\n",
       " 0.0011863934249293285,\n",
       " 0.001184987373239083,\n",
       " 0.001183584649770364,\n",
       " 0.0011821852476639194,\n",
       " 0.0011807891491048912,\n",
       " 0.0011793963497014385,\n",
       " 0.001178006833859373,\n",
       " 0.0011766205875439247,\n",
       " 0.001175237598123801,\n",
       " 0.001173857854230838,\n",
       " 0.0011724813456336887,\n",
       " 0.001171108063124141,\n",
       " 0.0011697379984148034,\n",
       " 0.0011683711324054916,\n",
       " 0.0011670074595475722,\n",
       " 0.0011656469632057424,\n",
       " 0.0011642896400497625,\n",
       " 0.001162935475440884,\n",
       " 0.0011615844562042337,\n",
       " 0.0011602365704824252,\n",
       " 0.0011588918076038113,\n",
       " 0.0011575501696054415,\n",
       " 0.0011562116233960638,\n",
       " 0.0011548761741181478,\n",
       " 0.001153543803116852,\n",
       " 0.001152214505244351,\n",
       " 0.0011508882758676023,\n",
       " 0.0011495650875338029,\n",
       " 0.0011482449388184946,\n",
       " 0.001146927828439665,\n",
       " 0.0011456137319604386,\n",
       " 0.00114430263902916,\n",
       " 0.0011429945519708705,\n",
       " 0.0011416894495950441,\n",
       " 0.001140387324471739,\n",
       " 0.0011390881582724755,\n",
       " 0.001137791957784686,\n",
       " 0.0011364987058339955,\n",
       " 0.0011352083869634664,\n",
       " 0.0011339209989033876,\n",
       " 0.0011326365279694859,\n",
       " 0.0011313549618459163,\n",
       " 0.001130076289448418,\n",
       " 0.0011288005008011559,\n",
       " 0.0011275275985674113,\n",
       " 0.0011262575618610725,\n",
       " 0.0011249903835261713,\n",
       " 0.0011237260454808043,\n",
       " 0.0011224645547343224,\n",
       " 0.0011212058943120774,\n",
       " 0.001119950048936886,\n",
       " 0.0011186970165008151,\n",
       " 0.0011174467834651927,\n",
       " 0.0011161993376452143,\n",
       " 0.001114954679716088,\n",
       " 0.0011137127986439682,\n",
       " 0.001112473672856863,\n",
       " 0.001111237306223043,\n",
       " 0.0011100036905824198,\n",
       " 0.001108772806949382,\n",
       " 0.001107544649878411,\n",
       " 0.0011063192144685405,\n",
       " 0.0011050964846673684,\n",
       " 0.0011038764576692648,\n",
       " 0.0011026591193076315,\n",
       " 0.0011014444568325289,\n",
       " 0.0011002324704105443,\n",
       " 0.0010990231485500741,\n",
       " 0.0010978164809086748,\n",
       " 0.0010966124698196795,\n",
       " 0.0010954110941000235,\n",
       " 0.0010942123463264812,\n",
       " 0.0010930162198181487,\n",
       " 0.0010918227085622136,\n",
       " 0.0010906317955056116,\n",
       " 0.0010894434769421176,\n",
       " 0.001088257749536129,\n",
       " 0.0010870745986440707,\n",
       " 0.001085894022728258,\n",
       " 0.0010847160087632422,\n",
       " 0.001083540545026119,\n",
       " 0.0010823676209662755,\n",
       " 0.0010811972387296915,\n",
       " 0.001080029388606217,\n",
       " 0.0010788640618567167,\n",
       " 0.001077701238974437,\n",
       " 0.0010765409256863637,\n",
       " 0.00107538310386372,\n",
       " 0.0010742277688321387,\n",
       " 0.0010730749163846893,\n",
       " 0.0010719245427351345,\n",
       " 0.0010707766328343286,\n",
       " 0.0010696311731380403,\n",
       " 0.0010684881630979937,\n",
       " 0.0010673475905792083,\n",
       " 0.0010662094563017338,\n",
       " 0.0010650737376305505,\n",
       " 0.0010639404374772053,\n",
       " 0.0010628095468205579,\n",
       " 0.001061681057541582,\n",
       " 0.0010605549739746867,\n",
       " 0.001059431265096243,\n",
       " 0.0010583099379095815,\n",
       " 0.0010571909870761677,\n",
       " 0.0010560743961497883,\n",
       " 0.0010549601619703835,\n",
       " 0.0010538482816938996,\n",
       " 0.0010527387411191557,\n",
       " 0.001051631539106622,\n",
       " 0.0010505266629891885,\n",
       " 0.0010494241013664567,\n",
       " 0.0010483238556196,\n",
       " 0.0010472259153501426,\n",
       " 0.001046130271199456,\n",
       " 0.0010450369147447746,\n",
       " 0.0010439458384056092,\n",
       " 0.0010428570353595197,\n",
       " 0.0010417705111078423,\n",
       " 0.001040686247318715,\n",
       " 0.0010396042391349943,\n",
       " 0.0010385244705437189,\n",
       " 0.0010374469504162876,\n",
       " 0.0010363716634538955,\n",
       " 0.001035298595887618,\n",
       " 0.001034227746966955,\n",
       " 0.0010331591160164567,\n",
       " 0.001032092690786686,\n",
       " 0.0010310284718946813,\n",
       " 0.0010299664366127132,\n",
       " 0.0010289065877688896,\n",
       " 0.0010278489279085075,\n",
       " 0.0010267934360392696,\n",
       " 0.0010257401049096411,\n",
       " 0.0010246889396347725,\n",
       " 0.0010236399215352349,\n",
       " 0.0010225930570826065,\n",
       " 0.0010215483288182436,\n",
       " 0.0010205057326708989,\n",
       " 0.0010194652649764499,\n",
       " 0.0010184269107956542,\n",
       " 0.0010173906683247256,\n",
       " 0.001016356535940257,\n",
       " 0.001015324500539649,\n",
       " 0.0010142945619720952,\n",
       " 0.0010132667084603371,\n",
       " 0.0010122409410463747,\n",
       " 0.0010112172490264752,\n",
       " 0.0010101956227672798,\n",
       " 0.001009176065240297]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(1000, dtype=torch.float32)\n",
    "x += 1\n",
    "y = 1 / x\n",
    "y = y.tolist()\n",
    "ma = []\n",
    "for i in y:\n",
    "    if ma:\n",
    "        ma.append(ma[-1]*0.9+i*0.1)\n",
    "    else :\n",
    "        ma.append(i)\n",
    "ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q学习算法实现**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体在训练中只做两件事：\n",
    "- 选择动作\n",
    "- 更新策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, cfg):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.n\n",
    "        self.action_size = env.action_space.n\n",
    "        self.Q = np.random.rand(self.state_size, self.action_size)\n",
    "        self.gamma = cfg.gamma\n",
    "        self.alpha = cfg.alpha\n",
    "        self.epsilon_min = cfg.epsilon_min\n",
    "        self.epsilon_max = cfg.epsilon_max\n",
    "        self.epsilon = cfg.epsilon\n",
    "        self.sample_count = 0\n",
    "        self.epsilon_decay = cfg.epsilon_decay\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        self.sample_count += 1\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-1. * self.sample_count / self.epsilon_decay) # decrease epsilon\n",
    "        # epsilon-greedy strategy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state, :])\n",
    "        \n",
    "    def qlearn(self, state, action, reward, next_state, done):\n",
    "        q_now = self.Q[state, action]\n",
    "        if done:\n",
    "            q_next = reward\n",
    "        else:\n",
    "            q_next = reward + self.gamma * np.max(self.Q[next_state, :])\n",
    "        self.Q[state, action] += self.alpha * (q_next - q_now)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.seed = 1 # 随机种子\n",
    "        self.epsilon = 0.95 #  e-greedy策略中epsilon的初始值\n",
    "        self.epsilon_max = 0.95 #  e-greedy策略中epsilon的初始值\n",
    "        self.epsilon_min = 0.01 #  e-greedy策略中epsilon的最终值\n",
    "        self.epsilon_decay = 300 #  e-greedy策略中epsilon的衰减率\n",
    "        self.gamma = 0.9 # 折扣因子\n",
    "        self.alpha = 0.1 # 学习率\n",
    "\n",
    "cfg = Config()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env,cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, reward: -3386, epsilon: 0.17888418913003543, ma: -3386\n",
      "Episode: 1, reward: -206, epsilon: 0.1282199109253413, ma: -3068.0\n",
      "Episode: 2, reward: -377, epsilon: 0.07509709179078522, ma: -2798.9\n",
      "Episode: 3, reward: -113, epsilon: 0.05466602781795325, ma: -2530.3100000000004\n",
      "Episode: 4, reward: -157, epsilon: 0.036466502459482096, ma: -2292.9790000000003\n",
      "Episode: 5, reward: -255, epsilon: 0.025734879545065793, ma: -2089.1811000000002\n",
      "Episode: 6, reward: -316, epsilon: 0.017633500658128767, ma: -1911.86299\n",
      "Episode: 7, reward: -339, epsilon: 0.012465874575496746, ma: -1754.5766910000002\n",
      "Episode: 8, reward: -79, epsilon: 0.011894989331299403, ma: -1587.0190219000003\n",
      "Episode: 9, reward: -203, epsilon: 0.010963239247691907, ma: -1448.6171197100002\n",
      "Episode: 10, reward: -301, epsilon: 0.010491257528167241, ma: -1333.8554077390002\n",
      "Episode: 11, reward: -183, epsilon: 0.01026692520486906, ma: -1218.7698669651002\n",
      "Episode: 12, reward: -202, epsilon: 0.010136133381881726, ma: -1117.0928802685903\n",
      "Episode: 13, reward: -55, epsilon: 0.010113329762479608, ma: -1010.8835922417313\n",
      "Episode: 14, reward: -284, epsilon: 0.01006116867146269, ma: -938.1952330175582\n",
      "Episode: 15, reward: -134, epsilon: 0.010039133093401875, ma: -857.7757097158023\n",
      "Episode: 16, reward: -167, epsilon: 0.010022427812562169, ma: -788.6981387442222\n",
      "Episode: 17, reward: -111, epsilon: 0.01001549166009779, ma: -720.9283248698\n",
      "Episode: 18, reward: -268, epsilon: 0.010008819529179454, ma: -675.63549238282\n",
      "Episode: 19, reward: -60, epsilon: 0.010007220819766887, ma: -614.071943144538\n",
      "Episode: 20, reward: -187, epsilon: 0.010003871473385084, ma: -571.3647488300842\n",
      "Episode: 21, reward: -125, epsilon: 0.010002552232554187, ma: -526.7282739470758\n",
      "Episode: 22, reward: -87, epsilon: 0.010001909742636286, ma: -482.7554465523682\n",
      "Episode: 23, reward: -118, epsilon: 0.0100012887015412, ma: -446.2799018971314\n",
      "Episode: 24, reward: -96, epsilon: 0.01000093578938322, ma: -411.2519117074183\n",
      "Episode: 25, reward: -90, epsilon: 0.010000693249825809, ma: -379.1267205366765\n",
      "Episode: 26, reward: -117, epsilon: 0.010000469369560309, ma: -352.91404848300886\n",
      "Episode: 27, reward: -108, epsilon: 0.010000327468030406, ma: -328.422643634708\n",
      "Episode: 28, reward: -112, epsilon: 0.010000225440687986, ma: -306.78037927123717\n",
      "Episode: 29, reward: -99, epsilon: 0.010000162074661074, ma: -286.00234134411346\n",
      "Episode: 30, reward: -96, epsilon: 0.010000117690359073, ma: -267.00210720970216\n",
      "Episode: 31, reward: -61, epsilon: 0.010000096036061982, ma: -246.40189648873195\n",
      "Episode: 32, reward: -75, epsilon: 0.010000074792960273, ma: -229.26170683985876\n",
      "Episode: 33, reward: -81, epsilon: 0.010000057095412193, ma: -214.43553615587288\n",
      "Episode: 34, reward: -49, epsilon: 0.01000004849159254, ma: -197.8919825402856\n",
      "Episode: 35, reward: -140, epsilon: 0.01000003040854841, ma: -192.10278428625705\n",
      "Episode: 36, reward: -87, epsilon: 0.010000022753608917, ma: -181.59250585763132\n",
      "Episode: 37, reward: -57, epsilon: 0.010000018816304725, ma: -169.1332552718682\n",
      "Episode: 38, reward: -122, epsilon: 0.010000012529139606, ma: -164.41992974468135\n",
      "Episode: 39, reward: -50, epsilon: 0.010000010605687707, ma: -152.97793677021323\n",
      "Episode: 40, reward: -85, epsilon: 0.010000007988932129, ma: -146.1801430931919\n",
      "Episode: 41, reward: -45, epsilon: 0.010000006876137607, ma: -136.0621287838727\n",
      "Episode: 42, reward: -109, epsilon: 0.010000004781353983, ma: -133.35591590548543\n",
      "Episode: 43, reward: -29, epsilon: 0.010000004340793215, ma: -122.92032431493689\n",
      "Episode: 44, reward: -83, epsilon: 0.010000003291654853, ma: -118.92829188344321\n",
      "Episode: 45, reward: -72, epsilon: 0.010000002589307416, ma: -114.23546269509889\n",
      "Episode: 46, reward: -78, epsilon: 0.01000000199648959, ma: -110.611916425589\n",
      "Episode: 47, reward: -61, epsilon: 0.010000001629147872, ma: -105.6507247830301\n",
      "Episode: 48, reward: -67, epsilon: 0.010000001303070974, ma: -101.7856523047271\n",
      "Episode: 49, reward: -79, epsilon: 0.0100000010013914, ma: -99.50708707425439\n",
      "Episode: 50, reward: -57, epsilon: 0.010000000828109765, ma: -95.25637836682895\n",
      "Episode: 51, reward: -59, epsilon: 0.010000000680262699, ma: -91.63074053014607\n",
      "Episode: 52, reward: -58, epsilon: 0.01000000056067741, ma: -88.26766647713146\n",
      "Episode: 53, reward: -74, epsilon: 0.010000000438113955, ma: -86.84089982941832\n",
      "Episode: 54, reward: -62, epsilon: 0.010000000356314005, ma: -84.3568098464765\n",
      "Episode: 55, reward: -73, epsilon: 0.01000000027935379, ma: -83.22112886182884\n",
      "Episode: 56, reward: -49, epsilon: 0.01000000023725742, ma: -79.79901597564597\n",
      "Episode: 57, reward: -75, epsilon: 0.010000000184776264, ma: -79.31911437808138\n",
      "Episode: 58, reward: -49, epsilon: 0.010000000156931968, ma: -76.28720294027325\n",
      "Episode: 59, reward: -129, epsilon: 0.010000000141997915, ma: -81.55848264624593\n",
      "Episode: 60, reward: -71, epsilon: 0.01000000011207247, ma: -80.50263438162133\n",
      "Episode: 61, reward: -70, epsilon: 0.010000000088749019, ma: -79.4523709434592\n",
      "Episode: 62, reward: -57, epsilon: 0.010000000073391812, ma: -77.20713384911329\n",
      "Episode: 63, reward: -62, epsilon: 0.010000000059688879, ma: -75.68642046420197\n",
      "Episode: 64, reward: -31, epsilon: 0.010000000053829002, ma: -71.21777841778177\n",
      "Episode: 65, reward: -38, epsilon: 0.01000000004742482, ma: -67.8960005760036\n",
      "Episode: 66, reward: -167, epsilon: 0.010000000037806425, ma: -77.80640051840324\n",
      "Episode: 67, reward: -71, epsilon: 0.010000000029838885, ma: -77.12576046656291\n",
      "Episode: 68, reward: -45, epsilon: 0.010000000025682566, ma: -73.91318441990661\n",
      "Episode: 69, reward: -35, epsilon: 0.010000000022854448, ma: -70.02186597791595\n",
      "Episode: 70, reward: -44, epsilon: 0.010000000019736685, ma: -67.41967938012436\n",
      "Episode: 71, reward: -47, epsilon: 0.010000000016874648, ma: -65.37771144211193\n",
      "Episode: 72, reward: -67, epsilon: 0.010000000013497157, ma: -65.53994029790074\n",
      "Episode: 73, reward: -50, epsilon: 0.010000000011425097, ma: -63.985946268110666\n",
      "Episode: 74, reward: -60, epsilon: 0.010000000009354078, ma: -63.5873516412996\n",
      "Episode: 75, reward: -38, epsilon: 0.010000000008241198, ma: -61.028616477169635\n",
      "Episode: 76, reward: -25, epsilon: 0.010000000007582268, ma: -57.42575482945267\n",
      "Episode: 77, reward: -67, epsilon: 0.010000000006064663, ma: -58.38317934650741\n",
      "Episode: 78, reward: -51, epsilon: 0.010000000005116543, ma: -57.644861411856674\n",
      "Episode: 79, reward: -62, epsilon: 0.010000000004161238, ma: -58.08037527067101\n",
      "Episode: 80, reward: -35, epsilon: 0.010000000003703009, ma: -55.77233774360391\n",
      "Episode: 81, reward: -41, epsilon: 0.01000000000322999, ma: -54.29510396924353\n",
      "Episode: 82, reward: -67, epsilon: 0.010000000002583501, ma: -55.56559357231918\n",
      "Episode: 83, reward: -40, epsilon: 0.010000000002261013, ma: -54.00903421508726\n",
      "Episode: 84, reward: -58, epsilon: 0.010000000001863543, ma: -54.408130793578536\n",
      "Episode: 85, reward: -31, epsilon: 0.010000000001680592, ma: -52.067317714220685\n",
      "Episode: 86, reward: -23, epsilon: 0.01000000000155656, ma: -49.16058594279861\n",
      "Episode: 87, reward: -60, epsilon: 0.010000000001274404, ma: -50.24452734851875\n",
      "Episode: 88, reward: -176, epsilon: 0.010000000000985913, ma: -62.820074613666876\n",
      "Episode: 89, reward: -30, epsilon: 0.01000000000089209, ma: -59.53806715230019\n",
      "Episode: 90, reward: -42, epsilon: 0.010000000000775547, ma: -57.784260437070174\n",
      "Episode: 91, reward: -49, epsilon: 0.010000000000658678, ma: -56.90583439336316\n",
      "Episode: 92, reward: -73, epsilon: 0.01000000000051641, ma: -58.51525095402684\n",
      "Episode: 93, reward: -30, epsilon: 0.010000000000467267, ma: -55.663725858624154\n",
      "Episode: 94, reward: -125, epsilon: 0.010000000000428475, ma: -62.59735327276174\n",
      "Episode: 95, reward: -38, epsilon: 0.010000000000377499, ma: -60.13761794548556\n",
      "Episode: 96, reward: -60, epsilon: 0.01000000000030907, ma: -60.123856150937\n",
      "Episode: 97, reward: -27, epsilon: 0.010000000000282469, ma: -56.811470535843306\n",
      "Episode: 98, reward: -32, epsilon: 0.01000000000025389, ma: -54.330323482258976\n",
      "Episode: 99, reward: -42, epsilon: 0.010000000000220721, ma: -53.097291134033085\n",
      "Episode: 100, reward: -58, epsilon: 0.01000000000018192, ma: -53.587562020629775\n",
      "Episode: 101, reward: -18, epsilon: 0.010000000000171327, ma: -50.028805818566795\n",
      "Episode: 102, reward: -35, epsilon: 0.01000000000015246, ma: -48.525925236710115\n",
      "Episode: 103, reward: -38, epsilon: 0.010000000000134322, ma: -47.4733327130391\n",
      "Episode: 104, reward: -40, epsilon: 0.010000000000117554, ma: -46.72599944173519\n",
      "Episode: 105, reward: -27, epsilon: 0.010000000000107437, ma: -44.753399497561674\n",
      "Episode: 106, reward: -49, epsilon: 0.010000000000091247, ma: -45.1780595478055\n",
      "Episode: 107, reward: -29, epsilon: 0.01000000000008284, ma: -43.56025359302495\n",
      "Episode: 108, reward: -31, epsilon: 0.010000000000074706, ma: -42.30422823372246\n",
      "Episode: 109, reward: -75, epsilon: 0.010000000000058181, ma: -45.57380541035022\n",
      "Episode: 110, reward: -23, epsilon: 0.010000000000053888, ma: -43.31642486931519\n",
      "Episode: 111, reward: -31, epsilon: 0.010000000000048597, ma: -42.08478238238367\n",
      "Episode: 112, reward: -29, epsilon: 0.01000000000004412, ma: -40.77630414414531\n",
      "Episode: 113, reward: -43, epsilon: 0.010000000000038228, ma: -40.998673729730776\n",
      "Episode: 114, reward: -166, epsilon: 0.010000000000030576, ma: -53.4988063567577\n",
      "Episode: 115, reward: -18, epsilon: 0.010000000000028797, ma: -49.94892572108193\n",
      "Episode: 116, reward: -24, epsilon: 0.010000000000026581, ma: -47.354033148973734\n",
      "Episode: 117, reward: -54, epsilon: 0.010000000000022203, ma: -48.01862983407636\n",
      "Episode: 118, reward: -30, epsilon: 0.01000000000002009, ma: -46.21676685066872\n",
      "Episode: 119, reward: -33, epsilon: 0.010000000000017998, ma: -44.89509016560184\n",
      "Episode: 120, reward: -40, epsilon: 0.010000000000015751, ma: -44.40558114904166\n",
      "Episode: 121, reward: -35, epsilon: 0.010000000000014017, ma: -43.465023034137495\n",
      "Episode: 122, reward: -28, epsilon: 0.010000000000012768, ma: -41.91852073072374\n",
      "Episode: 123, reward: -62, epsilon: 0.010000000000010384, ma: -43.92666865765137\n",
      "Episode: 124, reward: -15, epsilon: 0.010000000000009878, ma: -41.03400179188623\n",
      "Episode: 125, reward: -26, epsilon: 0.010000000000009057, ma: -39.53060161269761\n",
      "Episode: 126, reward: -22, epsilon: 0.010000000000008417, ma: -37.77754145142785\n",
      "Episode: 127, reward: -41, epsilon: 0.010000000000007342, ma: -38.099787306285066\n",
      "Episode: 128, reward: -38, epsilon: 0.010000000000006469, ma: -38.089808575656555\n",
      "Episode: 129, reward: -120, epsilon: 0.010000000000006032, ma: -46.2808277180909\n",
      "Episode: 130, reward: -55, epsilon: 0.01000000000000502, ma: -47.15274494628181\n",
      "Episode: 131, reward: -21, epsilon: 0.010000000000004682, ma: -44.537470451653626\n",
      "Episode: 132, reward: -34, epsilon: 0.01000000000000418, ma: -43.48372340648826\n",
      "Episode: 133, reward: -35, epsilon: 0.01000000000000372, ma: -42.63535106583944\n",
      "Episode: 134, reward: -16, epsilon: 0.010000000000003527, ma: -39.971815959255494\n",
      "Episode: 135, reward: -27, epsilon: 0.010000000000003223, ma: -38.67463436332995\n",
      "Episode: 136, reward: -54, epsilon: 0.010000000000002692, ma: -40.20717092699695\n",
      "Episode: 137, reward: -31, epsilon: 0.010000000000002427, ma: -39.28645383429726\n",
      "Episode: 138, reward: -19, epsilon: 0.01000000000000228, ma: -37.257808450867536\n",
      "Episode: 139, reward: -31, epsilon: 0.010000000000002056, ma: -36.632027605780785\n",
      "Episode: 140, reward: -29, epsilon: 0.010000000000001865, ma: -35.8688248452027\n",
      "Episode: 141, reward: -22, epsilon: 0.010000000000001733, ma: -34.48194236068244\n",
      "Episode: 142, reward: -41, epsilon: 0.010000000000001513, ma: -35.13374812461419\n",
      "Episode: 143, reward: -34, epsilon: 0.01000000000000135, ma: -35.02037331215278\n",
      "Episode: 144, reward: -30, epsilon: 0.010000000000001221, ma: -34.5183359809375\n",
      "Episode: 145, reward: -34, epsilon: 0.010000000000001091, ma: -34.46650238284375\n",
      "Episode: 146, reward: -21, epsilon: 0.010000000000001017, ma: -33.119852144559374\n",
      "Episode: 147, reward: -22, epsilon: 0.010000000000000946, ma: -32.00786693010344\n",
      "Episode: 148, reward: -31, epsilon: 0.010000000000000852, ma: -31.907080237093098\n",
      "Episode: 149, reward: -30, epsilon: 0.010000000000000772, ma: -31.71637221338379\n",
      "Episode: 150, reward: -17, epsilon: 0.010000000000000729, ma: -30.24473499204541\n",
      "Episode: 151, reward: -51, epsilon: 0.010000000000000614, ma: -32.32026149284087\n",
      "Episode: 152, reward: -26, epsilon: 0.010000000000000564, ma: -31.688235343556787\n",
      "Episode: 153, reward: -17, epsilon: 0.010000000000000533, ma: -30.21941180920111\n",
      "Episode: 154, reward: -38, epsilon: 0.01000000000000047, ma: -30.997470628281\n",
      "Episode: 155, reward: -27, epsilon: 0.010000000000000429, ma: -30.5977235654529\n",
      "Episode: 156, reward: -31, epsilon: 0.010000000000000387, ma: -30.637951208907612\n",
      "Episode: 157, reward: -16, epsilon: 0.010000000000000366, ma: -29.174156088016854\n",
      "Episode: 158, reward: -24, epsilon: 0.010000000000000338, ma: -28.656740479215166\n",
      "Episode: 159, reward: -32, epsilon: 0.010000000000000304, ma: -28.99106643129365\n",
      "Episode: 160, reward: -23, epsilon: 0.010000000000000281, ma: -28.391959788164286\n",
      "Episode: 161, reward: -32, epsilon: 0.010000000000000253, ma: -28.752763809347858\n",
      "Episode: 162, reward: -30, epsilon: 0.01000000000000023, ma: -28.877487428413072\n",
      "Episode: 163, reward: -16, epsilon: 0.010000000000000217, ma: -27.589738685571767\n",
      "Episode: 164, reward: -34, epsilon: 0.010000000000000194, ma: -28.230764817014588\n",
      "Episode: 165, reward: -23, epsilon: 0.01000000000000018, ma: -27.70768833531313\n",
      "Episode: 166, reward: -34, epsilon: 0.01000000000000016, ma: -28.336919501781814\n",
      "Episode: 167, reward: -24, epsilon: 0.010000000000000148, ma: -27.903227551603635\n",
      "Episode: 168, reward: -21, epsilon: 0.010000000000000139, ma: -27.212904796443272\n",
      "Episode: 169, reward: -22, epsilon: 0.010000000000000129, ma: -26.691614316798944\n",
      "Episode: 170, reward: -15, epsilon: 0.010000000000000122, ma: -25.52245288511905\n",
      "Episode: 171, reward: -35, epsilon: 0.01000000000000011, ma: -26.470207596607146\n",
      "Episode: 172, reward: -24, epsilon: 0.0100000000000001, ma: -26.223186836946432\n",
      "Episode: 173, reward: -29, epsilon: 0.01000000000000009, ma: -26.500868153251787\n",
      "Episode: 174, reward: -39, epsilon: 0.01000000000000008, ma: -27.75078133792661\n",
      "Episode: 175, reward: -37, epsilon: 0.010000000000000071, ma: -28.67570320413395\n",
      "Episode: 176, reward: -21, epsilon: 0.010000000000000066, ma: -27.908132883720555\n",
      "Episode: 177, reward: -17, epsilon: 0.010000000000000063, ma: -26.8173195953485\n",
      "Episode: 178, reward: -29, epsilon: 0.010000000000000057, ma: -27.03558763581365\n",
      "Episode: 179, reward: -19, epsilon: 0.010000000000000054, ma: -26.232028872232284\n",
      "Episode: 180, reward: -25, epsilon: 0.010000000000000049, ma: -26.108825985009055\n",
      "Episode: 181, reward: -21, epsilon: 0.010000000000000045, ma: -25.597943386508152\n",
      "Episode: 182, reward: -25, epsilon: 0.010000000000000042, ma: -25.53814904785734\n",
      "Episode: 183, reward: -25, epsilon: 0.010000000000000038, ma: -25.484334143071607\n",
      "Episode: 184, reward: -29, epsilon: 0.010000000000000035, ma: -25.83590072876445\n",
      "Episode: 185, reward: -22, epsilon: 0.010000000000000033, ma: -25.452310655888006\n",
      "Episode: 186, reward: -18, epsilon: 0.010000000000000031, ma: -24.707079590299205\n",
      "Episode: 187, reward: -17, epsilon: 0.01000000000000003, ma: -23.936371631269285\n",
      "Episode: 188, reward: -24, epsilon: 0.010000000000000026, ma: -23.942734468142355\n",
      "Episode: 189, reward: -34, epsilon: 0.010000000000000024, ma: -24.94846102132812\n",
      "Episode: 190, reward: -35, epsilon: 0.010000000000000021, ma: -25.95361491919531\n",
      "Episode: 191, reward: -17, epsilon: 0.010000000000000021, ma: -25.05825342727578\n",
      "Episode: 192, reward: -28, epsilon: 0.01000000000000002, ma: -25.352428084548205\n",
      "Episode: 193, reward: -14, epsilon: 0.010000000000000018, ma: -24.21718527609338\n",
      "Episode: 194, reward: -18, epsilon: 0.010000000000000016, ma: -23.595466748484046\n",
      "Episode: 195, reward: -27, epsilon: 0.010000000000000016, ma: -23.935920073635643\n",
      "Episode: 196, reward: -28, epsilon: 0.010000000000000014, ma: -24.34232806627208\n",
      "Episode: 197, reward: -17, epsilon: 0.010000000000000012, ma: -23.608095259644873\n",
      "Episode: 198, reward: -16, epsilon: 0.010000000000000012, ma: -22.847285733680387\n",
      "Episode: 199, reward: -32, epsilon: 0.01000000000000001, ma: -23.762557160312348\n",
      "Episode: 200, reward: -134, epsilon: 0.01000000000000001, ma: -34.786301444281115\n",
      "Episode: 201, reward: -17, epsilon: 0.010000000000000009, ma: -33.00767129985301\n",
      "Episode: 202, reward: -16, epsilon: 0.010000000000000009, ma: -31.30690416986771\n",
      "Episode: 203, reward: -15, epsilon: 0.010000000000000009, ma: -29.67621375288094\n",
      "Episode: 204, reward: -19, epsilon: 0.010000000000000009, ma: -28.608592377592846\n",
      "Episode: 205, reward: -25, epsilon: 0.010000000000000007, ma: -28.24773313983356\n",
      "Episode: 206, reward: -14, epsilon: 0.010000000000000007, ma: -26.8229598258502\n",
      "Episode: 207, reward: -20, epsilon: 0.010000000000000007, ma: -26.14066384326518\n",
      "Episode: 208, reward: -31, epsilon: 0.010000000000000005, ma: -26.626597458938665\n",
      "Episode: 209, reward: -156, epsilon: 0.010000000000000005, ma: -39.5639377130448\n",
      "Episode: 210, reward: -15, epsilon: 0.010000000000000005, ma: -37.107543941740325\n",
      "Episode: 211, reward: -30, epsilon: 0.010000000000000004, ma: -36.39678954756629\n",
      "Episode: 212, reward: -25, epsilon: 0.010000000000000004, ma: -35.25711059280967\n",
      "Episode: 213, reward: -21, epsilon: 0.010000000000000004, ma: -33.8313995335287\n",
      "Episode: 214, reward: -16, epsilon: 0.010000000000000004, ma: -32.04825958017583\n",
      "Episode: 215, reward: -24, epsilon: 0.010000000000000004, ma: -31.24343362215825\n",
      "Episode: 216, reward: -18, epsilon: 0.010000000000000004, ma: -29.919090259942426\n",
      "Episode: 217, reward: -141, epsilon: 0.010000000000000002, ma: -41.027181233948184\n",
      "Episode: 218, reward: -18, epsilon: 0.010000000000000002, ma: -38.724463110553366\n",
      "Episode: 219, reward: -17, epsilon: 0.010000000000000002, ma: -36.552016799498034\n",
      "Episode: 220, reward: -16, epsilon: 0.010000000000000002, ma: -34.49681511954823\n",
      "Episode: 221, reward: -14, epsilon: 0.010000000000000002, ma: -32.44713360759341\n",
      "Episode: 222, reward: -23, epsilon: 0.010000000000000002, ma: -31.502420246834067\n",
      "Episode: 223, reward: -29, epsilon: 0.010000000000000002, ma: -31.25217822215066\n",
      "Episode: 224, reward: -20, epsilon: 0.010000000000000002, ma: -30.126960399935594\n",
      "Episode: 225, reward: -27, epsilon: 0.010000000000000002, ma: -29.814264359942033\n",
      "Episode: 226, reward: -15, epsilon: 0.010000000000000002, ma: -28.33283792394783\n",
      "Episode: 227, reward: -31, epsilon: 0.010000000000000002, ma: -28.59955413155305\n",
      "Episode: 228, reward: -15, epsilon: 0.010000000000000002, ma: -27.239598718397744\n",
      "Episode: 229, reward: -13, epsilon: 0.010000000000000002, ma: -25.81563884655797\n",
      "Episode: 230, reward: -126, epsilon: 0.010000000000000002, ma: -35.834074961902175\n",
      "Episode: 231, reward: -19, epsilon: 0.010000000000000002, ma: -34.150667465711955\n",
      "Episode: 232, reward: -25, epsilon: 0.010000000000000002, ma: -33.23560071914076\n",
      "Episode: 233, reward: -17, epsilon: 0.01, ma: -31.612040647226685\n",
      "Episode: 234, reward: -15, epsilon: 0.01, ma: -29.950836582504017\n",
      "Episode: 235, reward: -13, epsilon: 0.01, ma: -28.255752924253617\n",
      "Episode: 236, reward: -16, epsilon: 0.01, ma: -27.030177631828256\n",
      "Episode: 237, reward: -23, epsilon: 0.01, ma: -26.627159868645432\n",
      "Episode: 238, reward: -34, epsilon: 0.01, ma: -27.364443881780886\n",
      "Episode: 239, reward: -18, epsilon: 0.01, ma: -26.427999493602798\n",
      "Episode: 240, reward: -16, epsilon: 0.01, ma: -25.38519954424252\n",
      "Episode: 241, reward: -13, epsilon: 0.01, ma: -24.14667958981827\n",
      "Episode: 242, reward: -19, epsilon: 0.01, ma: -23.63201163083644\n",
      "Episode: 243, reward: -22, epsilon: 0.01, ma: -23.468810467752796\n",
      "Episode: 244, reward: -24, epsilon: 0.01, ma: -23.521929420977514\n",
      "Episode: 245, reward: -16, epsilon: 0.01, ma: -22.769736478879764\n",
      "Episode: 246, reward: -13, epsilon: 0.01, ma: -21.79276283099179\n",
      "Episode: 247, reward: -18, epsilon: 0.01, ma: -21.413486547892614\n",
      "Episode: 248, reward: -19, epsilon: 0.01, ma: -21.17213789310335\n",
      "Episode: 249, reward: -16, epsilon: 0.01, ma: -20.654924103793018\n",
      "Episode: 250, reward: -23, epsilon: 0.01, ma: -20.889431693413716\n",
      "Episode: 251, reward: -126, epsilon: 0.01, ma: -31.400488524072347\n",
      "Episode: 252, reward: -16, epsilon: 0.01, ma: -29.860439671665116\n",
      "Episode: 253, reward: -24, epsilon: 0.01, ma: -29.274395704498602\n",
      "Episode: 254, reward: -29, epsilon: 0.01, ma: -29.246956134048745\n",
      "Episode: 255, reward: -13, epsilon: 0.01, ma: -27.62226052064387\n",
      "Episode: 256, reward: -14, epsilon: 0.01, ma: -26.260034468579484\n",
      "Episode: 257, reward: -22, epsilon: 0.01, ma: -25.834031021721536\n",
      "Episode: 258, reward: -127, epsilon: 0.01, ma: -35.95062791954938\n",
      "Episode: 259, reward: -21, epsilon: 0.01, ma: -34.45556512759445\n",
      "Episode: 260, reward: -18, epsilon: 0.01, ma: -32.810008614835\n",
      "Episode: 261, reward: -14, epsilon: 0.01, ma: -30.9290077533515\n",
      "Episode: 262, reward: -20, epsilon: 0.01, ma: -29.83610697801635\n",
      "Episode: 263, reward: -14, epsilon: 0.01, ma: -28.252496280214714\n",
      "Episode: 264, reward: -15, epsilon: 0.01, ma: -26.927246652193244\n",
      "Episode: 265, reward: -17, epsilon: 0.01, ma: -25.93452198697392\n",
      "Episode: 266, reward: -23, epsilon: 0.01, ma: -25.64106978827653\n",
      "Episode: 267, reward: -13, epsilon: 0.01, ma: -24.376962809448877\n",
      "Episode: 268, reward: -13, epsilon: 0.01, ma: -23.23926652850399\n",
      "Episode: 269, reward: -21, epsilon: 0.01, ma: -23.01533987565359\n",
      "Episode: 270, reward: -13, epsilon: 0.01, ma: -22.013805888088232\n",
      "Episode: 271, reward: -15, epsilon: 0.01, ma: -21.312425299279408\n",
      "Episode: 272, reward: -31, epsilon: 0.01, ma: -22.281182769351467\n",
      "Episode: 273, reward: -13, epsilon: 0.01, ma: -21.353064492416323\n",
      "Episode: 274, reward: -13, epsilon: 0.01, ma: -20.51775804317469\n",
      "Episode: 275, reward: -21, epsilon: 0.01, ma: -20.565982238857224\n",
      "Episode: 276, reward: -14, epsilon: 0.01, ma: -19.9093840149715\n",
      "Episode: 277, reward: -13, epsilon: 0.01, ma: -19.21844561347435\n",
      "Episode: 278, reward: -13, epsilon: 0.01, ma: -18.596601052126918\n",
      "Episode: 279, reward: -26, epsilon: 0.01, ma: -19.33694094691423\n",
      "Episode: 280, reward: -13, epsilon: 0.01, ma: -18.703246852222808\n",
      "Episode: 281, reward: -13, epsilon: 0.01, ma: -18.13292216700053\n",
      "Episode: 282, reward: -14, epsilon: 0.01, ma: -17.719629950300476\n",
      "Episode: 283, reward: -25, epsilon: 0.01, ma: -18.447666955270428\n",
      "Episode: 284, reward: -17, epsilon: 0.01, ma: -18.302900259743385\n",
      "Episode: 285, reward: -13, epsilon: 0.01, ma: -17.772610233769047\n",
      "Episode: 286, reward: -18, epsilon: 0.01, ma: -17.795349210392143\n",
      "Episode: 287, reward: -18, epsilon: 0.01, ma: -17.81581428935293\n",
      "Episode: 288, reward: -13, epsilon: 0.01, ma: -17.334232860417636\n",
      "Episode: 289, reward: -15, epsilon: 0.01, ma: -17.100809574375873\n",
      "Episode: 290, reward: -13, epsilon: 0.01, ma: -16.690728616938287\n",
      "Episode: 291, reward: -15, epsilon: 0.01, ma: -16.52165575524446\n",
      "Episode: 292, reward: -15, epsilon: 0.01, ma: -16.369490179720014\n",
      "Episode: 293, reward: -16, epsilon: 0.01, ma: -16.332541161748015\n",
      "Episode: 294, reward: -13, epsilon: 0.01, ma: -15.999287045573215\n",
      "Episode: 295, reward: -15, epsilon: 0.01, ma: -15.899358341015894\n",
      "Episode: 296, reward: -22, epsilon: 0.01, ma: -16.509422506914305\n",
      "Episode: 297, reward: -13, epsilon: 0.01, ma: -16.158480256222873\n",
      "Episode: 298, reward: -14, epsilon: 0.01, ma: -15.942632230600587\n",
      "Episode: 299, reward: -14, epsilon: 0.01, ma: -15.748369007540528\n",
      "Episode: 300, reward: -15, epsilon: 0.01, ma: -15.673532106786476\n",
      "Episode: 301, reward: -13, epsilon: 0.01, ma: -15.40617889610783\n",
      "Episode: 302, reward: -17, epsilon: 0.01, ma: -15.565561006497049\n",
      "Episode: 303, reward: -13, epsilon: 0.01, ma: -15.309004905847345\n",
      "Episode: 304, reward: -13, epsilon: 0.01, ma: -15.078104415262612\n",
      "Episode: 305, reward: -13, epsilon: 0.01, ma: -14.870293973736352\n",
      "Episode: 306, reward: -13, epsilon: 0.01, ma: -14.683264576362719\n",
      "Episode: 307, reward: -13, epsilon: 0.01, ma: -14.514938118726448\n",
      "Episode: 308, reward: -14, epsilon: 0.01, ma: -14.463444306853804\n",
      "Episode: 309, reward: -13, epsilon: 0.01, ma: -14.317099876168424\n",
      "Episode: 310, reward: -30, epsilon: 0.01, ma: -15.885389888551583\n",
      "Episode: 311, reward: -13, epsilon: 0.01, ma: -15.596850899696426\n",
      "Episode: 312, reward: -13, epsilon: 0.01, ma: -15.337165809726784\n",
      "Episode: 313, reward: -13, epsilon: 0.01, ma: -15.103449228754107\n",
      "Episode: 314, reward: -13, epsilon: 0.01, ma: -14.893104305878698\n",
      "Episode: 315, reward: -13, epsilon: 0.01, ma: -14.70379387529083\n",
      "Episode: 316, reward: -13, epsilon: 0.01, ma: -14.533414487761748\n",
      "Episode: 317, reward: -13, epsilon: 0.01, ma: -14.380073038985573\n",
      "Episode: 318, reward: -14, epsilon: 0.01, ma: -14.342065735087017\n",
      "Episode: 319, reward: -13, epsilon: 0.01, ma: -14.207859161578316\n",
      "Episode: 320, reward: -18, epsilon: 0.01, ma: -14.587073245420486\n",
      "Episode: 321, reward: -19, epsilon: 0.01, ma: -15.028365920878437\n",
      "Episode: 322, reward: -15, epsilon: 0.01, ma: -15.025529328790594\n",
      "Episode: 323, reward: -13, epsilon: 0.01, ma: -14.822976395911535\n",
      "Episode: 324, reward: -13, epsilon: 0.01, ma: -14.640678756320384\n",
      "Episode: 325, reward: -13, epsilon: 0.01, ma: -14.476610880688346\n",
      "Episode: 326, reward: -14, epsilon: 0.01, ma: -14.428949792619512\n",
      "Episode: 327, reward: -13, epsilon: 0.01, ma: -14.286054813357563\n",
      "Episode: 328, reward: -17, epsilon: 0.01, ma: -14.557449332021807\n",
      "Episode: 329, reward: -13, epsilon: 0.01, ma: -14.401704398819628\n",
      "Episode: 330, reward: -13, epsilon: 0.01, ma: -14.261533958937667\n",
      "Episode: 331, reward: -13, epsilon: 0.01, ma: -14.135380563043901\n",
      "Episode: 332, reward: -13, epsilon: 0.01, ma: -14.021842506739512\n",
      "Episode: 333, reward: -13, epsilon: 0.01, ma: -13.919658256065562\n",
      "Episode: 334, reward: -13, epsilon: 0.01, ma: -13.827692430459006\n",
      "Episode: 335, reward: -22, epsilon: 0.01, ma: -14.644923187413106\n",
      "Episode: 336, reward: -13, epsilon: 0.01, ma: -14.480430868671796\n",
      "Episode: 337, reward: -13, epsilon: 0.01, ma: -14.332387781804618\n",
      "Episode: 338, reward: -15, epsilon: 0.01, ma: -14.399149003624157\n",
      "Episode: 339, reward: -13, epsilon: 0.01, ma: -14.259234103261742\n",
      "Episode: 340, reward: -15, epsilon: 0.01, ma: -14.333310692935568\n",
      "Episode: 341, reward: -13, epsilon: 0.01, ma: -14.199979623642012\n",
      "Episode: 342, reward: -17, epsilon: 0.01, ma: -14.479981661277812\n",
      "Episode: 343, reward: -13, epsilon: 0.01, ma: -14.331983495150032\n",
      "Episode: 344, reward: -13, epsilon: 0.01, ma: -14.19878514563503\n",
      "Episode: 345, reward: -13, epsilon: 0.01, ma: -14.078906631071527\n",
      "Episode: 346, reward: -13, epsilon: 0.01, ma: -13.971015967964375\n",
      "Episode: 347, reward: -13, epsilon: 0.01, ma: -13.873914371167938\n",
      "Episode: 348, reward: -13, epsilon: 0.01, ma: -13.786522934051145\n",
      "Episode: 349, reward: -13, epsilon: 0.01, ma: -13.707870640646032\n",
      "Episode: 350, reward: -13, epsilon: 0.01, ma: -13.63708357658143\n",
      "Episode: 351, reward: -17, epsilon: 0.01, ma: -13.973375218923287\n",
      "Episode: 352, reward: -13, epsilon: 0.01, ma: -13.87603769703096\n",
      "Episode: 353, reward: -13, epsilon: 0.01, ma: -13.788433927327864\n",
      "Episode: 354, reward: -13, epsilon: 0.01, ma: -13.709590534595078\n",
      "Episode: 355, reward: -13, epsilon: 0.01, ma: -13.63863148113557\n",
      "Episode: 356, reward: -13, epsilon: 0.01, ma: -13.574768333022014\n",
      "Episode: 357, reward: -15, epsilon: 0.01, ma: -13.717291499719813\n",
      "Episode: 358, reward: -13, epsilon: 0.01, ma: -13.645562349747832\n",
      "Episode: 359, reward: -13, epsilon: 0.01, ma: -13.58100611477305\n",
      "Episode: 360, reward: -13, epsilon: 0.01, ma: -13.522905503295746\n",
      "Episode: 361, reward: -13, epsilon: 0.01, ma: -13.470614952966173\n",
      "Episode: 362, reward: -13, epsilon: 0.01, ma: -13.423553457669557\n",
      "Episode: 363, reward: -13, epsilon: 0.01, ma: -13.381198111902602\n",
      "Episode: 364, reward: -13, epsilon: 0.01, ma: -13.343078300712342\n",
      "Episode: 365, reward: -13, epsilon: 0.01, ma: -13.30877047064111\n",
      "Episode: 366, reward: -13, epsilon: 0.01, ma: -13.277893423577\n",
      "Episode: 367, reward: -13, epsilon: 0.01, ma: -13.2501040812193\n",
      "Episode: 368, reward: -13, epsilon: 0.01, ma: -13.225093673097371\n",
      "Episode: 369, reward: -13, epsilon: 0.01, ma: -13.202584305787635\n",
      "Episode: 370, reward: -13, epsilon: 0.01, ma: -13.182325875208873\n",
      "Episode: 371, reward: -13, epsilon: 0.01, ma: -13.164093287687987\n",
      "Episode: 372, reward: -13, epsilon: 0.01, ma: -13.147683958919188\n",
      "Episode: 373, reward: -13, epsilon: 0.01, ma: -13.13291556302727\n",
      "Episode: 374, reward: -14, epsilon: 0.01, ma: -13.219624006724544\n",
      "Episode: 375, reward: -13, epsilon: 0.01, ma: -13.19766160605209\n",
      "Episode: 376, reward: -13, epsilon: 0.01, ma: -13.177895445446882\n",
      "Episode: 377, reward: -13, epsilon: 0.01, ma: -13.160105900902195\n",
      "Episode: 378, reward: -13, epsilon: 0.01, ma: -13.144095310811977\n",
      "Episode: 379, reward: -13, epsilon: 0.01, ma: -13.12968577973078\n",
      "Episode: 380, reward: -13, epsilon: 0.01, ma: -13.116717201757703\n",
      "Episode: 381, reward: -13, epsilon: 0.01, ma: -13.105045481581934\n",
      "Episode: 382, reward: -13, epsilon: 0.01, ma: -13.094540933423742\n",
      "Episode: 383, reward: -13, epsilon: 0.01, ma: -13.085086840081368\n",
      "Episode: 384, reward: -19, epsilon: 0.01, ma: -13.676578156073232\n",
      "Episode: 385, reward: -13, epsilon: 0.01, ma: -13.60892034046591\n",
      "Episode: 386, reward: -13, epsilon: 0.01, ma: -13.548028306419319\n",
      "Episode: 387, reward: -13, epsilon: 0.01, ma: -13.493225475777388\n",
      "Episode: 388, reward: -120, epsilon: 0.01, ma: -24.14390292819965\n",
      "Episode: 389, reward: -13, epsilon: 0.01, ma: -23.02951263537969\n",
      "Episode: 390, reward: -13, epsilon: 0.01, ma: -22.026561371841723\n",
      "Episode: 391, reward: -13, epsilon: 0.01, ma: -21.123905234657553\n",
      "Episode: 392, reward: -13, epsilon: 0.01, ma: -20.311514711191798\n",
      "Episode: 393, reward: -13, epsilon: 0.01, ma: -19.580363240072618\n",
      "Episode: 394, reward: -13, epsilon: 0.01, ma: -18.92232691606536\n",
      "Episode: 395, reward: -13, epsilon: 0.01, ma: -18.330094224458826\n",
      "Episode: 396, reward: -119, epsilon: 0.01, ma: -28.39708480201294\n",
      "Episode: 397, reward: -13, epsilon: 0.01, ma: -26.85737632181165\n",
      "Episode: 398, reward: -13, epsilon: 0.01, ma: -25.471638689630485\n",
      "Episode: 399, reward: -13, epsilon: 0.01, ma: -24.22447482066744\n"
     ]
    }
   ],
   "source": [
    "episode = 400\n",
    "rewards = []\n",
    "ma_rewards = []\n",
    "for i in range(episode):\n",
    "    ep_reward = 0\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        agent.qlearn(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(ep_reward)\n",
    "    if ma_rewards:\n",
    "        ma_rewards.append(ma_rewards[-1]*0.9+ep_reward*0.1)\n",
    "    else :\n",
    "        ma_rewards.append(ep_reward)\n",
    "    print(\"Episode: {}, reward: {}, epsilon: {}, ma: {}\".format(i,ep_reward,agent.epsilon,ma_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.81727263e+00, -6.82303976e+00, -6.84089944e+00,\n",
       "        -6.83580475e+00],\n",
       "       [-6.67696820e+00, -6.64951068e+00, -6.68688604e+00,\n",
       "        -6.65207030e+00],\n",
       "       [-6.41489390e+00, -6.41361422e+00, -6.43292449e+00,\n",
       "        -6.42680430e+00],\n",
       "       [-6.15451645e+00, -6.13697066e+00, -6.16192968e+00,\n",
       "        -6.15802587e+00],\n",
       "       [-5.83962153e+00, -5.82556983e+00, -5.82538417e+00,\n",
       "        -5.86570196e+00],\n",
       "       [-5.47772527e+00, -5.48240972e+00, -5.47904396e+00,\n",
       "        -5.48963860e+00],\n",
       "       [-5.08144824e+00, -5.08498776e+00, -5.11963128e+00,\n",
       "        -5.15465335e+00],\n",
       "       [-4.69958578e+00, -4.66589370e+00, -4.65973157e+00,\n",
       "        -4.69612964e+00],\n",
       "       [-4.19172998e+00, -4.20218981e+00, -4.21274136e+00,\n",
       "        -4.23822864e+00],\n",
       "       [-3.71162809e+00, -3.70573302e+00, -3.69913480e+00,\n",
       "        -3.72432403e+00],\n",
       "       [-3.21000421e+00, -3.16447482e+00, -3.17091674e+00,\n",
       "        -3.23551514e+00],\n",
       "       [-2.65362525e+00, -2.65688320e+00, -2.62482864e+00,\n",
       "        -2.63267828e+00],\n",
       "       [-6.96937553e+00, -6.95269578e+00, -7.00059729e+00,\n",
       "        -6.97847676e+00],\n",
       "       [-6.74846805e+00, -6.73492435e+00, -6.73493035e+00,\n",
       "        -6.76891357e+00],\n",
       "       [-6.45965202e+00, -6.46442715e+00, -6.48869571e+00,\n",
       "        -6.48427258e+00],\n",
       "       [-6.16243768e+00, -6.15422448e+00, -6.14725024e+00,\n",
       "        -6.14988735e+00],\n",
       "       [-5.84157571e+00, -5.79836953e+00, -5.79933987e+00,\n",
       "        -5.81803449e+00],\n",
       "       [-5.44189914e+00, -5.40869158e+00, -5.43011605e+00,\n",
       "        -5.40645253e+00],\n",
       "       [-5.02911999e+00, -4.98585705e+00, -4.98326975e+00,\n",
       "        -5.05679032e+00],\n",
       "       [-4.55744368e+00, -4.49216427e+00, -4.51014607e+00,\n",
       "        -4.57194811e+00],\n",
       "       [-4.03228311e+00, -3.95213970e+00, -3.96211197e+00,\n",
       "        -4.03541862e+00],\n",
       "       [-3.42213233e+00, -3.35225825e+00, -3.35390498e+00,\n",
       "        -3.37318403e+00],\n",
       "       [-2.76675041e+00, -2.67306839e+00, -2.67081621e+00,\n",
       "        -2.79167304e+00],\n",
       "       [-2.03780624e+00, -1.94378775e+00, -1.89907250e+00,\n",
       "        -1.90111096e+00],\n",
       "       [-7.18191186e+00, -7.17564727e+00, -7.29356401e+00,\n",
       "        -7.19287158e+00],\n",
       "       [-6.87252430e+00, -6.86187568e+00, -1.85320554e+01,\n",
       "        -6.94066360e+00],\n",
       "       [-6.53605259e+00, -6.51321075e+00, -4.24638446e+01,\n",
       "        -6.55801845e+00],\n",
       "       [-6.12691864e+00, -6.12579407e+00, -4.17708196e+01,\n",
       "        -6.13624444e+00],\n",
       "       [-5.73788696e+00, -5.69532772e+00, -2.75608292e+01,\n",
       "        -5.74790438e+00],\n",
       "       [-5.23497770e+00, -5.21703098e+00, -4.12078007e+01,\n",
       "        -5.24092573e+00],\n",
       "       [-4.88118948e+00, -4.68559000e+00, -3.51592383e+01,\n",
       "        -4.78078009e+00],\n",
       "       [-4.09862506e+00, -4.09510000e+00, -2.79338044e+01,\n",
       "        -4.10018908e+00],\n",
       "       [-3.55956707e+00, -3.43900000e+00, -4.29187503e+01,\n",
       "        -3.45592436e+00],\n",
       "       [-2.86106676e+00, -2.71000000e+00, -1.95252509e+01,\n",
       "        -2.79158709e+00],\n",
       "       [-2.03832614e+00, -1.90000000e+00, -9.56424097e+00,\n",
       "        -2.08249746e+00],\n",
       "       [-1.36748631e+00, -1.08402813e+00, -1.00000000e+00,\n",
       "        -1.19489671e+00],\n",
       "       [-7.45798300e+00, -6.60931911e+01, -7.47061033e+00,\n",
       "        -7.48833657e+00],\n",
       "       [ 4.31906636e-01,  7.12656397e-01,  8.39782829e-01,\n",
       "         7.57123199e-01],\n",
       "       [ 8.48992172e-01,  6.64893983e-01,  6.36923438e-01,\n",
       "         6.93678039e-01],\n",
       "       [ 3.28333388e-01,  5.46316824e-01,  5.69974043e-01,\n",
       "         9.67241202e-01],\n",
       "       [ 3.31801822e-01,  1.21230923e-01,  2.39923141e-01,\n",
       "         3.64274094e-01],\n",
       "       [ 1.24355801e-03,  3.33353418e-01,  6.63302260e-01,\n",
       "         4.04719753e-01],\n",
       "       [ 9.82155680e-01,  9.57378039e-01,  3.44811008e-03,\n",
       "         4.40453184e-01],\n",
       "       [ 5.90458696e-01,  6.96732757e-01,  7.13335930e-01,\n",
       "         3.11132957e-01],\n",
       "       [ 7.99729691e-01,  3.28875694e-01,  3.66644906e-01,\n",
       "         6.67337707e-01],\n",
       "       [ 4.05482175e-01,  6.46708318e-01,  1.22139541e-01,\n",
       "         3.82709363e-01],\n",
       "       [ 6.83360350e-01,  7.75452350e-01,  7.60409820e-01,\n",
       "         6.66505942e-01],\n",
       "       [ 3.83396513e-01,  6.33883437e-01,  3.46390905e-01,\n",
       "         7.68218136e-01]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看路径："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 up\n",
      "25 right\n",
      "26 right\n",
      "27 right\n",
      "28 right\n",
      "29 right\n",
      "30 right\n",
      "31 right\n",
      "32 right\n",
      "33 right\n",
      "34 right\n",
      "35 down\n"
     ]
    }
   ],
   "source": [
    "action_lang=[\"up\",\"right\",\"down\",\"left\"]\n",
    "print(36,action_lang[np.argmax(agent.Q[36,:])])\n",
    "for i in range(25,36):\n",
    "    print(i,action_lang[np.argmax(agent.Q[i,:])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dailly38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
