{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EazyRL_v1.0.6 笔记\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 绪论\n",
    "> 9/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强化学习 RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 机器学习的一个分支\n",
    "- 研究智能主体在环境中如何采取行动以最大化累积奖励\n",
    "- 基本思想是通过智能体（Agent）与环境（Environment）的**交互**来**学习最优策略**，使得智能体能够采取一系列**动作**以**获取最大的长期奖励**\n",
    "- 目标是寻找最优策略\n",
    "\n",
    "![](./img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 产生 action 作用于 env\n",
    "\n",
    "env 根据 action 产生 reward 和 next_state\n",
    "\n",
    "agent 根据 reward 和 next_state 进行更新\n",
    "\n",
    "循环上述过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**与监督学习对比**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督学习：\n",
    "- 样本互不相关，是独立同分布的\n",
    "- 根据监督者给出的 label 确定正误\n",
    "- 通过反向传播更新模型参数，实现学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习：\n",
    "- 输入的是序列数据，数据之间有很强的关联\n",
    "- 不给出正确的动作，需要学习器不断尝试\n",
    "- 延迟奖励：奖励信号不是立即给出的，而是延迟一段时间\n",
    "- 动作会影响后面的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度强化学习：省去特征提取，实现端到端"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 序列决策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在与环境的交互过程中，智能体会获得很多观测\n",
    "\n",
    "每一个观测 - 采取一个动作 - 得到一个奖励\n",
    "\n",
    "**历史是观测 动作 奖励的序列**\n",
    "\n",
    "智能体采取**当前动作** 依赖于 **得到的历史**\n",
    "\n",
    "**状态是历史的函数**\n",
    "\n",
    "基本问题：**学习与规划**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体能够观察环境所有状态（上帝视角） - 完全可观测的环境 - 马尔可夫决策过程 MDP\n",
    "\n",
    "智能体只能看到部分的观测状态（有限信息） - 部分可观测的环境 - 部分可观测马尔可夫决策过程 POMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动作空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定环境中 有效动作的集合\n",
    "\n",
    "- 离散动作空间：动作数量有限，如围棋\n",
    "- 连续动作空间：动作是实值的向量，如智能驾驶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 智能体组成\n",
    "\n",
    "> 策略，价值函数，模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **策略**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入状态到动作的函数\n",
    "\n",
    "- 随机性策略：概率分布采样\n",
    "- 确定性策略：直接选择最有可能的动作（可能性最大的动作）\n",
    "\n",
    "通常采用随机性策略，避免出现局部最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **价值函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数值是对**未来奖励**的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**折扣因子**\n",
    "\n",
    "折扣因子γ（gamma） 反映了决策者对未来收益相对于当前收益的重视程度。\n",
    "\n",
    "折扣因子越大，表示决策者越看重未来的收益；\n",
    "\n",
    "折扣因子越小，表示决策者越倾向于即时满足，即更看重当前的收益。\n",
    "\n",
    "\n",
    "\n",
    "![](./img/2.png)\n",
    "\n",
    "pi策略 gamma折扣因子 r回报 s状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 函数**\n",
    "\n",
    "包含 状态 和 动作 两个变量，未来可以获得奖励的期望取决于当前的状态和当前的动作。\n",
    "\n",
    "![](./img/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 状态转移概率\n",
    "- 奖励函数：当前状态采取某个动作可以得到的奖励（价值函数是预测）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 智能体类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **基于价值 / 基于策略**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于价值：学习价值函数 （QL,Sarsa）\n",
    "- 基于策略：学习策略，通过状态给出动作概率 (PG)\n",
    "- 结合两者：演员（策略）- 评论员（价值）智能体\n",
    "\n",
    "基于价值迭代的方法只能应用在不连续的、离散的环境下，而基于策略迭代的方法可以应用在连续的环境中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **有模型 / 免模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是否需要对真实环境进行建模：\n",
    "\n",
    "- 有模型：学习状态转移来采取动作，如状态转移和奖励函数都是**已知**的，能知道某一状态采取某一决策后的奖励和状态（动规问题）\n",
    "- 免模型：学习价值函数和策略函数进行决策，没有环境转移函数和奖励函数。没有对真实环境进行建模，智能体只能在真实环境中通过一定的策略来执行动作，**等待**奖励和状态迁移，然后根据这些反馈信息来**更新**动作策略，这样反复迭代直到学习到最优策略。\n",
    "\n",
    "有模型同时在真实环境与虚拟世界学习\n",
    "\n",
    "免模型直接与真实环境交互来学习 一般需要大量数据样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索与利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单步强化学习任务 - K-臂赌博机\n",
    "\n",
    "- 仅探索：机会平均分配，最后计算期望\n",
    "- 仅利用：贪心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CartPole-v0 环境**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info, others = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态信息 observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.2427716, 3.7990518, 6.626667 , 4.7200627], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奖励 reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "游戏是否完成了 done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调试信息 info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`step()` 完成了一个完整的 S-A-R-S' 迭代过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看 gym 库中的环境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CartPole-v0',\n",
       " 'CartPole-v1',\n",
       " 'MountainCar-v0',\n",
       " 'MountainCarContinuous-v0',\n",
       " 'Pendulum-v1',\n",
       " 'Acrobot-v1',\n",
       " 'LunarLander-v2',\n",
       " 'LunarLanderContinuous-v2',\n",
       " 'BipedalWalker-v3',\n",
       " 'BipedalWalkerHardcore-v3',\n",
       " 'CarRacing-v2',\n",
       " 'Blackjack-v1',\n",
       " 'FrozenLake-v1',\n",
       " 'FrozenLake8x8-v1',\n",
       " 'CliffWalking-v0',\n",
       " 'Taxi-v3',\n",
       " 'Reacher-v2',\n",
       " 'Reacher-v4',\n",
       " 'Pusher-v2',\n",
       " 'Pusher-v4',\n",
       " 'InvertedPendulum-v2',\n",
       " 'InvertedPendulum-v4',\n",
       " 'InvertedDoublePendulum-v2',\n",
       " 'InvertedDoublePendulum-v4',\n",
       " 'HalfCheetah-v2',\n",
       " 'HalfCheetah-v3',\n",
       " 'HalfCheetah-v4',\n",
       " 'Hopper-v2',\n",
       " 'Hopper-v3',\n",
       " 'Hopper-v4',\n",
       " 'Swimmer-v2',\n",
       " 'Swimmer-v3',\n",
       " 'Swimmer-v4',\n",
       " 'Walker2d-v2',\n",
       " 'Walker2d-v3',\n",
       " 'Walker2d-v4',\n",
       " 'Ant-v2',\n",
       " 'Ant-v3',\n",
       " 'Ant-v4',\n",
       " 'Humanoid-v2',\n",
       " 'Humanoid-v3',\n",
       " 'Humanoid-v4',\n",
       " 'HumanoidStandup-v2',\n",
       " 'HumanoidStandup-v4']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym import envs\n",
    "\n",
    "specs = envs.registry.values() # 原书代码有误\n",
    "ids = [spec.id for spec in specs]\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 与 `gym` 库交互"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例：小车上山"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<MountainCarEnv<MountainCar-v0>>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观测空间：连续空间用 `gym.spaces.Box` 类 表示范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作空间：离散空间用 `gym.spaces.Discrete` 类 表示可取的数量，即动作数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现智能体，控制小车移动："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "    def decide(self, observation):\n",
    "        pos,vel = observation\n",
    "        lb = min( -0.09 * (pos + 0.25) ** 2 + 0.03,\n",
    "                0.3 * (pos + 0.9) ** 4 - 0.008 )\n",
    "        ub = -0.07 * (pos + 0.38) ** 2 + 0.07\n",
    "        if lb < vel < ub:\n",
    "            return  2\n",
    "        else: return 0\n",
    "    def learn(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SimpleAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与环境交互："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env,agent,render=False,train=False):\n",
    "    episode_reward = 0 # 回合总奖励\n",
    "    observation = env.reset() \n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.decide(observation)\n",
    "        next_obs,reward,done,_ = env.step(action) \n",
    "        episode_reward += reward\n",
    "        if train:\n",
    "            agent.learn(observation,action,reward,done)\n",
    "        if done:\n",
    "            break\n",
    "        observation = next_obs\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交互一回合：\n",
    "> !TODO notebook 中无法显示 gym 窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-105.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.seed(7) # 随机种子 v0.26 已经去掉了这个方法\n",
    "episode_reward= play(env,agent)\n",
    "episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估智能体的性能，需要计算出连续交互 100 回合的平均回合奖励："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-106.13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_rewards = [ play(env,agent) for _ in range(100) ]\n",
    "import numpy as np\n",
    "np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **总结**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('') # 取环境\n",
    "obs = env.reset()  # 初始化环境\n",
    "action = 1         # 随机动作\n",
    "next_obs,reward,done,info = env.step(action)   # 执行动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**习题**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 强化学习基本结构？**\n",
    "\n",
    "agent, environment, reward, value function, policy/action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 比监督学习训练过程困难的原因**\n",
    "\n",
    "延迟奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 基本特征**\n",
    "\n",
    "exploration, exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. 状态和观测？**\n",
    "\n",
    "状态：客观世界中事物的属性，是事物内部特征的集合，是全集\n",
    "\n",
    "观测：是状态的一种表现，是外界对状态的一种观察，是子集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. 智能体组成**\n",
    "\n",
    "policy, value function, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. 分类？**\n",
    "\n",
    "policy_based, value_based\n",
    "\n",
    "model_free, model_based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. 基于策略迭代和基于价值迭代的强化学习方法有什么区别**\n",
    "\n",
    "策略：通过策略给出概率，连续的环境\n",
    "\n",
    "价值：学习价值函数，离散的环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. 有模型学习和免模型学习有什么区别**\n",
    "\n",
    "有：奖励函数 / 价值函数 都是明确的，可以建立清晰的模型\n",
    "\n",
    "无：现实情况，无需模型。通过智能体与环境的交互来学习。需要较大的数据样本量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. 强化学习？**\n",
    "\n",
    "通过智能体与环境的不断交互，优化调整策略，以期达到最优奖励\n",
    "\n",
    "输入是一串序列，样本之间具有相关性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 马尔可夫决策过程\n",
    "\n",
    "> 9/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体与环境的交互过程 可以用 马尔可夫决策过程 表示，马尔可夫决策过程是强化学习的基本框架\n",
    "\n",
    "在马尔可夫决策过程中，它的环境是**全部可观测**的。\n",
    "\n",
    "但是很多时候环境里面有些量是不可观测的，但是这个部分观测的问题也可以转换成马尔可夫决策过程的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫过程 MP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **马尔可夫性质**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机过程 在某一时刻的状态 只与它前一个时刻的状态有关，而与它之前所有状态都无关的性质\n",
    "\n",
    "即 **未来的转移与过去的是独立的，无记忆性**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **马尔可夫链**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "马尔可夫过程：一组具有马尔可夫性质的随机变量序列\n",
    "\n",
    "马尔可夫链：离散时间的马尔可夫过程，状态是有限的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用状态转移矩阵来描述状态转移\n",
    "\n",
    "`a[i][j]` 表示 状态 `si` 到 状态 `sj` 的转移概率\n",
    "\n",
    "![](./img/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫奖励过程 MRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "马尔可夫链 + 奖励函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **回报和价值函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回报 return** 时刻t的回报是当前获得的所有奖励的和 (单个轨迹)\n",
    "\n",
    "**折扣回报 discounted return** 具有折扣因子的回报函数，折扣因子作为超参数，可以调整出不同偏好（长短期收益）下的 agent\n",
    "\n",
    "**价值函数** 状态价值函数：在某状态下，agent能获得的回报期望 v(pi,s) （所有轨迹计算后的期望）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **贝尔曼方程**\n",
    "\n",
    "> [推导](https://www.bilibili.com/video/BV1sd4y167NS?p=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单地说就是： 状态价值 = 即时奖励 + 未来奖励的折扣和， state value = immediate reward + sum( discount factor * next state value )\n",
    "\n",
    "![](./img/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展开来看，对于指定的策略，每个状态下的价值函数：\n",
    "\n",
    "![](./img/bellmaneq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多个状态的价值函数联立，就能得到状态转移概率矩阵：\n",
    "\n",
    "![](./img/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **计算状态价值**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学的方法需要求矩阵的逆，对于大型矩阵比较困难。可以通过迭代方法来求解。\n",
    "\n",
    "- 动态规划 bootstrapping方法\n",
    "- 蒙特卡洛\n",
    "- 时序差分学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫决策过程 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在奖励过程上添加了决策条件\n",
    "\n",
    "![](./img/7.png)\n",
    "\n",
    "在马尔可夫决策过程中，动作是由智能体决定的，智能体会采取动作来决定未来的状态转移"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q 函数 （动作价值函数）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即添加了 决策条件 后的状态价值函数，通过某一状态下采取某一动作，得出回报的期望\n",
    "\n",
    "![](./img/8.png)\n",
    "\n",
    "该状态下 每个决策的Q函数值 的加权平均就是 该状态的状态价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于 Q 函数的贝尔曼方程同理，只是增加了策略条件。整理得到的就是Q函数的**贝尔曼期望方程**：\n",
    "\n",
    "![](./img/9.png)\n",
    "\n",
    "即： 当前价值 = 期望(即时奖励 + 折扣 * 未来价值)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于某一个状态，它的当前价值是与它的未来价值线性相关的\n",
    "\n",
    "当前价值 = 即时奖励 + 折扣因子 * 未来价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **预测和控制**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测 就是评估决策的价值\n",
    "\n",
    "控制 就是搜索最优决策\n",
    "\n",
    "预测和控制是马尔可夫决策过程的两个重要方面\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过策略迭代和价值迭代来解决马尔可夫决策过程的控制问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **策略迭代**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包括：\n",
    "\n",
    "- **策略评估** 根据当前策略估算价值函数\n",
    "- **策略改进** 得到价值函数后 推算其Q函数，做贪心搜索\n",
    "\n",
    "两个步骤迭代进行，直到收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **价值迭代**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多次迭代贝尔曼最优方程，直到价值函数收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代算法过程：\n",
    "1. 初始化，所有状态的价值都设0\n",
    "2. 重复以下操作，直到所有状态的价值不再变化\n",
    "   - 计算每个状态下每个动作的Q值\n",
    "   - 利用Q值更新状态价值和动作条件（选取最大项）\n",
    "3. 收敛后，提取最优策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 折扣因子作用**\n",
    "\n",
    "超参数 调节智能体远视/近视 视野 避免无穷奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 解析解难求得**\n",
    "\n",
    "需要计算状态转移概率矩阵的逆 大型矩阵的逆很难求得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 贝尔曼方程求解**\n",
    "\n",
    "蒙特卡洛 动态规划 时序差分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 奖励过程与决策过程的区别**\n",
    "\n",
    "奖励过程只根据奖励信号来调整行为，而决策过程通过 agent 的介入具有了决策的自主性，agent 可以根据环境的变化做出相应的调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. 寻找最优策略？**\n",
    "\n",
    "策略迭代 / 价值迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 表格型方法\n",
    "\n",
    "> 9/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是查表\n",
    "\n",
    "- 蒙特卡洛\n",
    "- Q-learning\n",
    "- Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态、动作、状态转移概率和奖励 `(S、A、P 、R)`，这 4 个合集就构成了强化学习马尔可夫决策过程的四元组，后面也可能会再加上折扣因子构成五元组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习用价值函数 `V (S)` 来表示状态是好的还是坏的，\n",
    "\n",
    "用 Q 函数来判断在什么状态下采取什么动作能够取得最大奖励，即用Q 函数来表示状态-动作值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 免模型预测\n",
    "\n",
    "在无法获取环境模型的情况下，通过 **蒙特卡洛方法** 和 **时序差分方法** 估计某个策略的价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **蒙特卡洛策略评估**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采样大量轨迹、计算所有轨迹的真实回报、计算平均值\n",
    "\n",
    "局限性：只能用于 **有终止** 的马尔可夫决策过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**增量式蒙特卡洛方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获得 新的轨迹 后\n",
    "\n",
    "将原本蒙特卡洛的经验均值转换成增量均值\n",
    "\n",
    "用上一时刻的值和当前时刻的增量 更新现在的值\n",
    "\n",
    "![](./img/10.png)\n",
    "\n",
    "- $\\alpha$ 是一个超参数，用来控制更新速度\n",
    "- $G_t$ 代表当前时刻的回报，如 $G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma ^2 r_{t+3} + ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 增量式蒙特卡洛方法只需要一条轨迹就能更新轨迹上的所有状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **时序差分**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态价值迭代：下一个状态会影响上一个状态（例子：巴普洛夫的狗、多级条件反射）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 免模型的\n",
    "- 可以从不完整的回合中学习\n",
    "- 结合了自举的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于给定的策略 $\\pi$ ,一步一步地计算价值函数 $v_{\\pi}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一步时序差分 TD(0)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每往前走一步，就做一步自举，更新上一步的值：\n",
    "\n",
    "![](./img/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 免模型控制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\epsilon -greedy$ 探索**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有 $1-\\epsilon$ 概率按照Q函数来决定动作（取最大值）\n",
    "\n",
    "一般取 $\\epsilon = 0.1$ 在实现上会让 $\\epsilon$ 随时间递减\n",
    "\n",
    "> 最初的时候不知道那个动作较好，引入随机性实现探索；随着探索次数增加，随机性减小，就减少探索，选择最优动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sarsa 同策略时序差分控制**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用时序差分方法 估计 Q函数 (把时序的 V 改成 Q函数)\n",
    "\n",
    "![](./img/12.png)\n",
    "\n",
    "State Action Reward State' Action' SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码实现：\n",
    "- 根据 Q 表格选择动作 （$\\epsilon-greedy$ 策略）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_action(action_space, epsilon, pi, Q, s):\n",
    "    if np.random.rand() < epsilon: # explore\n",
    "        return np.random.choice(action_space,p=pi)\n",
    "    else : #  exploit\n",
    "        return np.nanargmax( Q[s,:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 获取 $s_t,a_t,r_{t+1},s_{t+1},a_{t+1}$ 更新 Q 表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(s,a,r,s_next,a_next,alpha,gamma,Q):\n",
    "    if s_next == 'terminal': # 结束状态\n",
    "        Q[s,a] = Q[s,a] + alpha * (r-Q[s,a])\n",
    "    else :\n",
    "        Q[s,a] = Q[s,a] + alpha * (r + gamma*Q[s_next,a_next]-Q[s,a]) # 时序差分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarsa 优化的是其实际执行的策略，而 Q-learning 优化的是其估计的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q学习 异策略时序差分控制**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 学习的下一个动作都是通过 argmax 函数来选择的\n",
    "\n",
    "![](./img/13.png)\n",
    "\n",
    "二者的更新公式是一样的，区别在于 Q 学习采取的动作是 argmax 选择的动作，Q 学习只需要接收 当前状态 s 和 动作 a 的 Q 值，然后根据 Q 值选择动作即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(s,a,r,s_next,gamma,alpha,Q):\n",
    "    Q[s,a] = Q[s,a] + alpha*(r + gamma*Q[s_next, np.nanargmax(Q[s_next,:]) ] - Q[s,a]) # 不考虑终点状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 学习不需要知道 a_next，其探索更加大胆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 学习解决悬崖寻路问题\n",
    "\n",
    "> 9/11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**环境：`CliffWalking-v0`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 每走一步 收到 -1 奖励\n",
    "- 掉入悬崖 返回起点并收到 -100 奖励\n",
    "- 超出边界 不会移动 但会收到 -1 奖励\n",
    "- 目标是以最少的步数到达终点\n",
    "\n",
    "![](./img/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化环境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import turtle\n",
    "import numpy as np\n",
    "\n",
    "# turtle tutorial : https://docs.python.org/3.3/library/turtle.html\n",
    "\n",
    "class CliffWalkingWapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.t = None\n",
    "        self.unit = 50\n",
    "        self.max_x = 12\n",
    "        self.max_y = 4\n",
    "\n",
    "    def draw_x_line(self, y, x0, x1, color='gray'):\n",
    "        assert x1 > x0\n",
    "        self.t.color(color)\n",
    "        self.t.setheading(0)\n",
    "        self.t.up()\n",
    "        self.t.goto(x0, y)\n",
    "        self.t.down()\n",
    "        self.t.forward(x1 - x0)\n",
    "\n",
    "    def draw_y_line(self, x, y0, y1, color='gray'):\n",
    "        assert y1 > y0\n",
    "        self.t.color(color)\n",
    "        self.t.setheading(90)\n",
    "        self.t.up()\n",
    "        self.t.goto(x, y0)\n",
    "        self.t.down()\n",
    "        self.t.forward(y1 - y0)\n",
    "\n",
    "    def draw_box(self, x, y, fillcolor='', line_color='gray'):\n",
    "        self.t.up()\n",
    "        self.t.goto(x * self.unit, y * self.unit)\n",
    "        self.t.color(line_color)\n",
    "        self.t.fillcolor(fillcolor)\n",
    "        self.t.setheading(90)\n",
    "        self.t.down()\n",
    "        self.t.begin_fill()\n",
    "        for i in range(4):\n",
    "            self.t.forward(self.unit)\n",
    "            self.t.right(90)\n",
    "        self.t.end_fill()\n",
    "\n",
    "    def move_player(self, x, y):\n",
    "        self.t.up()\n",
    "        self.t.setheading(90)\n",
    "        self.t.fillcolor('red')\n",
    "        self.t.goto((x + 0.5) * self.unit, (y + 0.5) * self.unit)\n",
    "\n",
    "    def render(self):\n",
    "        if self.t == None:\n",
    "            self.t = turtle.Turtle()\n",
    "            self.wn = turtle.Screen()\n",
    "            self.wn.setup(self.unit * self.max_x + 100,\n",
    "                          self.unit * self.max_y + 100)\n",
    "            self.wn.setworldcoordinates(0, 0, self.unit * self.max_x,\n",
    "                                        self.unit * self.max_y)\n",
    "            self.t.shape('circle')\n",
    "            self.t.width(2)\n",
    "            self.t.speed(0)\n",
    "            self.t.color('gray')\n",
    "            for _ in range(2):\n",
    "                self.t.forward(self.max_x * self.unit)\n",
    "                self.t.left(90)\n",
    "                self.t.forward(self.max_y * self.unit)\n",
    "                self.t.left(90)\n",
    "            for i in range(1, self.max_y):\n",
    "                self.draw_x_line(\n",
    "                    y=i * self.unit, x0=0, x1=self.max_x * self.unit)\n",
    "            for i in range(1, self.max_x):\n",
    "                self.draw_y_line(\n",
    "                    x=i * self.unit, y0=0, y1=self.max_y * self.unit)\n",
    "\n",
    "            for i in range(1, self.max_x - 1):\n",
    "                self.draw_box(i, 0, 'black')\n",
    "            self.draw_box(self.max_x - 1, 0, 'yellow')\n",
    "            self.t.shape('turtle')\n",
    "\n",
    "        x_pos = self.s % self.max_x\n",
    "        y_pos = self.max_y - 1 - int(self.s / self.max_x)\n",
    "        self.move_player(x_pos, y_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 4)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env_name = 'CliffWalking-v0'\n",
    "env = gym.make(env_name)\n",
    "env = CliffWalkingWapper(env)\n",
    "env.seed(1)\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "num_states, num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "a = env.action_space.sample()\n",
    "\n",
    "s,a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "环境初始化状态："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **强化学习基本过程**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 初始化 agent env\n",
    "2. agent.get_action(state)\n",
    "3. 执行 action , 返回 reward, new_state, done\n",
    "4. 学习，即进行策略更新\n",
    "5. 循环 2-4 直到算法收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**如何确定算法收敛：滑动平均奖励**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "滑动平均量 反映了奖励变化的趋势："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.9500000000000001,\n",
       " 0.8883333343267442,\n",
       " 0.8245000008940698,\n",
       " 0.7620500011026861,\n",
       " 0.7025116681557896,\n",
       " 0.6465462162645461,\n",
       " 0.5943915946380914,\n",
       " 0.5460635463681776,\n",
       " 0.5014571918803714,\n",
       " 0.4604023820541736,\n",
       " 0.4226954774304423,\n",
       " 0.3881182376662666,\n",
       " 0.35644927136180765,\n",
       " 0.3274710112399873,\n",
       " 0.30097391011598856,\n",
       " 0.2767588720674796,\n",
       " 0.2546385404576793,\n",
       " 0.23443784434586182,\n",
       " 0.21599405998578144,\n",
       " 0.19915655883780545,\n",
       " 0.18378635763494455,\n",
       " 0.16975554803939122,\n",
       " 0.15694666002629512,\n",
       " 0.14525199393425864,\n",
       " 0.13457294853026702,\n",
       " 0.12481935740853876,\n",
       " 0.11590885039876876,\n",
       " 0.10776624120811502,\n",
       " 0.10032295059448373,\n",
       " 0.09351646189051174,\n",
       " 0.08728981570146058,\n",
       " 0.08159113725192763,\n",
       " 0.07637320000827984,\n",
       " 0.07159302288056024,\n",
       " 0.06721149839097806,\n",
       " 0.06319305132002725,\n",
       " 0.05950532515499974,\n",
       " 0.05611889523703443,\n",
       " 0.05300700575058389,\n",
       " 0.05014532947490866,\n",
       " 0.047511748952718875,\n",
       " 0.04508615544413236,\n",
       " 0.042850267240178944,\n",
       " 0.04078746279219302,\n",
       " 0.03888262959694427,\n",
       " 0.037122026144345656,\n",
       " 0.0354931569253326,\n",
       " 0.03398465751751547,\n",
       " 0.03258619172106044,\n",
       " 0.031288356932072554,\n",
       " 0.03008259823358242,\n",
       " 0.028961130891169765,\n",
       " 0.027916869667702012,\n",
       " 0.026943364461540963,\n",
       " 0.026034742380928802,\n",
       " 0.025185654120819398,\n",
       " 0.024391226633349028,\n",
       " 0.023647019227408438,\n",
       " 0.022948984058257703,\n",
       " 0.022293429823121515,\n",
       " 0.021676990018547556,\n",
       " 0.021096592695648356,\n",
       " 0.02054943342608352,\n",
       " 0.020032951623369512,\n",
       " 0.01954480802133911,\n",
       " 0.019082864497887195,\n",
       " 0.018645166288870963,\n",
       " 0.018229925049297567,\n",
       " 0.017835503980922002,\n",
       " 0.017460404260820715,\n",
       " 0.017103252733975564,\n",
       " 0.01676279046407036,\n",
       " 0.01643786280173682,\n",
       " 0.01612740988718232,\n",
       " 0.015830458381951695,\n",
       " 0.015546113841248314,\n",
       " 0.01527355375589081,\n",
       " 0.01501202119694187,\n",
       " 0.014760819095874135,\n",
       " 0.014519305096719538,\n",
       " 0.014286886736739168,\n",
       " 0.014063017295290671,\n",
       " 0.013847191778412142,\n",
       " 0.013638943211815368,\n",
       " 0.013437839583976516,\n",
       " 0.013243480908653245,\n",
       " 0.013055496488017833,\n",
       " 0.012873542356344756,\n",
       " 0.012697299258726267,\n",
       " 0.012526470459387387,\n",
       " 0.012360779955433922,\n",
       " 0.01219997077609341,\n",
       " 0.012043803452031975,\n",
       " 0.011892054730871765,\n",
       " 0.011744515955495341,\n",
       " 0.011600992163313185,\n",
       " 0.011461301089339931,\n",
       " 0.011325271989566219,\n",
       " 0.011192744768257856,\n",
       " 0.011063569285657333,\n",
       " 0.010937604548650677,\n",
       " 0.010814717889235345,\n",
       " 0.01069478459767037,\n",
       " 0.010577687126650215,\n",
       " 0.010463314654457987,\n",
       " 0.010351562598671097,\n",
       " 0.0102423322716286,\n",
       " 0.010135530197822769,\n",
       " 0.010031068058345067,\n",
       " 0.009928862175226224,\n",
       " 0.00982883314047457,\n",
       " 0.009730905575342785,\n",
       " 0.009635008006800245,\n",
       " 0.009541072383829084,\n",
       " 0.00944903410775196,\n",
       " 0.009358831593865736,\n",
       " 0.009270406063176318,\n",
       " 0.009183701634356788,\n",
       " 0.009098664847716164,\n",
       " 0.00901524460930032,\n",
       " 0.00893339223371508,\n",
       " 0.008853061110137962,\n",
       " 0.008774206587993261,\n",
       " 0.008696785967191896,\n",
       " 0.008620758209950483,\n",
       " 0.008546083960825285,\n",
       " 0.008472725564742756,\n",
       " 0.008400646803830271,\n",
       " 0.008329812893394416,\n",
       " 0.008260190385169196,\n",
       " 0.008191747126805551,\n",
       " 0.008124452132279665,\n",
       " 0.008058275558392695,\n",
       " 0.007993188730186665,\n",
       " 0.007929163977554242,\n",
       " 0.007866174581319808,\n",
       " 0.007804194817844676,\n",
       " 0.0077431998129072616,\n",
       " 0.007683165549893633,\n",
       " 0.007624068830602874,\n",
       " 0.007565887286538044,\n",
       " 0.007508599249369645,\n",
       " 0.007452183774051139,\n",
       " 0.007396620566490653,\n",
       " 0.007341890011587766,\n",
       " 0.007287973105334366,\n",
       " 0.007234851486837677,\n",
       " 0.007182507280876469,\n",
       " 0.007130923235598414,\n",
       " 0.0070800825670509415,\n",
       " 0.007029969052089651,\n",
       " 0.00698056692573136,\n",
       " 0.006931860881904119,\n",
       " 0.006883836074122209,\n",
       " 0.006836478116093652,\n",
       " 0.0067897729891347085,\n",
       " 0.006743707098541307,\n",
       " 0.006698267184624952,\n",
       " 0.006653440475475683,\n",
       " 0.0066092144519197,\n",
       " 0.006565576961944137,\n",
       " 0.006522516183980973,\n",
       " 0.006480020640428666,\n",
       " 0.006438079163255516,\n",
       " 0.006396680863042674,\n",
       " 0.006355815194255115,\n",
       " 0.006315471781154872,\n",
       " 0.006275640587085844,\n",
       " 0.00623631183399948,\n",
       " 0.006197475976594024,\n",
       " 0.006159123725605965,\n",
       " 0.006121246022206211,\n",
       " 0.00608383406152278,\n",
       " 0.006046879220678953,\n",
       " 0.006010373133726013,\n",
       " 0.005974307572818183,\n",
       " 0.005938674574100718,\n",
       " 0.005903466318178546,\n",
       " 0.005868675255368684,\n",
       " 0.005834293939030648,\n",
       " 0.005800315108394457,\n",
       " 0.0057667316699735825,\n",
       " 0.00573353677396886,\n",
       " 0.005700723650201373,\n",
       " 0.0056682856932826755,\n",
       " 0.0056362164927018805,\n",
       " 0.005604509720205646,\n",
       " 0.005573159261270804,\n",
       " 0.005542159147165217,\n",
       " 0.005511503447236417,\n",
       " 0.005481186451368152,\n",
       " 0.005451202508228379,\n",
       " 0.00542154615908923,\n",
       " 0.005392212072000463,\n",
       " 0.005363194935979449,\n",
       " 0.005334489645178917,\n",
       " 0.005306091185241165,\n",
       " 0.005277994624149104,\n",
       " 0.005250195150558324,\n",
       " 0.005222688061729823,\n",
       " 0.005195468752669473,\n",
       " 0.005168532706353526,\n",
       " 0.005141875531497712,\n",
       " 0.005115492856851025,\n",
       " 0.00508938046889079,\n",
       " 0.005063534202917567,\n",
       " 0.005037950031305091,\n",
       " 0.005012623910094561,\n",
       " 0.004987552013458546,\n",
       " 0.004962730479057196,\n",
       " 0.004938155551387873,\n",
       " 0.004913823571101434,\n",
       " 0.004889730918820745,\n",
       " 0.004865874104275745,\n",
       " 0.004842249660260477,\n",
       " 0.004818854186892807,\n",
       " 0.00479568434488204,\n",
       " 0.004772736896035912,\n",
       " 0.004750008646584609,\n",
       " 0.004727496489304654,\n",
       " 0.004705197301732021,\n",
       " 0.004683108087384567,\n",
       " 0.004661225870031593,\n",
       " 0.004639547738234828,\n",
       " 0.004618070838869182,\n",
       " 0.00459679237129143,\n",
       " 0.004575709628658156,\n",
       " 0.004554819895413512,\n",
       " 0.004534120494726593,\n",
       " 0.004513608877751197,\n",
       " 0.0044932824711289695,\n",
       " 0.004473138772572878,\n",
       " 0.004453175343760075,\n",
       " 0.004433389710803231,\n",
       " 0.004413779554071486,\n",
       " 0.004394342506500298,\n",
       " 0.004375076344599319,\n",
       " 0.004355978741264336,\n",
       " 0.004337047555535429,\n",
       " 0.004318280581151897,\n",
       " 0.004299675646214593,\n",
       " 0.00428123070288203,\n",
       " 0.004262943675266222,\n",
       " 0.0042448125553696,\n",
       " 0.004226835349729834,\n",
       " 0.004209010124532442,\n",
       " 0.004191334906513745,\n",
       " 0.0041738078266041765,\n",
       " 0.004156427062942739,\n",
       " 0.0041391907430238925,\n",
       " 0.004122097088460392,\n",
       " 0.004105144312439203,\n",
       " 0.004088330667130208,\n",
       " 0.004071654486354043,\n",
       " 0.004055114037718639,\n",
       " 0.0040387076922219385,\n",
       " 0.00402243382078064,\n",
       " 0.0040062908175213865,\n",
       " 0.003990277120742834,\n",
       " 0.003974391161932322,\n",
       " 0.0039586314362961995,\n",
       " 0.003942996426538731,\n",
       " 0.003927484673961495,\n",
       " 0.003912094702754463,\n",
       " 0.003896825091556351,\n",
       " 0.003881674421443618,\n",
       " 0.003866641298969755,\n",
       " 0.0038517243769006456,\n",
       " 0.0038369223030272,\n",
       " 0.00382223376722898,\n",
       " 0.0038076574506992035,\n",
       " 0.0037931920733795107,\n",
       " 0.0037788363668020556,\n",
       " 0.003764589096213519,\n",
       " 0.0037504490339205915,\n",
       " 0.003736414958583974,\n",
       " 0.003722485701149103,\n",
       " 0.003708660069768486,\n",
       " 0.003694936921930186,\n",
       " 0.003681315112958138,\n",
       " 0.0036677935195116268,\n",
       " 0.0036543710607348148,\n",
       " 0.003641046624159062,\n",
       " 0.0036278191619964636,\n",
       " 0.0036146875915395197,\n",
       " 0.003601650891785352,\n",
       " 0.003588708027416046,\n",
       " 0.0035758579967962883,\n",
       " 0.0035630997820389736,\n",
       " 0.0035504324204795783,\n",
       " 0.0035378549293047096,\n",
       " 0.0035253663541331207,\n",
       " 0.003512965766172497,\n",
       " 0.003500652236377497,\n",
       " 0.0034884248587581214,\n",
       " 0.003476282701508048,\n",
       " 0.0034642249027185225,\n",
       " 0.003452250570256345,\n",
       " 0.0034403588546355066,\n",
       " 0.0034285488749032766,\n",
       " 0.0034168198149191337,\n",
       " 0.003405170847024351,\n",
       " 0.0033936011331938175,\n",
       " 0.003382109872638804,\n",
       " 0.003370696274800261,\n",
       " 0.0033593595350427867,\n",
       " 0.0033480989059114555,\n",
       " 0.003336913621564576,\n",
       " 0.003325802899612369,\n",
       " 0.0033147660126209696,\n",
       " 0.003303802236050705,\n",
       " 0.0032929108246836122,\n",
       " 0.0032820910845404614,\n",
       " 0.003271342297907688,\n",
       " 0.003260663772276954,\n",
       " 0.003250054814541563,\n",
       " 0.003239514731056295,\n",
       " 0.0032290428742573875,\n",
       " 0.003218638591488262,\n",
       " 0.0032083012022257385,\n",
       " 0.003198030093998957,\n",
       " 0.003187824599168821,\n",
       " 0.003177684116860143,\n",
       " 0.0031676080131529975,\n",
       " 0.003157595670953322,\n",
       " 0.0031476465115933975,\n",
       " 0.0031377598978569532,\n",
       " 0.003127935279532411,\n",
       " 0.0031181720450140287,\n",
       " 0.0031084696438566373,\n",
       " 0.003098827487527328,\n",
       " 0.0030892450463464835,\n",
       " 0.0030797217504701896,\n",
       " 0.003070257040472795,\n",
       " 0.00306085038958815,\n",
       " 0.0030515012538775256,\n",
       " 0.003042209120513003,\n",
       " 0.00303297345810026,\n",
       " 0.0030237937651013437,\n",
       " 0.0030146695202829038,\n",
       " 0.0030056002312518594,\n",
       " 0.0029965854082883094,\n",
       " 0.00298762454079515,\n",
       " 0.0029787171692349872,\n",
       " 0.00296986278689191,\n",
       " 0.002961060937721597,\n",
       " 0.002952311164718032,\n",
       " 0.0029436130100096705,\n",
       " 0.002934965991662929,\n",
       " 0.002926369675937584,\n",
       " 0.0029178236259013034,\n",
       " 0.002909327378465797,\n",
       " 0.002900880516851603,\n",
       " 0.002892482596108013,\n",
       " 0.002884133215779388,\n",
       " 0.0028758319456064618,\n",
       " 0.0028675783517897653,\n",
       " 0.0028593720439097753,\n",
       " 0.002851212624022794,\n",
       " 0.0028430996641302993,\n",
       " 0.0028350328023166857,\n",
       " 0.0028270116197256505,\n",
       " 0.002819035739386522,\n",
       " 0.002811104775459567,\n",
       " 0.0028032183341228957,\n",
       " 0.0027953760609368086,\n",
       " 0.002787577590339446,\n",
       " 0.0027798225467590074,\n",
       " 0.002772110568897806,\n",
       " 0.002764441285021796,\n",
       " 0.002756814360570336,\n",
       " 0.0027492294478727204,\n",
       " 0.002741686187459185,\n",
       " 0.002734184232523877,\n",
       " 0.002726723247658466,\n",
       " 0.0027193029077123895,\n",
       " 0.002711922873484012,\n",
       " 0.0027045828396918626,\n",
       " 0.0026972824617334233,\n",
       " 0.002690021406183364,\n",
       " 0.0026827993729588886,\n",
       " 0.0026756160454193506,\n",
       " 0.0026684711153051035,\n",
       " 0.0026613642586163382,\n",
       " 0.0026542951837532254,\n",
       " 0.0026472636049928544,\n",
       " 0.0026402691953354134,\n",
       " 0.0026333116837641344,\n",
       " 0.0026263907797977994,\n",
       " 0.0026195061754371823,\n",
       " 0.00261265759348298,\n",
       " 0.002605844761172756,\n",
       " 0.0025990673864541864,\n",
       " 0.0025923252064801827,\n",
       " 0.0025856179381222345,\n",
       " 0.0025789453032816186,\n",
       " 0.0025723070516694845,\n",
       " 0.0025657029114597375,\n",
       " 0.0025591326147258284,\n",
       " 0.0025525959203338625,\n",
       " 0.002546092541414142,\n",
       " 0.0025396222430670023,\n",
       " 0.0025331847673166178,\n",
       " 0.002526779881984745,\n",
       " 0.002520407308261771,\n",
       " 0.002514066818502109,\n",
       " 0.0025077581845416672,\n",
       " 0.0025014811544666333,\n",
       " 0.002495235478271512,\n",
       " 0.002489020930951358,\n",
       " 0.0024828372867186557,\n",
       " 0.0024766843190815765,\n",
       " 0.002470561777631347,\n",
       " 0.0024644694602831345,\n",
       " 0.002458407138594461,\n",
       " 0.0024523745836840323,\n",
       " 0.002446371566275619,\n",
       " 0.0024403979033036792,\n",
       " 0.002434453360160032,\n",
       " 0.002428537725697314,\n",
       " 0.002422650786599835,\n",
       " 0.002416792327600408,\n",
       " 0.0024109621549585655,\n",
       " 0.0024051600477417584,\n",
       " 0.0023993858303937563,\n",
       " 0.0023936392750647587,\n",
       " 0.00238792019997301,\n",
       " 0.0023822284182318655,\n",
       " 0.002376563715077216,\n",
       " 0.002370925897221364,\n",
       " 0.0023653147907053804,\n",
       " 0.002359730192400102,\n",
       " 0.0023541719194892802,\n",
       " 0.002348639784155228,\n",
       " 0.0023431335940789625,\n",
       " 0.002337653176173399,\n",
       " 0.002332198351377097,\n",
       " 0.0023267689585347597,\n",
       " 0.0023213647827574277,\n",
       " 0.002315985677211165,\n",
       " 0.002310631463179302,\n",
       " 0.00230530195653377,\n",
       " 0.0022999969915593094,\n",
       " 0.0022947163958291195,\n",
       " 0.0022894600141590816,\n",
       " 0.00228422766031736,\n",
       " 0.0022790191899783657,\n",
       " 0.0022738344266829413,\n",
       " 0.002268673211617844,\n",
       " 0.0022635353785680924,\n",
       " 0.002258420777940201,\n",
       " 0.002253329251816971,\n",
       " 0.002248260634789857,\n",
       " 0.002243214801274234,\n",
       " 0.002238191568394745,\n",
       " 0.0022331907948466346,\n",
       " 0.0022282123301725575,\n",
       " 0.002223256015677838,\n",
       " 0.0022183217085372706,\n",
       " 0.0022134092802081674,\n",
       " 0.002208518568435982,\n",
       " 0.0022036494271916226,\n",
       " 0.0021988017250489066,\n",
       " 0.0021939753204412047,\n",
       " 0.002189170062675487,\n",
       " 0.0021843858161280255,\n",
       " 0.0021796224587374655,\n",
       " 0.00217487983408246,\n",
       " 0.0021701578013837955,\n",
       " 0.0021654562572232445,\n",
       " 0.002160775038675209,\n",
       " 0.002156114022389716,\n",
       " 0.0021514730740687248,\n",
       " 0.0021468520728439985,\n",
       " 0.002142250909934124,\n",
       " 0.002137669417586637,\n",
       " 0.0021331074913904483,\n",
       " 0.002128565014092512,\n",
       " 0.0021240418568820242,\n",
       " 0.0021195378805461958,\n",
       " 0.0021150529830765816,\n",
       " 0.0021105870260994517,\n",
       " 0.0021061398850784497,\n",
       " 0.0021017114712139186,\n",
       " 0.0020973016347369746,\n",
       " 0.0020929102874217504,\n",
       " 0.0020885372800157733,\n",
       " 0.0020841825247579454,\n",
       " 0.0020798458960971508,\n",
       " 0.0020755273043204492,\n",
       " 0.0020712265988370015,\n",
       " 0.0020669436906811806,\n",
       " 0.002062678476500858,\n",
       " 0.002058430839996054,\n",
       " 0.0020542006532137226,\n",
       " 0.002049987824279186,\n",
       " 0.0020457922472221705,\n",
       " 0.0020416138266694294,\n",
       " 0.002037452453501977,\n",
       " 0.0020333080062296606,\n",
       " 0.0020291803987944083,\n",
       " 0.0020250695071778258,\n",
       " 0.0020209752663294875,\n",
       " 0.002016897547854817,\n",
       " 0.002012836259481761,\n",
       " 0.0020087912948824043,\n",
       " 0.0020047625583616262,\n",
       " 0.002000749963793817,\n",
       " 0.0019967534103828635,\n",
       " 0.0019927727841291994,\n",
       " 0.0019888080057162793,\n",
       " 0.0019848589804761486,\n",
       " 0.001980925611566115,\n",
       " 0.001977007811827757,\n",
       " 0.001973105479535429,\n",
       " 0.0019692185347775833,\n",
       " 0.00196534687070923,\n",
       " 0.001961490414452943,\n",
       " 0.0019576490654944416,\n",
       " 0.0019538227333708736,\n",
       " 0.001950011336665672,\n",
       " 0.0019462147904623945,\n",
       " 0.0019424330066947102,\n",
       " 0.0019386658944613897,\n",
       " 0.0019349133719513264,\n",
       " 0.0019311753538929922,\n",
       " 0.0019274517635420117,\n",
       " 0.0019237425085453824,\n",
       " 0.001920047505785403,\n",
       " 0.001916366668814609,\n",
       " 0.0019126999314718154,\n",
       " 0.0019090471992873586,\n",
       " 0.0019054083988800739,\n",
       " 0.0019017834525653806,\n",
       " 0.0018981722671440916,\n",
       " 0.0018945747820200037,\n",
       " 0.0018909909077319362,\n",
       " 0.00188742057540636,\n",
       " 0.0018838636997740336,\n",
       " 0.001880320215734172,\n",
       " 0.0018767900414130047,\n",
       " 0.0018732731031241377,\n",
       " 0.0018697693229082846,\n",
       " 0.0018662786306020878,\n",
       " 0.0018628009514169927,\n",
       " 0.0018593362180433007,\n",
       " 0.0018558843466192185,\n",
       " 0.0018524452733106641,\n",
       " 0.0018490189290254321,\n",
       " 0.0018456052399390048,\n",
       " 0.0018422041396093166,\n",
       " 0.0018388155565969761,\n",
       " 0.0018354394149649993,\n",
       " 0.0018320756580116322,\n",
       " 0.0018287242114222323,\n",
       " 0.0018253850083136207,\n",
       " 0.0018220579768494054,\n",
       " 0.0018187430523768323,\n",
       " 0.0018154401767084233,\n",
       " 0.0018121492741923837,\n",
       " 0.0018088702883836307,\n",
       " 0.0018056031568400572,\n",
       " 0.0018023478000807028,\n",
       " 0.001799104169855767,\n",
       " 0.0017958721994573656,\n",
       " 0.00179265181720687,\n",
       " 0.0017894429702350473,\n",
       " 0.0017862455993180551,\n",
       " 0.0017830596395129036,\n",
       " 0.0017798850207293695,\n",
       " 0.001776721691527784,\n",
       " 0.0017735695939709128,\n",
       " 0.0017704286642737134,\n",
       " 0.001767298845029647,\n",
       " 0.001764180072931297,\n",
       " 0.0017610722910019882,\n",
       " 0.0017579754479627127,\n",
       " 0.0017548894743792994,\n",
       " 0.0017518143194025265,\n",
       " 0.0017487499256265617,\n",
       " 0.0017456962413861564,\n",
       " 0.0017426532085410545,\n",
       " 0.0017396207631234935,\n",
       " 0.0017365988592040191,\n",
       " 0.0017335874321630581,\n",
       " 0.0017305864354845272,\n",
       " 0.0017275958156624189,\n",
       " 0.001724615524541326,\n",
       " 0.0017216454954983183,\n",
       " 0.0017186856802142907,\n",
       " 0.0017157360352020486,\n",
       " 0.0017127964980400523,\n",
       " 0.0017098670125489164,\n",
       " 0.001706947539808726,\n",
       " 0.001704038021508493,\n",
       " 0.001701138405168329,\n",
       " 0.0016982486435563335,\n",
       " 0.0016953686825222644,\n",
       " 0.0016924984849724359,\n",
       " 0.001689637982597934,\n",
       " 0.0016867871372038008,\n",
       " 0.0016839459027730417,\n",
       " 0.0016811142262488298,\n",
       " 0.0016782920598800207,\n",
       " 0.001675479360690584,\n",
       " 0.0016726760667190288,\n",
       " 0.0016698821454830767,\n",
       " 0.0016670975444656136,\n",
       " 0.0016643222164012363,\n",
       " 0.0016615561187510823,\n",
       " 0.0016587992015886428,\n",
       " 0.00165605141998006,\n",
       " 0.00165331272184333,\n",
       " 0.0016505830719461812,\n",
       " 0.0016478624269380367,\n",
       " 0.0016451507361618456,\n",
       " 0.001642447965667794,\n",
       " 0.0016397540616179852,\n",
       " 0.0016370689755583123,\n",
       " 0.0016343926755216108,\n",
       " 0.0016317251094543684,\n",
       " 0.0016290662421499565,\n",
       " 0.0016264160302808769,\n",
       " 0.0016237744232108493,\n",
       " 0.0016211413870087534,\n",
       " 0.0016185168794950488,\n",
       " 0.0016159008627081493,\n",
       " 0.001613293290841095,\n",
       " 0.0016106941226676215,\n",
       " 0.0016081033210840919,\n",
       " 0.0016055208410557002,\n",
       " 0.0016029466420511207,\n",
       " 0.001600380687592161,\n",
       " 0.0015978229332069148,\n",
       " 0.0015952733388706673,\n",
       " 0.0015927318802027074,\n",
       " 0.0015901985003357977,\n",
       " 0.0015876731713724895,\n",
       " 0.001585155856563547,\n",
       " 0.0015826465111931254,\n",
       " 0.0015801450950169644,\n",
       " 0.0015776515834567619,\n",
       " 0.001575165931108982,\n",
       " 0.001572688097110442,\n",
       " 0.0015702180446842775,\n",
       " 0.0015677557407313108,\n",
       " 0.0015653011554622818,\n",
       " 0.001562854250425324,\n",
       " 0.0015604149793722259,\n",
       " 0.001557983312321129,\n",
       " 0.0015555592106468283,\n",
       " 0.0015531426512281718,\n",
       " 0.0015507335899730586,\n",
       " 0.001548331987198597,\n",
       " 0.0015459378071901852,\n",
       " 0.0015435510294462135,\n",
       " 0.0015411716122321686,\n",
       " 0.0015387995179869892,\n",
       " 0.0015364347129057194,\n",
       " 0.0015340771665638994,\n",
       " 0.001531726851579515,\n",
       " 0.0015293837316672216,\n",
       " 0.0015270477625286768,\n",
       " 0.0015247189275784366,\n",
       " 0.0015223971886065368,\n",
       " 0.0015200825112240105,\n",
       " 0.0015177748644807866,\n",
       " 0.0015154742088802691,\n",
       " 0.0015131805205170546,\n",
       " 0.0015108937662347466,\n",
       " 0.001508613916192589,\n",
       " 0.0015063409318923694,\n",
       " 0.001504074790327228,\n",
       " 0.0015018154591494592,\n",
       " 0.0014995629092461281,\n",
       " 0.001497317102774061,\n",
       " 0.0014950780173159336,\n",
       " 0.001492845621054622,\n",
       " 0.0014906198853547148,\n",
       " 0.0014884007728028096,\n",
       " 0.001486188261368376,\n",
       " 0.0014839823195824044,\n",
       " 0.0014817829191227868,\n",
       " 0.0014795900228580935,\n",
       " 0.001477403620653102,\n",
       " 0.0014752236684615159,\n",
       " 0.0014730501382832,\n",
       " 0.0014708830049180333,\n",
       " 0.001468722245685906,\n",
       " 0.001466567816891656,\n",
       " 0.0014644197024927012,\n",
       " 0.0014622778764091856,\n",
       " 0.0014601423035277062,\n",
       " 0.0014580129638877334,\n",
       " 0.0014558898278832575,\n",
       " 0.0014537728688688704,\n",
       " 0.001451662062863704,\n",
       " 0.001449557376643446,\n",
       " 0.0014474587803062144,\n",
       " 0.0014453662585818478,\n",
       " 0.0014432797744441369,\n",
       " 0.001441199306211025,\n",
       " 0.0014391248343686616,\n",
       " 0.0014370563180715163,\n",
       " 0.0014349937438416767,\n",
       " 0.001432937087907489,\n",
       " 0.001430886317232933,\n",
       " 0.0014288414137271224,\n",
       " 0.0014268023378251951,\n",
       " 0.00142476907720184,\n",
       " 0.0014227416091227442,\n",
       " 0.0014207199014854947,\n",
       " 0.0014187039370394516,\n",
       " 0.0014166936886175067,\n",
       " 0.0014146891201277307,\n",
       " 0.0014126902223704516,\n",
       " 0.0014106969637828998,\n",
       " 0.0014087093276001156,\n",
       " 0.0014067272870921022,\n",
       " 0.0014047508182018632,\n",
       " 0.0014027798876365693,\n",
       " 0.0014008144887157385,\n",
       " 0.0013988545921438727,\n",
       " 0.0013969001715550238,\n",
       " 0.0013949512032198383,\n",
       " 0.0013930076657818979,\n",
       " 0.0013910695400204264,\n",
       " 0.001389136796995193,\n",
       " 0.0013872094223015222,\n",
       " 0.0013852873913336736,\n",
       " 0.001383370670304949,\n",
       " 0.001381459252090384,\n",
       " 0.0013795531069944467,\n",
       " 0.0013776522199326902,\n",
       " 0.0013757565656875802,\n",
       " 0.0013738661215633363,\n",
       " 0.0013719808671337555,\n",
       " 0.001370100772373723,\n",
       " 0.0013682258335437005,\n",
       " 0.001366356012353506,\n",
       " 0.0013644912976250407,\n",
       " 0.0013626316676564835,\n",
       " 0.001360777102916195,\n",
       " 0.0013589275858257002,\n",
       " 0.0013570830889228394,\n",
       " 0.0013552435874916683,\n",
       " 0.0013534090709293697,\n",
       " 0.001351579518051876,\n",
       " 0.0013497548981519939,\n",
       " 0.0013479351952347817,\n",
       " 0.001346120394904792,\n",
       " 0.0013443104842061218,\n",
       " 0.0013425054281953946,\n",
       " 0.0013407052187066355,\n",
       " 0.0013389098251074026,\n",
       " 0.0013371192431115631,\n",
       " 0.0013353334455785326,\n",
       " 0.0013335524080817852,\n",
       " 0.0013317761202789801,\n",
       " 0.0013300045495789462,\n",
       " 0.0013282376899326933,\n",
       " 0.0013264755242545986,\n",
       " 0.0013247180255260692,\n",
       " 0.0013229651810719037,\n",
       " 0.0013212169678428888,\n",
       " 0.001319473365094732,\n",
       " 0.001317734365799103,\n",
       " 0.0013159999403474394,\n",
       " 0.001314270073733568,\n",
       " 0.0013125447524519323,\n",
       " 0.0013108239527059996,\n",
       " 0.0013091076647204225,\n",
       " 0.001307395868055856,\n",
       " 0.00130568854431689,\n",
       " 0.001303985665306123,\n",
       " 0.001302287228928958,\n",
       " 0.0013005932100171936,\n",
       " 0.0012989035975609802,\n",
       " 0.0012972183700099213,\n",
       " 0.0012955375079686602,\n",
       " 0.0012938609939813757,\n",
       " 0.0012921888123378286,\n",
       " 0.001290520948898804,\n",
       " 0.0012888573909390078,\n",
       " 0.0012871981153641438,\n",
       " 0.0012855431013893454,\n",
       " 0.001283892341949764,\n",
       " 0.0012822458190455048,\n",
       " 0.0012806035164766617,\n",
       " 0.0012789654196633197,\n",
       " 0.0012773315038420226,\n",
       " 0.0012757017583671928,\n",
       " 0.0012740761620162772,\n",
       " 0.0012724547073305113,\n",
       " 0.0012708373759554741,\n",
       " 0.001269224151372187,\n",
       " 0.001267615018713569,\n",
       " 0.0012660099645992475,\n",
       " 0.0012644089653453552,\n",
       " 0.001262812011277943,\n",
       " 0.0012612190936904571,\n",
       " 0.001259630181463935,\n",
       " 0.0012580452698743745,\n",
       " 0.0012564643430286412,\n",
       " 0.0012548873866229143,\n",
       " 0.0012533143877837556,\n",
       " 0.0012517453232835378,\n",
       " 0.0012501801838589292,\n",
       " 0.001248618961172931,\n",
       " 0.0012470616244391792,\n",
       " 0.0012455081692330115,\n",
       " 0.0012439585799306995,\n",
       " 0.0012424128424708874,\n",
       " 0.0012408709441983547,\n",
       " 0.0012393328737234038,\n",
       " 0.0012377986091537739,\n",
       " 0.001236268142427963,\n",
       " 0.0012347414546490867,\n",
       " 0.001233218528809949,\n",
       " 0.0012316993612456063,\n",
       " 0.001230183925374544,\n",
       " 0.0012286722205564647,\n",
       " 0.001227164222932035,\n",
       " 0.0012256599226693878,\n",
       " 0.0012241593109198422,\n",
       " 0.001222662368078054,\n",
       " 0.0012211690764992126,\n",
       " 0.0012196794319445214,\n",
       " 0.0012181934189575304,\n",
       " 0.0012167110236274352,\n",
       " 0.0012152322334345116,\n",
       " 0.0012137570254694761,\n",
       " 0.0012122854023970371,\n",
       " 0.0012108173433323687,\n",
       " 0.0012093528294791268,\n",
       " 0.0012078918555621344,\n",
       " 0.001206434416833732,\n",
       " 0.0012049804857379616,\n",
       " 0.0012035300607575257,\n",
       " 0.001202083117243792,\n",
       " 0.0012006396562960556,\n",
       " 0.0011991996556206177,\n",
       " 0.001197763106794682,\n",
       " 0.0011963299905961994,\n",
       " 0.0011949003013668575,\n",
       " 0.0011934740223726438,\n",
       " 0.0011920511501944786,\n",
       " 0.0011906316701136583,\n",
       " 0.0011892155572418171,\n",
       " 0.0011878028124624907,\n",
       " 0.0011863934249293285,\n",
       " 0.001184987373239083,\n",
       " 0.001183584649770364,\n",
       " 0.0011821852476639194,\n",
       " 0.0011807891491048912,\n",
       " 0.0011793963497014385,\n",
       " 0.001178006833859373,\n",
       " 0.0011766205875439247,\n",
       " 0.001175237598123801,\n",
       " 0.001173857854230838,\n",
       " 0.0011724813456336887,\n",
       " 0.001171108063124141,\n",
       " 0.0011697379984148034,\n",
       " 0.0011683711324054916,\n",
       " 0.0011670074595475722,\n",
       " 0.0011656469632057424,\n",
       " 0.0011642896400497625,\n",
       " 0.001162935475440884,\n",
       " 0.0011615844562042337,\n",
       " 0.0011602365704824252,\n",
       " 0.0011588918076038113,\n",
       " 0.0011575501696054415,\n",
       " 0.0011562116233960638,\n",
       " 0.0011548761741181478,\n",
       " 0.001153543803116852,\n",
       " 0.001152214505244351,\n",
       " 0.0011508882758676023,\n",
       " 0.0011495650875338029,\n",
       " 0.0011482449388184946,\n",
       " 0.001146927828439665,\n",
       " 0.0011456137319604386,\n",
       " 0.00114430263902916,\n",
       " 0.0011429945519708705,\n",
       " 0.0011416894495950441,\n",
       " 0.001140387324471739,\n",
       " 0.0011390881582724755,\n",
       " 0.001137791957784686,\n",
       " 0.0011364987058339955,\n",
       " 0.0011352083869634664,\n",
       " 0.0011339209989033876,\n",
       " 0.0011326365279694859,\n",
       " 0.0011313549618459163,\n",
       " 0.001130076289448418,\n",
       " 0.0011288005008011559,\n",
       " 0.0011275275985674113,\n",
       " 0.0011262575618610725,\n",
       " 0.0011249903835261713,\n",
       " 0.0011237260454808043,\n",
       " 0.0011224645547343224,\n",
       " 0.0011212058943120774,\n",
       " 0.001119950048936886,\n",
       " 0.0011186970165008151,\n",
       " 0.0011174467834651927,\n",
       " 0.0011161993376452143,\n",
       " 0.001114954679716088,\n",
       " 0.0011137127986439682,\n",
       " 0.001112473672856863,\n",
       " 0.001111237306223043,\n",
       " 0.0011100036905824198,\n",
       " 0.001108772806949382,\n",
       " 0.001107544649878411,\n",
       " 0.0011063192144685405,\n",
       " 0.0011050964846673684,\n",
       " 0.0011038764576692648,\n",
       " 0.0011026591193076315,\n",
       " 0.0011014444568325289,\n",
       " 0.0011002324704105443,\n",
       " 0.0010990231485500741,\n",
       " 0.0010978164809086748,\n",
       " 0.0010966124698196795,\n",
       " 0.0010954110941000235,\n",
       " 0.0010942123463264812,\n",
       " 0.0010930162198181487,\n",
       " 0.0010918227085622136,\n",
       " 0.0010906317955056116,\n",
       " 0.0010894434769421176,\n",
       " 0.001088257749536129,\n",
       " 0.0010870745986440707,\n",
       " 0.001085894022728258,\n",
       " 0.0010847160087632422,\n",
       " 0.001083540545026119,\n",
       " 0.0010823676209662755,\n",
       " 0.0010811972387296915,\n",
       " 0.001080029388606217,\n",
       " 0.0010788640618567167,\n",
       " 0.001077701238974437,\n",
       " 0.0010765409256863637,\n",
       " 0.00107538310386372,\n",
       " 0.0010742277688321387,\n",
       " 0.0010730749163846893,\n",
       " 0.0010719245427351345,\n",
       " 0.0010707766328343286,\n",
       " 0.0010696311731380403,\n",
       " 0.0010684881630979937,\n",
       " 0.0010673475905792083,\n",
       " 0.0010662094563017338,\n",
       " 0.0010650737376305505,\n",
       " 0.0010639404374772053,\n",
       " 0.0010628095468205579,\n",
       " 0.001061681057541582,\n",
       " 0.0010605549739746867,\n",
       " 0.001059431265096243,\n",
       " 0.0010583099379095815,\n",
       " 0.0010571909870761677,\n",
       " 0.0010560743961497883,\n",
       " 0.0010549601619703835,\n",
       " 0.0010538482816938996,\n",
       " 0.0010527387411191557,\n",
       " 0.001051631539106622,\n",
       " 0.0010505266629891885,\n",
       " 0.0010494241013664567,\n",
       " 0.0010483238556196,\n",
       " 0.0010472259153501426,\n",
       " 0.001046130271199456,\n",
       " 0.0010450369147447746,\n",
       " 0.0010439458384056092,\n",
       " 0.0010428570353595197,\n",
       " 0.0010417705111078423,\n",
       " 0.001040686247318715,\n",
       " 0.0010396042391349943,\n",
       " 0.0010385244705437189,\n",
       " 0.0010374469504162876,\n",
       " 0.0010363716634538955,\n",
       " 0.001035298595887618,\n",
       " 0.001034227746966955,\n",
       " 0.0010331591160164567,\n",
       " 0.001032092690786686,\n",
       " 0.0010310284718946813,\n",
       " 0.0010299664366127132,\n",
       " 0.0010289065877688896,\n",
       " 0.0010278489279085075,\n",
       " 0.0010267934360392696,\n",
       " 0.0010257401049096411,\n",
       " 0.0010246889396347725,\n",
       " 0.0010236399215352349,\n",
       " 0.0010225930570826065,\n",
       " 0.0010215483288182436,\n",
       " 0.0010205057326708989,\n",
       " 0.0010194652649764499,\n",
       " 0.0010184269107956542,\n",
       " 0.0010173906683247256,\n",
       " 0.001016356535940257,\n",
       " 0.001015324500539649,\n",
       " 0.0010142945619720952,\n",
       " 0.0010132667084603371,\n",
       " 0.0010122409410463747,\n",
       " 0.0010112172490264752,\n",
       " 0.0010101956227672798,\n",
       " 0.001009176065240297]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(1000, dtype=torch.float32)\n",
    "x += 1\n",
    "y = 1 / x\n",
    "y = y.tolist()\n",
    "ma = []\n",
    "for i in y:\n",
    "    if ma:\n",
    "        ma.append(ma[-1]*0.9+i*0.1)\n",
    "    else :\n",
    "        ma.append(i)\n",
    "ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q学习算法实现**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体在训练中只做两件事：\n",
    "- 选择动作\n",
    "- 更新策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, cfg):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.n\n",
    "        self.action_size = env.action_space.n\n",
    "        self.Q = np.random.rand(self.state_size, self.action_size)\n",
    "        self.gamma = cfg.gamma\n",
    "        self.alpha = cfg.alpha\n",
    "        self.epsilon_min = cfg.epsilon_min\n",
    "        self.epsilon_max = cfg.epsilon_max\n",
    "        self.epsilon = cfg.epsilon\n",
    "        self.sample_count = 0\n",
    "        self.epsilon_decay = cfg.epsilon_decay\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        self.sample_count += 1\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-1. * self.sample_count / self.epsilon_decay) # decrease epsilon\n",
    "        # epsilon-greedy strategy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state, :])\n",
    "        \n",
    "    def qlearn(self, state, action, reward, next_state, done):\n",
    "        q_now = self.Q[state, action]\n",
    "        if done:\n",
    "            q_next = reward\n",
    "        else:\n",
    "            q_next = reward + self.gamma * np.max(self.Q[next_state, :])\n",
    "        self.Q[state, action] += self.alpha * (q_next - q_now)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.seed = 1 # 随机种子\n",
    "        self.epsilon = 0.95 #  e-greedy策略中epsilon的初始值\n",
    "        self.epsilon_max = 0.95 #  e-greedy策略中epsilon的初始值\n",
    "        self.epsilon_min = 0.01 #  e-greedy策略中epsilon的最终值\n",
    "        self.epsilon_decay = 300 #  e-greedy策略中epsilon的衰减率\n",
    "        self.gamma = 0.9 # 折扣因子\n",
    "        self.alpha = 0.1 # 学习率\n",
    "\n",
    "cfg = Config()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env,cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, reward: -3386, epsilon: 0.17888418913003543, ma: -3386\n",
      "Episode: 1, reward: -206, epsilon: 0.1282199109253413, ma: -3068.0\n",
      "Episode: 2, reward: -377, epsilon: 0.07509709179078522, ma: -2798.9\n",
      "Episode: 3, reward: -113, epsilon: 0.05466602781795325, ma: -2530.3100000000004\n",
      "Episode: 4, reward: -157, epsilon: 0.036466502459482096, ma: -2292.9790000000003\n",
      "Episode: 5, reward: -255, epsilon: 0.025734879545065793, ma: -2089.1811000000002\n",
      "Episode: 6, reward: -316, epsilon: 0.017633500658128767, ma: -1911.86299\n",
      "Episode: 7, reward: -339, epsilon: 0.012465874575496746, ma: -1754.5766910000002\n",
      "Episode: 8, reward: -79, epsilon: 0.011894989331299403, ma: -1587.0190219000003\n",
      "Episode: 9, reward: -203, epsilon: 0.010963239247691907, ma: -1448.6171197100002\n",
      "Episode: 10, reward: -301, epsilon: 0.010491257528167241, ma: -1333.8554077390002\n",
      "Episode: 11, reward: -183, epsilon: 0.01026692520486906, ma: -1218.7698669651002\n",
      "Episode: 12, reward: -202, epsilon: 0.010136133381881726, ma: -1117.0928802685903\n",
      "Episode: 13, reward: -55, epsilon: 0.010113329762479608, ma: -1010.8835922417313\n",
      "Episode: 14, reward: -284, epsilon: 0.01006116867146269, ma: -938.1952330175582\n",
      "Episode: 15, reward: -134, epsilon: 0.010039133093401875, ma: -857.7757097158023\n",
      "Episode: 16, reward: -167, epsilon: 0.010022427812562169, ma: -788.6981387442222\n",
      "Episode: 17, reward: -111, epsilon: 0.01001549166009779, ma: -720.9283248698\n",
      "Episode: 18, reward: -268, epsilon: 0.010008819529179454, ma: -675.63549238282\n",
      "Episode: 19, reward: -60, epsilon: 0.010007220819766887, ma: -614.071943144538\n",
      "Episode: 20, reward: -187, epsilon: 0.010003871473385084, ma: -571.3647488300842\n",
      "Episode: 21, reward: -125, epsilon: 0.010002552232554187, ma: -526.7282739470758\n",
      "Episode: 22, reward: -87, epsilon: 0.010001909742636286, ma: -482.7554465523682\n",
      "Episode: 23, reward: -118, epsilon: 0.0100012887015412, ma: -446.2799018971314\n",
      "Episode: 24, reward: -96, epsilon: 0.01000093578938322, ma: -411.2519117074183\n",
      "Episode: 25, reward: -90, epsilon: 0.010000693249825809, ma: -379.1267205366765\n",
      "Episode: 26, reward: -117, epsilon: 0.010000469369560309, ma: -352.91404848300886\n",
      "Episode: 27, reward: -108, epsilon: 0.010000327468030406, ma: -328.422643634708\n",
      "Episode: 28, reward: -112, epsilon: 0.010000225440687986, ma: -306.78037927123717\n",
      "Episode: 29, reward: -99, epsilon: 0.010000162074661074, ma: -286.00234134411346\n",
      "Episode: 30, reward: -96, epsilon: 0.010000117690359073, ma: -267.00210720970216\n",
      "Episode: 31, reward: -61, epsilon: 0.010000096036061982, ma: -246.40189648873195\n",
      "Episode: 32, reward: -75, epsilon: 0.010000074792960273, ma: -229.26170683985876\n",
      "Episode: 33, reward: -81, epsilon: 0.010000057095412193, ma: -214.43553615587288\n",
      "Episode: 34, reward: -49, epsilon: 0.01000004849159254, ma: -197.8919825402856\n",
      "Episode: 35, reward: -140, epsilon: 0.01000003040854841, ma: -192.10278428625705\n",
      "Episode: 36, reward: -87, epsilon: 0.010000022753608917, ma: -181.59250585763132\n",
      "Episode: 37, reward: -57, epsilon: 0.010000018816304725, ma: -169.1332552718682\n",
      "Episode: 38, reward: -122, epsilon: 0.010000012529139606, ma: -164.41992974468135\n",
      "Episode: 39, reward: -50, epsilon: 0.010000010605687707, ma: -152.97793677021323\n",
      "Episode: 40, reward: -85, epsilon: 0.010000007988932129, ma: -146.1801430931919\n",
      "Episode: 41, reward: -45, epsilon: 0.010000006876137607, ma: -136.0621287838727\n",
      "Episode: 42, reward: -109, epsilon: 0.010000004781353983, ma: -133.35591590548543\n",
      "Episode: 43, reward: -29, epsilon: 0.010000004340793215, ma: -122.92032431493689\n",
      "Episode: 44, reward: -83, epsilon: 0.010000003291654853, ma: -118.92829188344321\n",
      "Episode: 45, reward: -72, epsilon: 0.010000002589307416, ma: -114.23546269509889\n",
      "Episode: 46, reward: -78, epsilon: 0.01000000199648959, ma: -110.611916425589\n",
      "Episode: 47, reward: -61, epsilon: 0.010000001629147872, ma: -105.6507247830301\n",
      "Episode: 48, reward: -67, epsilon: 0.010000001303070974, ma: -101.7856523047271\n",
      "Episode: 49, reward: -79, epsilon: 0.0100000010013914, ma: -99.50708707425439\n",
      "Episode: 50, reward: -57, epsilon: 0.010000000828109765, ma: -95.25637836682895\n",
      "Episode: 51, reward: -59, epsilon: 0.010000000680262699, ma: -91.63074053014607\n",
      "Episode: 52, reward: -58, epsilon: 0.01000000056067741, ma: -88.26766647713146\n",
      "Episode: 53, reward: -74, epsilon: 0.010000000438113955, ma: -86.84089982941832\n",
      "Episode: 54, reward: -62, epsilon: 0.010000000356314005, ma: -84.3568098464765\n",
      "Episode: 55, reward: -73, epsilon: 0.01000000027935379, ma: -83.22112886182884\n",
      "Episode: 56, reward: -49, epsilon: 0.01000000023725742, ma: -79.79901597564597\n",
      "Episode: 57, reward: -75, epsilon: 0.010000000184776264, ma: -79.31911437808138\n",
      "Episode: 58, reward: -49, epsilon: 0.010000000156931968, ma: -76.28720294027325\n",
      "Episode: 59, reward: -129, epsilon: 0.010000000141997915, ma: -81.55848264624593\n",
      "Episode: 60, reward: -71, epsilon: 0.01000000011207247, ma: -80.50263438162133\n",
      "Episode: 61, reward: -70, epsilon: 0.010000000088749019, ma: -79.4523709434592\n",
      "Episode: 62, reward: -57, epsilon: 0.010000000073391812, ma: -77.20713384911329\n",
      "Episode: 63, reward: -62, epsilon: 0.010000000059688879, ma: -75.68642046420197\n",
      "Episode: 64, reward: -31, epsilon: 0.010000000053829002, ma: -71.21777841778177\n",
      "Episode: 65, reward: -38, epsilon: 0.01000000004742482, ma: -67.8960005760036\n",
      "Episode: 66, reward: -167, epsilon: 0.010000000037806425, ma: -77.80640051840324\n",
      "Episode: 67, reward: -71, epsilon: 0.010000000029838885, ma: -77.12576046656291\n",
      "Episode: 68, reward: -45, epsilon: 0.010000000025682566, ma: -73.91318441990661\n",
      "Episode: 69, reward: -35, epsilon: 0.010000000022854448, ma: -70.02186597791595\n",
      "Episode: 70, reward: -44, epsilon: 0.010000000019736685, ma: -67.41967938012436\n",
      "Episode: 71, reward: -47, epsilon: 0.010000000016874648, ma: -65.37771144211193\n",
      "Episode: 72, reward: -67, epsilon: 0.010000000013497157, ma: -65.53994029790074\n",
      "Episode: 73, reward: -50, epsilon: 0.010000000011425097, ma: -63.985946268110666\n",
      "Episode: 74, reward: -60, epsilon: 0.010000000009354078, ma: -63.5873516412996\n",
      "Episode: 75, reward: -38, epsilon: 0.010000000008241198, ma: -61.028616477169635\n",
      "Episode: 76, reward: -25, epsilon: 0.010000000007582268, ma: -57.42575482945267\n",
      "Episode: 77, reward: -67, epsilon: 0.010000000006064663, ma: -58.38317934650741\n",
      "Episode: 78, reward: -51, epsilon: 0.010000000005116543, ma: -57.644861411856674\n",
      "Episode: 79, reward: -62, epsilon: 0.010000000004161238, ma: -58.08037527067101\n",
      "Episode: 80, reward: -35, epsilon: 0.010000000003703009, ma: -55.77233774360391\n",
      "Episode: 81, reward: -41, epsilon: 0.01000000000322999, ma: -54.29510396924353\n",
      "Episode: 82, reward: -67, epsilon: 0.010000000002583501, ma: -55.56559357231918\n",
      "Episode: 83, reward: -40, epsilon: 0.010000000002261013, ma: -54.00903421508726\n",
      "Episode: 84, reward: -58, epsilon: 0.010000000001863543, ma: -54.408130793578536\n",
      "Episode: 85, reward: -31, epsilon: 0.010000000001680592, ma: -52.067317714220685\n",
      "Episode: 86, reward: -23, epsilon: 0.01000000000155656, ma: -49.16058594279861\n",
      "Episode: 87, reward: -60, epsilon: 0.010000000001274404, ma: -50.24452734851875\n",
      "Episode: 88, reward: -176, epsilon: 0.010000000000985913, ma: -62.820074613666876\n",
      "Episode: 89, reward: -30, epsilon: 0.01000000000089209, ma: -59.53806715230019\n",
      "Episode: 90, reward: -42, epsilon: 0.010000000000775547, ma: -57.784260437070174\n",
      "Episode: 91, reward: -49, epsilon: 0.010000000000658678, ma: -56.90583439336316\n",
      "Episode: 92, reward: -73, epsilon: 0.01000000000051641, ma: -58.51525095402684\n",
      "Episode: 93, reward: -30, epsilon: 0.010000000000467267, ma: -55.663725858624154\n",
      "Episode: 94, reward: -125, epsilon: 0.010000000000428475, ma: -62.59735327276174\n",
      "Episode: 95, reward: -38, epsilon: 0.010000000000377499, ma: -60.13761794548556\n",
      "Episode: 96, reward: -60, epsilon: 0.01000000000030907, ma: -60.123856150937\n",
      "Episode: 97, reward: -27, epsilon: 0.010000000000282469, ma: -56.811470535843306\n",
      "Episode: 98, reward: -32, epsilon: 0.01000000000025389, ma: -54.330323482258976\n",
      "Episode: 99, reward: -42, epsilon: 0.010000000000220721, ma: -53.097291134033085\n",
      "Episode: 100, reward: -58, epsilon: 0.01000000000018192, ma: -53.587562020629775\n",
      "Episode: 101, reward: -18, epsilon: 0.010000000000171327, ma: -50.028805818566795\n",
      "Episode: 102, reward: -35, epsilon: 0.01000000000015246, ma: -48.525925236710115\n",
      "Episode: 103, reward: -38, epsilon: 0.010000000000134322, ma: -47.4733327130391\n",
      "Episode: 104, reward: -40, epsilon: 0.010000000000117554, ma: -46.72599944173519\n",
      "Episode: 105, reward: -27, epsilon: 0.010000000000107437, ma: -44.753399497561674\n",
      "Episode: 106, reward: -49, epsilon: 0.010000000000091247, ma: -45.1780595478055\n",
      "Episode: 107, reward: -29, epsilon: 0.01000000000008284, ma: -43.56025359302495\n",
      "Episode: 108, reward: -31, epsilon: 0.010000000000074706, ma: -42.30422823372246\n",
      "Episode: 109, reward: -75, epsilon: 0.010000000000058181, ma: -45.57380541035022\n",
      "Episode: 110, reward: -23, epsilon: 0.010000000000053888, ma: -43.31642486931519\n",
      "Episode: 111, reward: -31, epsilon: 0.010000000000048597, ma: -42.08478238238367\n",
      "Episode: 112, reward: -29, epsilon: 0.01000000000004412, ma: -40.77630414414531\n",
      "Episode: 113, reward: -43, epsilon: 0.010000000000038228, ma: -40.998673729730776\n",
      "Episode: 114, reward: -166, epsilon: 0.010000000000030576, ma: -53.4988063567577\n",
      "Episode: 115, reward: -18, epsilon: 0.010000000000028797, ma: -49.94892572108193\n",
      "Episode: 116, reward: -24, epsilon: 0.010000000000026581, ma: -47.354033148973734\n",
      "Episode: 117, reward: -54, epsilon: 0.010000000000022203, ma: -48.01862983407636\n",
      "Episode: 118, reward: -30, epsilon: 0.01000000000002009, ma: -46.21676685066872\n",
      "Episode: 119, reward: -33, epsilon: 0.010000000000017998, ma: -44.89509016560184\n",
      "Episode: 120, reward: -40, epsilon: 0.010000000000015751, ma: -44.40558114904166\n",
      "Episode: 121, reward: -35, epsilon: 0.010000000000014017, ma: -43.465023034137495\n",
      "Episode: 122, reward: -28, epsilon: 0.010000000000012768, ma: -41.91852073072374\n",
      "Episode: 123, reward: -62, epsilon: 0.010000000000010384, ma: -43.92666865765137\n",
      "Episode: 124, reward: -15, epsilon: 0.010000000000009878, ma: -41.03400179188623\n",
      "Episode: 125, reward: -26, epsilon: 0.010000000000009057, ma: -39.53060161269761\n",
      "Episode: 126, reward: -22, epsilon: 0.010000000000008417, ma: -37.77754145142785\n",
      "Episode: 127, reward: -41, epsilon: 0.010000000000007342, ma: -38.099787306285066\n",
      "Episode: 128, reward: -38, epsilon: 0.010000000000006469, ma: -38.089808575656555\n",
      "Episode: 129, reward: -120, epsilon: 0.010000000000006032, ma: -46.2808277180909\n",
      "Episode: 130, reward: -55, epsilon: 0.01000000000000502, ma: -47.15274494628181\n",
      "Episode: 131, reward: -21, epsilon: 0.010000000000004682, ma: -44.537470451653626\n",
      "Episode: 132, reward: -34, epsilon: 0.01000000000000418, ma: -43.48372340648826\n",
      "Episode: 133, reward: -35, epsilon: 0.01000000000000372, ma: -42.63535106583944\n",
      "Episode: 134, reward: -16, epsilon: 0.010000000000003527, ma: -39.971815959255494\n",
      "Episode: 135, reward: -27, epsilon: 0.010000000000003223, ma: -38.67463436332995\n",
      "Episode: 136, reward: -54, epsilon: 0.010000000000002692, ma: -40.20717092699695\n",
      "Episode: 137, reward: -31, epsilon: 0.010000000000002427, ma: -39.28645383429726\n",
      "Episode: 138, reward: -19, epsilon: 0.01000000000000228, ma: -37.257808450867536\n",
      "Episode: 139, reward: -31, epsilon: 0.010000000000002056, ma: -36.632027605780785\n",
      "Episode: 140, reward: -29, epsilon: 0.010000000000001865, ma: -35.8688248452027\n",
      "Episode: 141, reward: -22, epsilon: 0.010000000000001733, ma: -34.48194236068244\n",
      "Episode: 142, reward: -41, epsilon: 0.010000000000001513, ma: -35.13374812461419\n",
      "Episode: 143, reward: -34, epsilon: 0.01000000000000135, ma: -35.02037331215278\n",
      "Episode: 144, reward: -30, epsilon: 0.010000000000001221, ma: -34.5183359809375\n",
      "Episode: 145, reward: -34, epsilon: 0.010000000000001091, ma: -34.46650238284375\n",
      "Episode: 146, reward: -21, epsilon: 0.010000000000001017, ma: -33.119852144559374\n",
      "Episode: 147, reward: -22, epsilon: 0.010000000000000946, ma: -32.00786693010344\n",
      "Episode: 148, reward: -31, epsilon: 0.010000000000000852, ma: -31.907080237093098\n",
      "Episode: 149, reward: -30, epsilon: 0.010000000000000772, ma: -31.71637221338379\n",
      "Episode: 150, reward: -17, epsilon: 0.010000000000000729, ma: -30.24473499204541\n",
      "Episode: 151, reward: -51, epsilon: 0.010000000000000614, ma: -32.32026149284087\n",
      "Episode: 152, reward: -26, epsilon: 0.010000000000000564, ma: -31.688235343556787\n",
      "Episode: 153, reward: -17, epsilon: 0.010000000000000533, ma: -30.21941180920111\n",
      "Episode: 154, reward: -38, epsilon: 0.01000000000000047, ma: -30.997470628281\n",
      "Episode: 155, reward: -27, epsilon: 0.010000000000000429, ma: -30.5977235654529\n",
      "Episode: 156, reward: -31, epsilon: 0.010000000000000387, ma: -30.637951208907612\n",
      "Episode: 157, reward: -16, epsilon: 0.010000000000000366, ma: -29.174156088016854\n",
      "Episode: 158, reward: -24, epsilon: 0.010000000000000338, ma: -28.656740479215166\n",
      "Episode: 159, reward: -32, epsilon: 0.010000000000000304, ma: -28.99106643129365\n",
      "Episode: 160, reward: -23, epsilon: 0.010000000000000281, ma: -28.391959788164286\n",
      "Episode: 161, reward: -32, epsilon: 0.010000000000000253, ma: -28.752763809347858\n",
      "Episode: 162, reward: -30, epsilon: 0.01000000000000023, ma: -28.877487428413072\n",
      "Episode: 163, reward: -16, epsilon: 0.010000000000000217, ma: -27.589738685571767\n",
      "Episode: 164, reward: -34, epsilon: 0.010000000000000194, ma: -28.230764817014588\n",
      "Episode: 165, reward: -23, epsilon: 0.01000000000000018, ma: -27.70768833531313\n",
      "Episode: 166, reward: -34, epsilon: 0.01000000000000016, ma: -28.336919501781814\n",
      "Episode: 167, reward: -24, epsilon: 0.010000000000000148, ma: -27.903227551603635\n",
      "Episode: 168, reward: -21, epsilon: 0.010000000000000139, ma: -27.212904796443272\n",
      "Episode: 169, reward: -22, epsilon: 0.010000000000000129, ma: -26.691614316798944\n",
      "Episode: 170, reward: -15, epsilon: 0.010000000000000122, ma: -25.52245288511905\n",
      "Episode: 171, reward: -35, epsilon: 0.01000000000000011, ma: -26.470207596607146\n",
      "Episode: 172, reward: -24, epsilon: 0.0100000000000001, ma: -26.223186836946432\n",
      "Episode: 173, reward: -29, epsilon: 0.01000000000000009, ma: -26.500868153251787\n",
      "Episode: 174, reward: -39, epsilon: 0.01000000000000008, ma: -27.75078133792661\n",
      "Episode: 175, reward: -37, epsilon: 0.010000000000000071, ma: -28.67570320413395\n",
      "Episode: 176, reward: -21, epsilon: 0.010000000000000066, ma: -27.908132883720555\n",
      "Episode: 177, reward: -17, epsilon: 0.010000000000000063, ma: -26.8173195953485\n",
      "Episode: 178, reward: -29, epsilon: 0.010000000000000057, ma: -27.03558763581365\n",
      "Episode: 179, reward: -19, epsilon: 0.010000000000000054, ma: -26.232028872232284\n",
      "Episode: 180, reward: -25, epsilon: 0.010000000000000049, ma: -26.108825985009055\n",
      "Episode: 181, reward: -21, epsilon: 0.010000000000000045, ma: -25.597943386508152\n",
      "Episode: 182, reward: -25, epsilon: 0.010000000000000042, ma: -25.53814904785734\n",
      "Episode: 183, reward: -25, epsilon: 0.010000000000000038, ma: -25.484334143071607\n",
      "Episode: 184, reward: -29, epsilon: 0.010000000000000035, ma: -25.83590072876445\n",
      "Episode: 185, reward: -22, epsilon: 0.010000000000000033, ma: -25.452310655888006\n",
      "Episode: 186, reward: -18, epsilon: 0.010000000000000031, ma: -24.707079590299205\n",
      "Episode: 187, reward: -17, epsilon: 0.01000000000000003, ma: -23.936371631269285\n",
      "Episode: 188, reward: -24, epsilon: 0.010000000000000026, ma: -23.942734468142355\n",
      "Episode: 189, reward: -34, epsilon: 0.010000000000000024, ma: -24.94846102132812\n",
      "Episode: 190, reward: -35, epsilon: 0.010000000000000021, ma: -25.95361491919531\n",
      "Episode: 191, reward: -17, epsilon: 0.010000000000000021, ma: -25.05825342727578\n",
      "Episode: 192, reward: -28, epsilon: 0.01000000000000002, ma: -25.352428084548205\n",
      "Episode: 193, reward: -14, epsilon: 0.010000000000000018, ma: -24.21718527609338\n",
      "Episode: 194, reward: -18, epsilon: 0.010000000000000016, ma: -23.595466748484046\n",
      "Episode: 195, reward: -27, epsilon: 0.010000000000000016, ma: -23.935920073635643\n",
      "Episode: 196, reward: -28, epsilon: 0.010000000000000014, ma: -24.34232806627208\n",
      "Episode: 197, reward: -17, epsilon: 0.010000000000000012, ma: -23.608095259644873\n",
      "Episode: 198, reward: -16, epsilon: 0.010000000000000012, ma: -22.847285733680387\n",
      "Episode: 199, reward: -32, epsilon: 0.01000000000000001, ma: -23.762557160312348\n",
      "Episode: 200, reward: -134, epsilon: 0.01000000000000001, ma: -34.786301444281115\n",
      "Episode: 201, reward: -17, epsilon: 0.010000000000000009, ma: -33.00767129985301\n",
      "Episode: 202, reward: -16, epsilon: 0.010000000000000009, ma: -31.30690416986771\n",
      "Episode: 203, reward: -15, epsilon: 0.010000000000000009, ma: -29.67621375288094\n",
      "Episode: 204, reward: -19, epsilon: 0.010000000000000009, ma: -28.608592377592846\n",
      "Episode: 205, reward: -25, epsilon: 0.010000000000000007, ma: -28.24773313983356\n",
      "Episode: 206, reward: -14, epsilon: 0.010000000000000007, ma: -26.8229598258502\n",
      "Episode: 207, reward: -20, epsilon: 0.010000000000000007, ma: -26.14066384326518\n",
      "Episode: 208, reward: -31, epsilon: 0.010000000000000005, ma: -26.626597458938665\n",
      "Episode: 209, reward: -156, epsilon: 0.010000000000000005, ma: -39.5639377130448\n",
      "Episode: 210, reward: -15, epsilon: 0.010000000000000005, ma: -37.107543941740325\n",
      "Episode: 211, reward: -30, epsilon: 0.010000000000000004, ma: -36.39678954756629\n",
      "Episode: 212, reward: -25, epsilon: 0.010000000000000004, ma: -35.25711059280967\n",
      "Episode: 213, reward: -21, epsilon: 0.010000000000000004, ma: -33.8313995335287\n",
      "Episode: 214, reward: -16, epsilon: 0.010000000000000004, ma: -32.04825958017583\n",
      "Episode: 215, reward: -24, epsilon: 0.010000000000000004, ma: -31.24343362215825\n",
      "Episode: 216, reward: -18, epsilon: 0.010000000000000004, ma: -29.919090259942426\n",
      "Episode: 217, reward: -141, epsilon: 0.010000000000000002, ma: -41.027181233948184\n",
      "Episode: 218, reward: -18, epsilon: 0.010000000000000002, ma: -38.724463110553366\n",
      "Episode: 219, reward: -17, epsilon: 0.010000000000000002, ma: -36.552016799498034\n",
      "Episode: 220, reward: -16, epsilon: 0.010000000000000002, ma: -34.49681511954823\n",
      "Episode: 221, reward: -14, epsilon: 0.010000000000000002, ma: -32.44713360759341\n",
      "Episode: 222, reward: -23, epsilon: 0.010000000000000002, ma: -31.502420246834067\n",
      "Episode: 223, reward: -29, epsilon: 0.010000000000000002, ma: -31.25217822215066\n",
      "Episode: 224, reward: -20, epsilon: 0.010000000000000002, ma: -30.126960399935594\n",
      "Episode: 225, reward: -27, epsilon: 0.010000000000000002, ma: -29.814264359942033\n",
      "Episode: 226, reward: -15, epsilon: 0.010000000000000002, ma: -28.33283792394783\n",
      "Episode: 227, reward: -31, epsilon: 0.010000000000000002, ma: -28.59955413155305\n",
      "Episode: 228, reward: -15, epsilon: 0.010000000000000002, ma: -27.239598718397744\n",
      "Episode: 229, reward: -13, epsilon: 0.010000000000000002, ma: -25.81563884655797\n",
      "Episode: 230, reward: -126, epsilon: 0.010000000000000002, ma: -35.834074961902175\n",
      "Episode: 231, reward: -19, epsilon: 0.010000000000000002, ma: -34.150667465711955\n",
      "Episode: 232, reward: -25, epsilon: 0.010000000000000002, ma: -33.23560071914076\n",
      "Episode: 233, reward: -17, epsilon: 0.01, ma: -31.612040647226685\n",
      "Episode: 234, reward: -15, epsilon: 0.01, ma: -29.950836582504017\n",
      "Episode: 235, reward: -13, epsilon: 0.01, ma: -28.255752924253617\n",
      "Episode: 236, reward: -16, epsilon: 0.01, ma: -27.030177631828256\n",
      "Episode: 237, reward: -23, epsilon: 0.01, ma: -26.627159868645432\n",
      "Episode: 238, reward: -34, epsilon: 0.01, ma: -27.364443881780886\n",
      "Episode: 239, reward: -18, epsilon: 0.01, ma: -26.427999493602798\n",
      "Episode: 240, reward: -16, epsilon: 0.01, ma: -25.38519954424252\n",
      "Episode: 241, reward: -13, epsilon: 0.01, ma: -24.14667958981827\n",
      "Episode: 242, reward: -19, epsilon: 0.01, ma: -23.63201163083644\n",
      "Episode: 243, reward: -22, epsilon: 0.01, ma: -23.468810467752796\n",
      "Episode: 244, reward: -24, epsilon: 0.01, ma: -23.521929420977514\n",
      "Episode: 245, reward: -16, epsilon: 0.01, ma: -22.769736478879764\n",
      "Episode: 246, reward: -13, epsilon: 0.01, ma: -21.79276283099179\n",
      "Episode: 247, reward: -18, epsilon: 0.01, ma: -21.413486547892614\n",
      "Episode: 248, reward: -19, epsilon: 0.01, ma: -21.17213789310335\n",
      "Episode: 249, reward: -16, epsilon: 0.01, ma: -20.654924103793018\n",
      "Episode: 250, reward: -23, epsilon: 0.01, ma: -20.889431693413716\n",
      "Episode: 251, reward: -126, epsilon: 0.01, ma: -31.400488524072347\n",
      "Episode: 252, reward: -16, epsilon: 0.01, ma: -29.860439671665116\n",
      "Episode: 253, reward: -24, epsilon: 0.01, ma: -29.274395704498602\n",
      "Episode: 254, reward: -29, epsilon: 0.01, ma: -29.246956134048745\n",
      "Episode: 255, reward: -13, epsilon: 0.01, ma: -27.62226052064387\n",
      "Episode: 256, reward: -14, epsilon: 0.01, ma: -26.260034468579484\n",
      "Episode: 257, reward: -22, epsilon: 0.01, ma: -25.834031021721536\n",
      "Episode: 258, reward: -127, epsilon: 0.01, ma: -35.95062791954938\n",
      "Episode: 259, reward: -21, epsilon: 0.01, ma: -34.45556512759445\n",
      "Episode: 260, reward: -18, epsilon: 0.01, ma: -32.810008614835\n",
      "Episode: 261, reward: -14, epsilon: 0.01, ma: -30.9290077533515\n",
      "Episode: 262, reward: -20, epsilon: 0.01, ma: -29.83610697801635\n",
      "Episode: 263, reward: -14, epsilon: 0.01, ma: -28.252496280214714\n",
      "Episode: 264, reward: -15, epsilon: 0.01, ma: -26.927246652193244\n",
      "Episode: 265, reward: -17, epsilon: 0.01, ma: -25.93452198697392\n",
      "Episode: 266, reward: -23, epsilon: 0.01, ma: -25.64106978827653\n",
      "Episode: 267, reward: -13, epsilon: 0.01, ma: -24.376962809448877\n",
      "Episode: 268, reward: -13, epsilon: 0.01, ma: -23.23926652850399\n",
      "Episode: 269, reward: -21, epsilon: 0.01, ma: -23.01533987565359\n",
      "Episode: 270, reward: -13, epsilon: 0.01, ma: -22.013805888088232\n",
      "Episode: 271, reward: -15, epsilon: 0.01, ma: -21.312425299279408\n",
      "Episode: 272, reward: -31, epsilon: 0.01, ma: -22.281182769351467\n",
      "Episode: 273, reward: -13, epsilon: 0.01, ma: -21.353064492416323\n",
      "Episode: 274, reward: -13, epsilon: 0.01, ma: -20.51775804317469\n",
      "Episode: 275, reward: -21, epsilon: 0.01, ma: -20.565982238857224\n",
      "Episode: 276, reward: -14, epsilon: 0.01, ma: -19.9093840149715\n",
      "Episode: 277, reward: -13, epsilon: 0.01, ma: -19.21844561347435\n",
      "Episode: 278, reward: -13, epsilon: 0.01, ma: -18.596601052126918\n",
      "Episode: 279, reward: -26, epsilon: 0.01, ma: -19.33694094691423\n",
      "Episode: 280, reward: -13, epsilon: 0.01, ma: -18.703246852222808\n",
      "Episode: 281, reward: -13, epsilon: 0.01, ma: -18.13292216700053\n",
      "Episode: 282, reward: -14, epsilon: 0.01, ma: -17.719629950300476\n",
      "Episode: 283, reward: -25, epsilon: 0.01, ma: -18.447666955270428\n",
      "Episode: 284, reward: -17, epsilon: 0.01, ma: -18.302900259743385\n",
      "Episode: 285, reward: -13, epsilon: 0.01, ma: -17.772610233769047\n",
      "Episode: 286, reward: -18, epsilon: 0.01, ma: -17.795349210392143\n",
      "Episode: 287, reward: -18, epsilon: 0.01, ma: -17.81581428935293\n",
      "Episode: 288, reward: -13, epsilon: 0.01, ma: -17.334232860417636\n",
      "Episode: 289, reward: -15, epsilon: 0.01, ma: -17.100809574375873\n",
      "Episode: 290, reward: -13, epsilon: 0.01, ma: -16.690728616938287\n",
      "Episode: 291, reward: -15, epsilon: 0.01, ma: -16.52165575524446\n",
      "Episode: 292, reward: -15, epsilon: 0.01, ma: -16.369490179720014\n",
      "Episode: 293, reward: -16, epsilon: 0.01, ma: -16.332541161748015\n",
      "Episode: 294, reward: -13, epsilon: 0.01, ma: -15.999287045573215\n",
      "Episode: 295, reward: -15, epsilon: 0.01, ma: -15.899358341015894\n",
      "Episode: 296, reward: -22, epsilon: 0.01, ma: -16.509422506914305\n",
      "Episode: 297, reward: -13, epsilon: 0.01, ma: -16.158480256222873\n",
      "Episode: 298, reward: -14, epsilon: 0.01, ma: -15.942632230600587\n",
      "Episode: 299, reward: -14, epsilon: 0.01, ma: -15.748369007540528\n",
      "Episode: 300, reward: -15, epsilon: 0.01, ma: -15.673532106786476\n",
      "Episode: 301, reward: -13, epsilon: 0.01, ma: -15.40617889610783\n",
      "Episode: 302, reward: -17, epsilon: 0.01, ma: -15.565561006497049\n",
      "Episode: 303, reward: -13, epsilon: 0.01, ma: -15.309004905847345\n",
      "Episode: 304, reward: -13, epsilon: 0.01, ma: -15.078104415262612\n",
      "Episode: 305, reward: -13, epsilon: 0.01, ma: -14.870293973736352\n",
      "Episode: 306, reward: -13, epsilon: 0.01, ma: -14.683264576362719\n",
      "Episode: 307, reward: -13, epsilon: 0.01, ma: -14.514938118726448\n",
      "Episode: 308, reward: -14, epsilon: 0.01, ma: -14.463444306853804\n",
      "Episode: 309, reward: -13, epsilon: 0.01, ma: -14.317099876168424\n",
      "Episode: 310, reward: -30, epsilon: 0.01, ma: -15.885389888551583\n",
      "Episode: 311, reward: -13, epsilon: 0.01, ma: -15.596850899696426\n",
      "Episode: 312, reward: -13, epsilon: 0.01, ma: -15.337165809726784\n",
      "Episode: 313, reward: -13, epsilon: 0.01, ma: -15.103449228754107\n",
      "Episode: 314, reward: -13, epsilon: 0.01, ma: -14.893104305878698\n",
      "Episode: 315, reward: -13, epsilon: 0.01, ma: -14.70379387529083\n",
      "Episode: 316, reward: -13, epsilon: 0.01, ma: -14.533414487761748\n",
      "Episode: 317, reward: -13, epsilon: 0.01, ma: -14.380073038985573\n",
      "Episode: 318, reward: -14, epsilon: 0.01, ma: -14.342065735087017\n",
      "Episode: 319, reward: -13, epsilon: 0.01, ma: -14.207859161578316\n",
      "Episode: 320, reward: -18, epsilon: 0.01, ma: -14.587073245420486\n",
      "Episode: 321, reward: -19, epsilon: 0.01, ma: -15.028365920878437\n",
      "Episode: 322, reward: -15, epsilon: 0.01, ma: -15.025529328790594\n",
      "Episode: 323, reward: -13, epsilon: 0.01, ma: -14.822976395911535\n",
      "Episode: 324, reward: -13, epsilon: 0.01, ma: -14.640678756320384\n",
      "Episode: 325, reward: -13, epsilon: 0.01, ma: -14.476610880688346\n",
      "Episode: 326, reward: -14, epsilon: 0.01, ma: -14.428949792619512\n",
      "Episode: 327, reward: -13, epsilon: 0.01, ma: -14.286054813357563\n",
      "Episode: 328, reward: -17, epsilon: 0.01, ma: -14.557449332021807\n",
      "Episode: 329, reward: -13, epsilon: 0.01, ma: -14.401704398819628\n",
      "Episode: 330, reward: -13, epsilon: 0.01, ma: -14.261533958937667\n",
      "Episode: 331, reward: -13, epsilon: 0.01, ma: -14.135380563043901\n",
      "Episode: 332, reward: -13, epsilon: 0.01, ma: -14.021842506739512\n",
      "Episode: 333, reward: -13, epsilon: 0.01, ma: -13.919658256065562\n",
      "Episode: 334, reward: -13, epsilon: 0.01, ma: -13.827692430459006\n",
      "Episode: 335, reward: -22, epsilon: 0.01, ma: -14.644923187413106\n",
      "Episode: 336, reward: -13, epsilon: 0.01, ma: -14.480430868671796\n",
      "Episode: 337, reward: -13, epsilon: 0.01, ma: -14.332387781804618\n",
      "Episode: 338, reward: -15, epsilon: 0.01, ma: -14.399149003624157\n",
      "Episode: 339, reward: -13, epsilon: 0.01, ma: -14.259234103261742\n",
      "Episode: 340, reward: -15, epsilon: 0.01, ma: -14.333310692935568\n",
      "Episode: 341, reward: -13, epsilon: 0.01, ma: -14.199979623642012\n",
      "Episode: 342, reward: -17, epsilon: 0.01, ma: -14.479981661277812\n",
      "Episode: 343, reward: -13, epsilon: 0.01, ma: -14.331983495150032\n",
      "Episode: 344, reward: -13, epsilon: 0.01, ma: -14.19878514563503\n",
      "Episode: 345, reward: -13, epsilon: 0.01, ma: -14.078906631071527\n",
      "Episode: 346, reward: -13, epsilon: 0.01, ma: -13.971015967964375\n",
      "Episode: 347, reward: -13, epsilon: 0.01, ma: -13.873914371167938\n",
      "Episode: 348, reward: -13, epsilon: 0.01, ma: -13.786522934051145\n",
      "Episode: 349, reward: -13, epsilon: 0.01, ma: -13.707870640646032\n",
      "Episode: 350, reward: -13, epsilon: 0.01, ma: -13.63708357658143\n",
      "Episode: 351, reward: -17, epsilon: 0.01, ma: -13.973375218923287\n",
      "Episode: 352, reward: -13, epsilon: 0.01, ma: -13.87603769703096\n",
      "Episode: 353, reward: -13, epsilon: 0.01, ma: -13.788433927327864\n",
      "Episode: 354, reward: -13, epsilon: 0.01, ma: -13.709590534595078\n",
      "Episode: 355, reward: -13, epsilon: 0.01, ma: -13.63863148113557\n",
      "Episode: 356, reward: -13, epsilon: 0.01, ma: -13.574768333022014\n",
      "Episode: 357, reward: -15, epsilon: 0.01, ma: -13.717291499719813\n",
      "Episode: 358, reward: -13, epsilon: 0.01, ma: -13.645562349747832\n",
      "Episode: 359, reward: -13, epsilon: 0.01, ma: -13.58100611477305\n",
      "Episode: 360, reward: -13, epsilon: 0.01, ma: -13.522905503295746\n",
      "Episode: 361, reward: -13, epsilon: 0.01, ma: -13.470614952966173\n",
      "Episode: 362, reward: -13, epsilon: 0.01, ma: -13.423553457669557\n",
      "Episode: 363, reward: -13, epsilon: 0.01, ma: -13.381198111902602\n",
      "Episode: 364, reward: -13, epsilon: 0.01, ma: -13.343078300712342\n",
      "Episode: 365, reward: -13, epsilon: 0.01, ma: -13.30877047064111\n",
      "Episode: 366, reward: -13, epsilon: 0.01, ma: -13.277893423577\n",
      "Episode: 367, reward: -13, epsilon: 0.01, ma: -13.2501040812193\n",
      "Episode: 368, reward: -13, epsilon: 0.01, ma: -13.225093673097371\n",
      "Episode: 369, reward: -13, epsilon: 0.01, ma: -13.202584305787635\n",
      "Episode: 370, reward: -13, epsilon: 0.01, ma: -13.182325875208873\n",
      "Episode: 371, reward: -13, epsilon: 0.01, ma: -13.164093287687987\n",
      "Episode: 372, reward: -13, epsilon: 0.01, ma: -13.147683958919188\n",
      "Episode: 373, reward: -13, epsilon: 0.01, ma: -13.13291556302727\n",
      "Episode: 374, reward: -14, epsilon: 0.01, ma: -13.219624006724544\n",
      "Episode: 375, reward: -13, epsilon: 0.01, ma: -13.19766160605209\n",
      "Episode: 376, reward: -13, epsilon: 0.01, ma: -13.177895445446882\n",
      "Episode: 377, reward: -13, epsilon: 0.01, ma: -13.160105900902195\n",
      "Episode: 378, reward: -13, epsilon: 0.01, ma: -13.144095310811977\n",
      "Episode: 379, reward: -13, epsilon: 0.01, ma: -13.12968577973078\n",
      "Episode: 380, reward: -13, epsilon: 0.01, ma: -13.116717201757703\n",
      "Episode: 381, reward: -13, epsilon: 0.01, ma: -13.105045481581934\n",
      "Episode: 382, reward: -13, epsilon: 0.01, ma: -13.094540933423742\n",
      "Episode: 383, reward: -13, epsilon: 0.01, ma: -13.085086840081368\n",
      "Episode: 384, reward: -19, epsilon: 0.01, ma: -13.676578156073232\n",
      "Episode: 385, reward: -13, epsilon: 0.01, ma: -13.60892034046591\n",
      "Episode: 386, reward: -13, epsilon: 0.01, ma: -13.548028306419319\n",
      "Episode: 387, reward: -13, epsilon: 0.01, ma: -13.493225475777388\n",
      "Episode: 388, reward: -120, epsilon: 0.01, ma: -24.14390292819965\n",
      "Episode: 389, reward: -13, epsilon: 0.01, ma: -23.02951263537969\n",
      "Episode: 390, reward: -13, epsilon: 0.01, ma: -22.026561371841723\n",
      "Episode: 391, reward: -13, epsilon: 0.01, ma: -21.123905234657553\n",
      "Episode: 392, reward: -13, epsilon: 0.01, ma: -20.311514711191798\n",
      "Episode: 393, reward: -13, epsilon: 0.01, ma: -19.580363240072618\n",
      "Episode: 394, reward: -13, epsilon: 0.01, ma: -18.92232691606536\n",
      "Episode: 395, reward: -13, epsilon: 0.01, ma: -18.330094224458826\n",
      "Episode: 396, reward: -119, epsilon: 0.01, ma: -28.39708480201294\n",
      "Episode: 397, reward: -13, epsilon: 0.01, ma: -26.85737632181165\n",
      "Episode: 398, reward: -13, epsilon: 0.01, ma: -25.471638689630485\n",
      "Episode: 399, reward: -13, epsilon: 0.01, ma: -24.22447482066744\n"
     ]
    }
   ],
   "source": [
    "episode = 400\n",
    "rewards = []\n",
    "ma_rewards = []\n",
    "for i in range(episode):\n",
    "    ep_reward = 0\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        agent.qlearn(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(ep_reward)\n",
    "    if ma_rewards:\n",
    "        ma_rewards.append(ma_rewards[-1]*0.9+ep_reward*0.1)\n",
    "    else :\n",
    "        ma_rewards.append(ep_reward)\n",
    "    print(\"Episode: {}, reward: {}, epsilon: {}, ma: {}\".format(i,ep_reward,agent.epsilon,ma_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.81727263e+00, -6.82303976e+00, -6.84089944e+00,\n",
       "        -6.83580475e+00],\n",
       "       [-6.67696820e+00, -6.64951068e+00, -6.68688604e+00,\n",
       "        -6.65207030e+00],\n",
       "       [-6.41489390e+00, -6.41361422e+00, -6.43292449e+00,\n",
       "        -6.42680430e+00],\n",
       "       [-6.15451645e+00, -6.13697066e+00, -6.16192968e+00,\n",
       "        -6.15802587e+00],\n",
       "       [-5.83962153e+00, -5.82556983e+00, -5.82538417e+00,\n",
       "        -5.86570196e+00],\n",
       "       [-5.47772527e+00, -5.48240972e+00, -5.47904396e+00,\n",
       "        -5.48963860e+00],\n",
       "       [-5.08144824e+00, -5.08498776e+00, -5.11963128e+00,\n",
       "        -5.15465335e+00],\n",
       "       [-4.69958578e+00, -4.66589370e+00, -4.65973157e+00,\n",
       "        -4.69612964e+00],\n",
       "       [-4.19172998e+00, -4.20218981e+00, -4.21274136e+00,\n",
       "        -4.23822864e+00],\n",
       "       [-3.71162809e+00, -3.70573302e+00, -3.69913480e+00,\n",
       "        -3.72432403e+00],\n",
       "       [-3.21000421e+00, -3.16447482e+00, -3.17091674e+00,\n",
       "        -3.23551514e+00],\n",
       "       [-2.65362525e+00, -2.65688320e+00, -2.62482864e+00,\n",
       "        -2.63267828e+00],\n",
       "       [-6.96937553e+00, -6.95269578e+00, -7.00059729e+00,\n",
       "        -6.97847676e+00],\n",
       "       [-6.74846805e+00, -6.73492435e+00, -6.73493035e+00,\n",
       "        -6.76891357e+00],\n",
       "       [-6.45965202e+00, -6.46442715e+00, -6.48869571e+00,\n",
       "        -6.48427258e+00],\n",
       "       [-6.16243768e+00, -6.15422448e+00, -6.14725024e+00,\n",
       "        -6.14988735e+00],\n",
       "       [-5.84157571e+00, -5.79836953e+00, -5.79933987e+00,\n",
       "        -5.81803449e+00],\n",
       "       [-5.44189914e+00, -5.40869158e+00, -5.43011605e+00,\n",
       "        -5.40645253e+00],\n",
       "       [-5.02911999e+00, -4.98585705e+00, -4.98326975e+00,\n",
       "        -5.05679032e+00],\n",
       "       [-4.55744368e+00, -4.49216427e+00, -4.51014607e+00,\n",
       "        -4.57194811e+00],\n",
       "       [-4.03228311e+00, -3.95213970e+00, -3.96211197e+00,\n",
       "        -4.03541862e+00],\n",
       "       [-3.42213233e+00, -3.35225825e+00, -3.35390498e+00,\n",
       "        -3.37318403e+00],\n",
       "       [-2.76675041e+00, -2.67306839e+00, -2.67081621e+00,\n",
       "        -2.79167304e+00],\n",
       "       [-2.03780624e+00, -1.94378775e+00, -1.89907250e+00,\n",
       "        -1.90111096e+00],\n",
       "       [-7.18191186e+00, -7.17564727e+00, -7.29356401e+00,\n",
       "        -7.19287158e+00],\n",
       "       [-6.87252430e+00, -6.86187568e+00, -1.85320554e+01,\n",
       "        -6.94066360e+00],\n",
       "       [-6.53605259e+00, -6.51321075e+00, -4.24638446e+01,\n",
       "        -6.55801845e+00],\n",
       "       [-6.12691864e+00, -6.12579407e+00, -4.17708196e+01,\n",
       "        -6.13624444e+00],\n",
       "       [-5.73788696e+00, -5.69532772e+00, -2.75608292e+01,\n",
       "        -5.74790438e+00],\n",
       "       [-5.23497770e+00, -5.21703098e+00, -4.12078007e+01,\n",
       "        -5.24092573e+00],\n",
       "       [-4.88118948e+00, -4.68559000e+00, -3.51592383e+01,\n",
       "        -4.78078009e+00],\n",
       "       [-4.09862506e+00, -4.09510000e+00, -2.79338044e+01,\n",
       "        -4.10018908e+00],\n",
       "       [-3.55956707e+00, -3.43900000e+00, -4.29187503e+01,\n",
       "        -3.45592436e+00],\n",
       "       [-2.86106676e+00, -2.71000000e+00, -1.95252509e+01,\n",
       "        -2.79158709e+00],\n",
       "       [-2.03832614e+00, -1.90000000e+00, -9.56424097e+00,\n",
       "        -2.08249746e+00],\n",
       "       [-1.36748631e+00, -1.08402813e+00, -1.00000000e+00,\n",
       "        -1.19489671e+00],\n",
       "       [-7.45798300e+00, -6.60931911e+01, -7.47061033e+00,\n",
       "        -7.48833657e+00],\n",
       "       [ 4.31906636e-01,  7.12656397e-01,  8.39782829e-01,\n",
       "         7.57123199e-01],\n",
       "       [ 8.48992172e-01,  6.64893983e-01,  6.36923438e-01,\n",
       "         6.93678039e-01],\n",
       "       [ 3.28333388e-01,  5.46316824e-01,  5.69974043e-01,\n",
       "         9.67241202e-01],\n",
       "       [ 3.31801822e-01,  1.21230923e-01,  2.39923141e-01,\n",
       "         3.64274094e-01],\n",
       "       [ 1.24355801e-03,  3.33353418e-01,  6.63302260e-01,\n",
       "         4.04719753e-01],\n",
       "       [ 9.82155680e-01,  9.57378039e-01,  3.44811008e-03,\n",
       "         4.40453184e-01],\n",
       "       [ 5.90458696e-01,  6.96732757e-01,  7.13335930e-01,\n",
       "         3.11132957e-01],\n",
       "       [ 7.99729691e-01,  3.28875694e-01,  3.66644906e-01,\n",
       "         6.67337707e-01],\n",
       "       [ 4.05482175e-01,  6.46708318e-01,  1.22139541e-01,\n",
       "         3.82709363e-01],\n",
       "       [ 6.83360350e-01,  7.75452350e-01,  7.60409820e-01,\n",
       "         6.66505942e-01],\n",
       "       [ 3.83396513e-01,  6.33883437e-01,  3.46390905e-01,\n",
       "         7.68218136e-01]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看路径："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 up\n",
      "25 right\n",
      "26 right\n",
      "27 right\n",
      "28 right\n",
      "29 right\n",
      "30 right\n",
      "31 right\n",
      "32 right\n",
      "33 right\n",
      "34 right\n",
      "35 down\n"
     ]
    }
   ],
   "source": [
    "action_lang=[\"up\",\"right\",\"down\",\"left\"]\n",
    "print(36,action_lang[np.argmax(agent.Q[36,:])])\n",
    "for i in range(25,36):\n",
    "    print(i,action_lang[np.argmax(agent.Q[i,:])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 习题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 构成马尔可夫决策过程的四元组**\n",
    "\n",
    "$(state,action,reward,P)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 学习**\n",
    "\n",
    "exploration & exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Sarsa 学习过程**\n",
    "\n",
    "$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha ( R_{t+1} + \\gamma Q(s_{t+1},a_{t+1} - Q(s,a)) )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. QLearning 与 Sarsa 区别**\n",
    "\n",
    "QL：异策略，大胆探索，不需要知道下一步的策略（直接贪心）\n",
    "Sa：同策略，保守探索，需要知道下一步的策略\n",
    "\n",
    "一般来说 Sarsa 显得更加保守和安全，而 QLearning 更能找到最佳策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 策略梯度\n",
    "\n",
    "> 9/12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 策略梯度算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习具有三个组成部分：\n",
    "- 演员 actor\n",
    "- 环境 env\n",
    "- 奖励函数 reward\n",
    "\n",
    "环境 与 奖励函数 都不是我们可以控制的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 策略函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**策略** 一般记作 $\\pi$，在基于模型的学习中 策略是一个函数\n",
    "\n",
    "若是采用深度学习方法来做强化学习，策略就是一个网络，网络的参数记作 $\\theta$\n",
    "\n",
    "**例如：**\n",
    "\n",
    "给网络输入一个游戏画面；输出是可以执行的动作的概率\n",
    "\n",
    "有三个动作就有三个输出神经元\n",
    "\n",
    "经过 softmax 操作后就得到了概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 轨迹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**轨迹** 就是一个回合内环境输出的状态s和演员输出的动作a的组合\n",
    "\n",
    "$\\tau = \\{ s_1,a_1,s_2,a_2...,s_t,a_t \\}$\n",
    "\n",
    "根据给定的参数 $\\theta$ 可以计算某个轨迹的发生概率：\n",
    "\n",
    "$p_\\theta(\\tau) = p(s_1)p_\\theta(a_1|s_1)p(s_2|s_1,a_1)p_\\theta(a_2|s_2)...$\n",
    "\n",
    "即：\n",
    "\n",
    "![](./img/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个式子中：\n",
    "\n",
    "$p(s_t+1|s_t,a_t)$ 涉及到状态转移，因此是环境决定的\n",
    "\n",
    "$p_\\theta(a_t|s_t)$ 是可以控制的，即：**给定参数 $\\theta$,在 $s_t$ 条件下选择 $a_t$ 的概率**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 奖励函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奖励函数：$reward_t = r(s_t,a_t)$\n",
    "\n",
    "将一条轨迹上所有s-a的奖励加起来，就是轨迹的回报：$R(\\tau)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**学习的目的就是通过调整 $\\theta$，使 $R(\\tau)$ 的值取到最大**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R(\\tau)$ 是一个随机变量，其所有轨迹的概率加权和就是期望奖励\n",
    "\n",
    "$R_\\theta=\\sum R(\\tau)p_\\theta(\\tau)$\n",
    "\n",
    "使用 **梯度上升** 最大化期望奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际期望值难以计算，利用蒙特卡洛思想 采用采样的方式：\n",
    "\n",
    "**采样 N 个轨迹并计算得到的回报，加起来就得到了梯度**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/16.png)\n",
    "\n",
    "n 指第n回合，t 指第t时间步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 采样，得到轨迹\n",
    "- 代入轨迹，得到梯度\n",
    "- 通过梯度来更新参数 $\\theta$ : $\\theta \\leftarrow \\theta + \\eta \\nabla R_\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$\\eta$ 是学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新一遍后，重新采样数据再更新模型，一次采样只使用一次（一次采样多个回合）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以认为强化学习是一个分类问题\n",
    "\n",
    "一般分类问题 目标函数写成最小化交叉熵，即最大化:\n",
    "\n",
    "![](./img/17.png)\n",
    "\n",
    "在强化学习中，还要在前面乘一个权重 轨迹回报 $R(\\tau)$\n",
    "\n",
    "![](./img/18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 添加基线\n",
    "- 分配合适的分数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **添加基线**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：**\n",
    "\n",
    "理想情况下，目标函数的值来自于对大量轨迹采样的期望值；然而现实情况下的采样只是一小部分。\n",
    "\n",
    "如果没有采样到动作a，即使a是一个不错的动作，它也会被忽略（改率降低）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解决：**\n",
    "\n",
    "对 $R(\\tau)$ 添加一条基线，选取某个奖励值 $b$，在计算 $\\nabla R_\\theta$ 时，将所有奖励值都减去 $b$。得到 **相对优势**\n",
    "\n",
    "即：![](./img/19.png)\n",
    "\n",
    "这样使得权重值有正有负，从而使得梯度下降方向更加合理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b$ 通常是由一个网络 (评论员网络) 估计出来的，$R-b$ 称为 **优势函数**\n",
    "\n",
    "$A^\\theta (s_t,a_t) = R-b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **分配合适分数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：**\n",
    "\n",
    "我们需要的是评价某个状态下的策略的好坏，然而根据轨迹的回报来计算是不公正的：\n",
    "\n",
    "<u>一条好的轨迹不代表其中的动作都是好的；而一条坏的轨迹却可能包含很多好的动作。</u>\n",
    "\n",
    "**解决：**\n",
    "\n",
    "在计算某个 s-a 的奖励时，不把整个轨迹的回报代入，而是计算这个动作执行之后得到的的所有奖励\n",
    "\n",
    "并且引入折扣因子 $\\gamma$ 用于调整对长短期收益的关注程度： $r_{a_t}=r_1 + \\gamma  r_2 + \\gamma ^2  r_3 + ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE 蒙特卡洛策略梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 蒙特卡洛方法：使用采样到的某个s-a的回报期望来近似其策略q值：$q_\\pi(s_t,a_t) = q_t(s_t,a_t) = r_t + \\gamma  r_{t+1} + \\gamma ^2  r_{t+2} + ...$\n",
    "- 策略梯度算法：使用 $\\pi (\\theta)$ 策略函数代替 Q 表格，利用梯度更新参数：$\\theta \\leftarrow \\theta + \\alpha  \\nabla_\\theta J(\\pi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 策略模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用深度神经网络作为策略模型 $\\pi_\\theta (s_t,a_t)$ 输入状态 $s_t,a_t$ 输出动作概率 $p_\\theta(a | s_t)$\n",
    "\n",
    "在神经网络中：\n",
    "\n",
    "- 输入层：状态 $s_t$， `num_inputs = num_states`\n",
    "- 隐藏层：神经元\n",
    "- 输出层：动作概率 $p_\\theta(a | s_t)$ `num_outputs = num_actions`\n",
    "\n",
    "最后，还需要加上一层 softmax 层，将输出转换为概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128,lr = 1e-3):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.num_actions = action_size\n",
    "        self.num_states = state_size\n",
    "        self.lr = lr\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 智能体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 选择动作：概率采样\n",
    "- 计算收益：\n",
    "$$\n",
    "G_t=\\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}\n",
    "$$\n",
    "- 更新参数：策略梯度算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import autograd\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, cfg):\n",
    "        self.gamma = cfg.gamma\n",
    "        self.name = cfg.name\n",
    "        self.env = env\n",
    "        self.state_size = cfg.state_size\n",
    "        self.action_size = cfg.action_size\n",
    "        self.net = PolicyNet(self.state_size, self.action_size,lr=cfg.lr)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        # 使用网络得到概率分布\n",
    "        # print(\">>> \",type(state),state.size())\n",
    "        probs = self.net(state)\n",
    "        # 预测时不参与计算梯度\n",
    "        choice = np.random.choice(self.action_size, p=probs[0].detach().numpy())\n",
    "        # 对数化\n",
    "        log_prob = torch.log(probs.squeeze(0)[choice])\n",
    "        return choice, log_prob\n",
    "    \n",
    "    def get_action(self, state): # for test\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        return np.argmax(self.net(state)[0].detach().numpy())\n",
    "    \n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = []\n",
    "        # 计算每条轨迹的折扣奖励\n",
    "        for t in range(len(rewards)):\n",
    "            Gt = 0\n",
    "            pw = 0\n",
    "            for reward in rewards[t:]:\n",
    "                Gt += reward * (self.gamma ** pw)\n",
    "                pw += 1\n",
    "            discounted_rewards.append(Gt)\n",
    "        return discounted_rewards\n",
    "    \n",
    "    def update_policy(self, log_probs, rewards):\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "        discounted_rewards = torch.tensor(discounted_rewards,dtype=torch.float)\n",
    "        # 对回报作归一化\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "        # 计算梯度\n",
    "        policy_grad = []\n",
    "        for log_prob, discounted_reward in zip(log_probs, discounted_rewards):\n",
    "            policy_grad.append(-discounted_reward * log_prob)\n",
    "        # 优化\n",
    "        self.net.optimizer.zero_grad()\n",
    "        pg = torch.stack(policy_grad).sum()\n",
    "        pg.backward()\n",
    "        self.net.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`CarPole-v0` 环境：**\n",
    "\n",
    "控制一个小车向左或向右移动，目的是使小车上的摆件保持在竖直位置上。每一步使得摆件竖直都会得到 1 单位奖励。\n",
    "\n",
    "动作空间：0，1 表示向左和向右\n",
    "\n",
    "观察空间：一个四维连续空间，包含小车位置、速度和摆件角度、角速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n, env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.num_episodes = 2000\n",
    "        self.gamma = 0.9\n",
    "        self.name = 'DeepPolicyGradient'\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.max_steps = 500\n",
    "        self.lr = 0.0005\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**每得到一条轨迹，就对参数进行更新**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env,agent,cfg):\n",
    "    print('Training Starts with LR {}'.format(cfg.lr))\n",
    "    num_steps = []\n",
    "    avg_num_steps = []\n",
    "    rewards = []\n",
    "    for ep in range(cfg.num_episodes): # 回合开始\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        ep_rewards = []\n",
    "        for step in range(cfg.max_steps): # 每个回合的步数\n",
    "            action, log_prob = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            log_probs.append(log_prob)\n",
    "            ep_rewards.append(reward)\n",
    "            if done:\n",
    "                # 得到一条轨迹\n",
    "                agent.update_policy(log_probs,ep_rewards)\n",
    "                num_steps.append(step)\n",
    "                avg_num_steps.append(np.mean(num_steps[-10:]))\n",
    "                rewards.append(sum(ep_rewards))\n",
    "                if ep % 100 == 0:\n",
    "                    print('Episode: {}/{}, Steps: {}, avg_R:{}'.format(ep, cfg.num_episodes, step,np.mean(avg_num_steps)))\n",
    "\n",
    "                break\n",
    "            state = next_state\n",
    "    return num_steps, avg_num_steps, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_plot(data1, data2, data3, label1, label2, label3, xlabel):\n",
    "    plt.plot(data1)\n",
    "    plt.plot(data2)\n",
    "    plt.plot(data3)\n",
    "    plt.legend([label1, label2,label3])\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**开始训练：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Starts with LR 0.0005\n",
      "Episode: 0/2000, Steps: 27, avg_R:27.0\n",
      "Episode: 100/2000, Steps: 51, avg_R:17.705174446016027\n",
      "Episode: 200/2000, Steps: 42, avg_R:24.176231935560292\n",
      "Episode: 300/2000, Steps: 60, avg_R:31.76652032906186\n",
      "Episode: 400/2000, Steps: 135, avg_R:45.50230079562997\n",
      "Episode: 500/2000, Steps: 140, avg_R:65.96930662484554\n",
      "Episode: 600/2000, Steps: 199, avg_R:83.87774146264164\n",
      "Episode: 700/2000, Steps: 199, avg_R:96.85695095441883\n",
      "Episode: 800/2000, Steps: 188, avg_R:108.18716931216933\n",
      "Episode: 900/2000, Steps: 199, avg_R:117.17494186353787\n",
      "Episode: 1000/2000, Steps: 168, avg_R:124.59842419485275\n",
      "Episode: 1100/2000, Steps: 129, avg_R:128.76359910903508\n",
      "Episode: 1200/2000, Steps: 199, avg_R:132.0468964355101\n",
      "Episode: 1300/2000, Steps: 199, avg_R:136.93353006844552\n",
      "Episode: 1400/2000, Steps: 199, avg_R:141.07239301859215\n",
      "Episode: 1500/2000, Steps: 199, avg_R:144.6146053424701\n",
      "Episode: 1600/2000, Steps: 199, avg_R:147.89351818803723\n",
      "Episode: 1700/2000, Steps: 199, avg_R:150.62464586657708\n",
      "Episode: 1800/2000, Steps: 199, avg_R:153.06575381401868\n",
      "Episode: 1900/2000, Steps: 199, avg_R:155.21805503369154\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env,cfg)\n",
    "num_steps, avg_num_steps, rewards = train(env,agent,cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_plot(num_steps,avg_num_steps,[],'num_steps','avg_num_steps', 'reward','episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNet(\n",
       "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent,env,episodes=10,max_step=500,render=False):\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    for episode in range(episodes):\n",
    "        env.render() if render else None\n",
    "        episode_reward = 0\n",
    "        state = env.reset()\n",
    "        for step in range(max_step):\n",
    "            next_state,reward,done,_ = env.step(agent.get_action(state))\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                rewards.append(episode_reward)\n",
    "                steps.append(step+1)\n",
    "                print('Episode: {}, Steps: {}, R:{}'.format(episode, step+1,episode_reward))\n",
    "                break\n",
    "            state = next_state\n",
    "    return rewards,steps\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Steps: 200, R:200.0\n",
      "Episode: 1, Steps: 200, R:200.0\n",
      "Episode: 2, Steps: 200, R:200.0\n",
      "Episode: 3, Steps: 200, R:200.0\n",
      "Episode: 4, Steps: 200, R:200.0\n",
      "Episode: 5, Steps: 200, R:200.0\n",
      "Episode: 6, Steps: 200, R:200.0\n",
      "Episode: 7, Steps: 200, R:200.0\n",
      "Episode: 8, Steps: 200, R:200.0\n",
      "Episode: 9, Steps: 200, R:200.0\n"
     ]
    }
   ],
   "source": [
    "rewards,steps = test(agent,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Steps: 200, R:200.0\n"
     ]
    }
   ],
   "source": [
    "test(agent,env,1,render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 习题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 一个具体的轨迹 s1,a1,s2,a2 出现的概率取决于什么？**\n",
    "\n",
    "环境模型的状态转移概率 $p(s_{t+1}|s_t,a_t)$ 和智能体模型的动作选择概率 $p_\\theta(a_t|s_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 最大化期望奖励时 应该使用什么方法**\n",
    "\n",
    "梯度上升法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 理解策略梯度公式**\n",
    "\n",
    "![](./img/pg.png)\n",
    "\n",
    "如果在 st 时刻执行动作 at 使得最后的回报为正，就加大 at 的概率，反之则减小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. 梯度提升计算的方法**\n",
    "\n",
    "深度学习 学习器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. 优化技巧**\n",
    "\n",
    "添加基线 分配合适分数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. 蒙特卡洛强化学习 和 时序差分强化学习的区别**\n",
    "\n",
    "蒙特卡洛需要取得一条完整的轨迹再进行学习\n",
    "\n",
    "时序差分法不需要完整轨迹，只需要一个状态和下一个状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REINFORCE 算法的计算过程**\n",
    "\n",
    "- 确定策略模型\n",
    "- 采样轨迹\n",
    "- 优化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 近端策略优化\n",
    "\n",
    "> 9/15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "近端策略优化（proximal policy optimization，PPO）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**同策略训练方法：** 与环境交互的智能体和学习的智能体是相同的，如策略梯度、sarsa\n",
    "\n",
    "**异策略训练方法：** 学习的智能体和环境交互的智能体是不同的，如 q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略梯度是同策略的算法，算法中需要：\n",
    "\n",
    "- 智能体\n",
    "- 策略\n",
    "- 演员\n",
    "\n",
    "演员的作用是与环境交互获得采样数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：**\n",
    "\n",
    "由于是同策略算法，使用待学习的模型来与环境交互，那么每次更新完参数后都会得到一个新的模型，原有的采样数据不能使用了，这样就非常花费时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解决：**\n",
    "\n",
    "变成异策略的：设计使得 $\\theta$ 能够 使用演员 $\\theta '$ 交互采样到的数据来训练\n",
    "\n",
    "这样： $\\theta '$ 只需要采样一次就可以让 $\\theta $ 多次训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重要性采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**背景：**\n",
    "- 函数 $f(x)$\n",
    "- 需要计算从分布 $p$ 采样数据，代入函数 $f(x)$ \n",
    "- 但是不能直接从 分布 $p$ 采样\n",
    "- 只能从另一个分布 $q$ 采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算期望就是积分求平均，因此：\n",
    "\n",
    "$\\int f(x)p(x)dx = \\int f(x) \\frac{p(x)}{q(x)} q(x)dx = \\int \\frac{p(x)}{q(x)} f(x)q(x)dx$\n",
    "\n",
    "即：\n",
    "\n",
    "![](./img/20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 $q$ 采样出来的数据，都需要乘以 **重要性权重** $\\frac{p(x)}{q(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q$ 可以是任意分布，只要在 $q=0$ 时 $p \\neq 0$。 但在实现上，二者差距不能太大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO 实现了对两个分布差距太大的避免"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 近端策略优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL 散度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了确保训练出来的 $\\theta '$ 与 真实 $\\theta $ 之间相差不大，应添加一个约束，即 $\\theta '$ 与 $\\theta$ 输出动作的 KL散度，用于衡量两者之间的相似程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL 散度可以认为是两种决策概率分布导致的输出动作之间的距离 （而不是两个参数之间的距离）\n",
    "\n",
    "因为在强化学习中在意的是动作的差距，而不是参数的差距：\n",
    "\n",
    "- 参数上很大的改变，可能对输出动作的影响很小\n",
    "- 参数上很小的改变，可能对输出动作的影响很大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于用于学习的 $\\theta '$ 与用于交互的 $\\theta $ 非常相似，因此可以认为是同一个策略\n",
    "\n",
    "**PPO 依然是同策略算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 近端策略优化惩罚 PPO1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要优化的目标函数：\n",
    "\n",
    "![](./img/21.png)\n",
    "\n",
    "$A^{\\theta '}(s_t,a_t)$ 表示状态 s_t 采取动作 a_t 的优势函数，$\\theta$ 是参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**采用惩罚方式进行优化：**\n",
    "\n",
    "**$J^{\\theta '}_{PPO}(\\theta) = J^{\\theta '}(\\theta)-\\beta KL(\\theta,\\theta ')$**\n",
    "\n",
    "其中：$\\beta$ 是一个超参数，用于控制KL散度的惩罚程度\n",
    "\n",
    "可以事先确定一个 KL 散度的范围 $[KL_{min} , KL_{max}]$\n",
    "\n",
    "- 如果上一轮计算出来的 KL 散度 $KL(\\theta,\\theta ') < KL_{min}$，说明模型更新过快，需要降低学习率；\n",
    "- 如果上一轮计算出来的 KL 散度 $KL(\\theta,\\theta ') > KL_{max}$，说明模型更新过慢，需要增加学习率；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 近端策略优化裁剪 PPO2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO2 的目标函数里不需要计算 KL散度：\n",
    "\n",
    "![](./img/22.png)\n",
    "\n",
    "- $clip$ 裁剪函数 （`torch.clamp`）\n",
    "\n",
    "\n",
    "$\n",
    "clip(a,l,r) =\n",
    "\\begin{cases}\n",
    "l, & \\text{if } a < l \\\\\n",
    "a, & \\text{if } l < a < r \\\\\n",
    "r, & \\text{if } a > r\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "- $\\epsilon$ 是一个超参数，一般设置为 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个方法的目的也是控制 $\\theta$ 与 $\\theta '$ 之间的差距，但是这个方法不使用 KL散度 进行正则化，而是采用一个裁剪法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO 解决 CarPole-v1 问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演员，负责与环境交互获取经验。这个网络输出动作概率，需要加上一层softmax层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        probs = F.softmax(self.l3(x),dim=1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评论员，负责做出策略评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim,hidden_size=256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 经验回放池"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO 学习采用的是整条轨迹，因此回放池中采样的是全部的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGReplay:\n",
    "    def __init__(self, capacity=300):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "\n",
    "    def push(self, state, action, log_prob, reward, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, log_prob, reward, done))\n",
    "\n",
    "    def sample(self):\n",
    "        return zip(*list(self.buffer))\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 智能体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 动作采样函数 `sample(state)`\n",
    "- 动作预测函数 `predict(state)`\n",
    "- 参数更新函数 `update()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "class Agent:\n",
    "    def __init__(self,cfg) -> None:\n",
    "        \n",
    "        self.gamma = cfg.gamma\n",
    "        self.device = torch.device(cfg.device) \n",
    "        self.actor = Actor(cfg.n_states,cfg.n_actions, hidden_size = cfg.actor_hidden_dim).to(self.device)\n",
    "        self.critic = Critic(cfg.n_states,1,hidden_size=cfg.critic_hidden_dim).to(self.device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=cfg.actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=cfg.critic_lr)\n",
    "        self.memory = PGReplay()\n",
    "        self.k_epochs = cfg.k_epochs # update policy for K epochs\n",
    "        self.eps_clip = cfg.eps_clip # clip parameter for PPO\n",
    "        self.entropy_coef = cfg.entropy_coef # entropy coefficient\n",
    "        self.sample_count = 0\n",
    "        self.update_freq = cfg.update_freq\n",
    "\n",
    "    def sample(self,state):\n",
    "        self.sample_count += 1\n",
    "        state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        self.log_prob = dist.log_prob(action).detach()\n",
    "        return action.detach().cpu().numpy().item()\n",
    "    @torch.no_grad()\n",
    "    def predict(self,state):\n",
    "        state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.detach().cpu().numpy().item()\n",
    "    def update(self):\n",
    "        # update policy every n steps\n",
    "        if self.sample_count % self.update_freq != 0:\n",
    "            return\n",
    "        # print(\"update policy\")\n",
    "        old_states, old_actions, old_log_probs, old_rewards, old_dones = self.memory.sample()\n",
    "        # convert to tensor\n",
    "        old_states = torch.tensor(np.array(old_states), device=self.device, dtype=torch.float32)\n",
    "        old_actions = torch.tensor(np.array(old_actions), device=self.device, dtype=torch.float32)\n",
    "        old_log_probs = torch.tensor(old_log_probs, device=self.device, dtype=torch.float32)\n",
    "        # monte carlo estimate of state rewards\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for reward, done in zip(reversed(old_rewards), reversed(old_dones)):\n",
    "            if done:\n",
    "                discounted_sum = 0\n",
    "            discounted_sum = reward + (self.gamma * discounted_sum)\n",
    "            returns.insert(0, discounted_sum)\n",
    "        # Normalizing the rewards:\n",
    "        returns = torch.tensor(returns, device=self.device, dtype=torch.float32)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-5) # 1e-5 to avoid division by zero\n",
    "        for _ in range(self.k_epochs):\n",
    "            # compute advantage\n",
    "            values = self.critic(old_states) # detach to avoid backprop through the critic\n",
    "            advantage = returns - values.detach()\n",
    "            # get action probabilities\n",
    "            probs = self.actor(old_states)\n",
    "            dist = Categorical(probs)\n",
    "            # get new action probabilities\n",
    "            new_probs = dist.log_prob(old_actions)\n",
    "            # compute ratio (pi_theta / pi_theta__old):\n",
    "            ratio = torch.exp(new_probs - old_log_probs) # old_log_probs must be detached\n",
    "            # compute surrogate loss\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantage\n",
    "            # compute actor loss\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() + self.entropy_coef * dist.entropy().mean()\n",
    "            # compute critic loss\n",
    "            critic_loss = (returns - values).pow(2).mean()\n",
    "            # take gradient step\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "        self.memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证：计算未来期望回报"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 14, 12, 9, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def returns_counter(rewards,dones):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "        if done:\n",
    "            R = 0\n",
    "        R = reward + 1 * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "r=[1,2,3,4,5]\n",
    "d=[False,False,False,False,True]\n",
    "returns_counter(r,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def train(cfg,env,agent):\n",
    "    print('Start training...')\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    best_ep_reward = 0\n",
    "    for ep in range(cfg.train_eps):\n",
    "        ep_reward = 0\n",
    "        ep_step = 0\n",
    "        state = env.reset()\n",
    "        for _ in range(cfg.max_steps):\n",
    "            ep_step += 1\n",
    "            action = agent.sample(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.memory.push(state, action, agent.log_prob, reward, done)\n",
    "            state = next_state\n",
    "            agent.update()\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        if (ep+1) % cfg.eval_per_episode == 0:\n",
    "            sum_eval_reward = 0\n",
    "            for _ in range(cfg.eval_eps):\n",
    "                eval_state = env.reset()\n",
    "                eval_ep_reward = 0\n",
    "                for _ in range(cfg.max_steps):\n",
    "                    eval_action = agent.predict(eval_state)\n",
    "                    next_eval_state, eval_reward, done, _ = env.step(eval_action)\n",
    "                    eval_ep_reward += eval_reward\n",
    "                    eval_state = next_eval_state\n",
    "                    if done:\n",
    "                        break\n",
    "                sum_eval_reward += eval_ep_reward\n",
    "            mean_eval_reward = sum_eval_reward / cfg.eval_eps\n",
    "            if mean_eval_reward >= best_ep_reward:\n",
    "                best_ep_reward = mean_eval_reward\n",
    "                final_agent = copy.deepcopy(agent)\n",
    "                print(f\"回合：{ep+1}/{cfg.train_eps}，奖励：{ep_reward:.2f}，评估奖励：{mean_eval_reward:.2f}，最佳评估奖励：{best_ep_reward:.2f}，更新模型！\")\n",
    "            else:\n",
    "                print(f\"回合：{ep+1}/{cfg.train_eps}，奖励：{ep_reward:.2f}，评估奖励：{mean_eval_reward:.2f}，最佳评估奖励：{best_ep_reward:.2f}\")\n",
    "        steps.append(ep_step)\n",
    "        rewards.append(ep_reward)\n",
    "    print(\"完成训练！\")\n",
    "    env.close()\n",
    "    return final_agent,{'rewards':rewards}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def all_seed(env,seed = 1):\n",
    "    ''' 万能的seed函数\n",
    "    '''\n",
    "    if seed == 0:\n",
    "        return\n",
    "    env.seed(seed) # env config\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed) # config for CPU\n",
    "    torch.cuda.manual_seed(seed) # config for GPU\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # config for python scripts\n",
    "    # config for cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "def env_agent_config(cfg):\n",
    "    env = gym.make(cfg.env_name) # 创建环境\n",
    "    all_seed(env,seed=cfg.seed)\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}\")\n",
    "    # 更新n_states和n_actions到cfg参数中\n",
    "    setattr(cfg, 'n_states', n_states)\n",
    "    setattr(cfg, 'n_actions', n_actions) \n",
    "    agent = Agent(cfg)\n",
    "    return env,agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self) -> None:\n",
    "        self.env_name = \"CartPole-v1\" # 环境名字\n",
    "        self.new_step_api = False # 是否用gym的新api\n",
    "        self.algo_name = \"PPO\" # 算法名字\n",
    "        self.mode = \"train\" # train or test\n",
    "        self.seed = 1 # 随机种子\n",
    "        self.device = \"cuda\" # device to use\n",
    "        self.train_eps = 200 # 训练的回合数\n",
    "        self.test_eps = 20 # 测试的回合数\n",
    "        self.max_steps = 200 # 每个回合的最大步数\n",
    "        self.eval_eps = 5 # 评估的回合数\n",
    "        self.eval_per_episode = 10 # 评估的频率\n",
    "\n",
    "        self.gamma = 0.99 # 折扣因子\n",
    "        self.k_epochs = 4 # 更新策略网络的次数\n",
    "        self.actor_lr = 0.0003 # actor网络的学习率\n",
    "        self.critic_lr = 0.0003 # critic网络的学习率\n",
    "        self.eps_clip = 0.2 # epsilon-clip\n",
    "        self.entropy_coef = 0.01 # entropy的系数\n",
    "        self.update_freq = 100 # 更新频率\n",
    "        self.actor_hidden_dim = 256 # actor网络的隐藏层维度\n",
    "        self.critic_hidden_dim = 256 # critic网络的隐藏层维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间维度：4，动作空间维度：2\n",
      "Start training...\n",
      "回合：10/200，奖励：40.00，评估奖励：18.80，最佳评估奖励：18.80，更新模型！\n",
      "回合：20/200，奖励：17.00，评估奖励：24.40，最佳评估奖励：24.40，更新模型！\n",
      "回合：30/200，奖励：64.00，评估奖励：45.00，最佳评估奖励：45.00，更新模型！\n",
      "回合：40/200，奖励：49.00，评估奖励：43.40，最佳评估奖励：45.00\n",
      "回合：50/200，奖励：59.00，评估奖励：32.00，最佳评估奖励：45.00\n",
      "回合：60/200，奖励：59.00，评估奖励：59.40，最佳评估奖励：59.40，更新模型！\n",
      "回合：70/200，奖励：55.00，评估奖励：71.60，最佳评估奖励：71.60，更新模型！\n",
      "回合：80/200，奖励：200.00，评估奖励：107.60，最佳评估奖励：107.60，更新模型！\n",
      "回合：90/200，奖励：118.00，评估奖励：98.80，最佳评估奖励：107.60\n",
      "回合：100/200，奖励：122.00，评估奖励：123.20，最佳评估奖励：123.20，更新模型！\n",
      "回合：110/200，奖励：125.00，评估奖励：173.60，最佳评估奖励：173.60，更新模型！\n",
      "回合：120/200，奖励：167.00，评估奖励：130.80，最佳评估奖励：173.60\n",
      "回合：130/200，奖励：128.00，评估奖励：128.40，最佳评估奖励：173.60\n",
      "回合：140/200，奖励：152.00，评估奖励：187.20，最佳评估奖励：187.20，更新模型！\n",
      "回合：150/200，奖励：119.00，评估奖励：109.00，最佳评估奖励：187.20\n",
      "回合：160/200，奖励：120.00，评估奖励：134.60，最佳评估奖励：187.20\n",
      "回合：170/200，奖励：145.00，评估奖励：173.40，最佳评估奖励：187.20\n",
      "回合：180/200，奖励：200.00，评估奖励：200.00，最佳评估奖励：200.00，更新模型！\n",
      "回合：190/200，奖励：182.00，评估奖励：192.80，最佳评估奖励：200.00\n",
      "回合：200/200，奖励：120.00，评估奖励：88.60，最佳评估奖励：200.00\n",
      "完成训练！\n"
     ]
    }
   ],
   "source": [
    "# 获取参数\n",
    "cfg = Config() \n",
    "# 训练\n",
    "env, agent = env_agent_config(cfg)\n",
    "best_agent,res_dic = train(cfg, env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 习题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 基于同策略的策略梯度有什么改进之处？**\n",
    "\n",
    "采样数据只用一次 效率低下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 重要性采样要注意的问题？**\n",
    "\n",
    "用于近似的分布不能相差太大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3。 近端策略优化 KL散度**\n",
    "\n",
    "一个函数 计算两个动作之间的行为距离（概率分布上的差距）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 深度Q网络 DQN\n",
    "\n",
    "> 9/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 表格 -> Q函数\n",
    "\n",
    "即 **价值函数近似**\n",
    "\n",
    "函数可以是一个神经网络 即 Q网络 Q-Nerwork\n",
    "\n",
    "深度Q网络 DQN 就是基于深度学习的 Q学习算法\n",
    "\n",
    "- 对于连续动作：Q 函数输出动作的标量估计值\n",
    "- 对于离散动作：Q 函数输出每个动作的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 状态价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 on-value 的算法中，要学习的是 **评论员Critic**\n",
    "\n",
    "评论员的任务是做出策略评估，以此来评价一个 **演员Actor** 的好坏\n",
    "\n",
    "因此评论员和演员是绑定的，评论员需要知道状态的价值，而状态的价值取决于演员"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入是当前的状态 s\n",
    "\n",
    "衡量价值函数 $V_\\pi(s)$ 可以通过：\n",
    "- 基于蒙特卡洛的方法：交互、统计、回归,训练到满足 $V(s_t) = G_t$\n",
    "- 基于时序差分的方法：训练到满足 $V_\\pi(s_t) = V_\\pi(s_{t+1})+r_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动作价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即 Q函数\n",
    "\n",
    "输入 s-a对，输出条件下期望的累计奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过学习 Q函数，更新策略： $\\pi'(s)=arg maxQ_\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **目标网络**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN 的学习过程是训练到两个 Q 网络输出的结果差距足够小（MSE）：\n",
    "\n",
    "$Q_\\pi(s_t,a_t) = r_t + Q_\\pi(s_{t+1},\\pi(s_{t+1}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在学习时，如果同时更新左右两个 Q 网络的参数，会很难回归\n",
    "\n",
    "因此一般固定右边的网络不变，专门用于生成目标，称为目标网络 \n",
    "\n",
    "不断更新左边的网络，再用更新后的 Q网络 去替换目标网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **探索**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据Q函数的值，策略的选择将完全取决于最大值，不具有随机性\n",
    "\n",
    "因此，如果Q函数一次都没有更新到某些策略，那么就永远不会使用那些策略，即 **探索-利用窘境**\n",
    "\n",
    "有两个方法解决在这个问题：\n",
    "\n",
    "- $\\epsilon-greedy$\n",
    "- 玻尔兹曼探索 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\epsilon-greedy$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "a=\n",
    "\\begin{cases}\n",
    "argmaxQ(s,a) & ,p=1-\\epsilon \\\\\n",
    "随机 & ,else\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "一般取$\\epsilon = 0.1$,或采用递减的方法，开始的时候探索多一些，然后逐渐减少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**玻尔兹曼探索**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取温度系数 $T > 0$\n",
    "\n",
    "如果 T 很大，那么所有动作等概率选择\n",
    "\n",
    "如果 T 很小，那么Q值大的动作更容易选中\n",
    "\n",
    "T 趋于 0，只选择最优动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 经验回放"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造一个回放缓冲区 replay buffer\n",
    "\n",
    "存放一定量的交互记录：在状态 st 执行动作 at 后，进入了状态 st+1，并得到奖励 rt\n",
    "\n",
    "将其视作训练集，从中随机选择一个批量来训练Q网络\n",
    "\n",
    "数据不一定来自同一个策略，但是没有关系，因为数据记录的不是一条轨迹，而是分散的状态上的动作和奖励，只是经验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用了经验回放技巧的算法就成了一个 **异策略算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度 Q 网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 初始化函数（神经网络） $Q$ 目标函数 $Q'$ ，令 $Q' = Q$\n",
    "- 对于每个回合：\n",
    "  - 对于每一步：\n",
    "    - 基于 Q （$\\epsilon-greedy$）执行动作 $a_t$\n",
    "    - 得到动作执行的 $r_t,s_{t+1}$\n",
    "    - 将 $(s_t,a_t,r_t,s_{t+1})$ 放入经验回放池\n",
    "    - 采样一个批量的经验\n",
    "    - 目标值 $y=r_i+maxQ'(s_{t+1,a})$\n",
    "    - 更新 Q 参数使 $Q(s_{i+1},a)$ 接近 y\n",
    "    - 每隔一段时间更新覆盖 $Q' = Q$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用一个神经网络作为 Q 函数。此处以 多层感知机 MLP 为例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, hidden_dim=128):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_states, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用经验回放池"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- push\n",
    "- sample 支持随机取样\n",
    "- clear 清除所有数据\n",
    "- len 返回数据长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "    def push(self,transitions):\n",
    "        ''' 存储transition到经验回放中\n",
    "        '''\n",
    "        self.buffer.append(transitions)\n",
    "    def sample(self, batch_size: int, sequential: bool = False):\n",
    "        if batch_size > len(self.buffer): # 如果批量大小大于经验回放的容量，则取经验回放的容量\n",
    "            batch_size = len(self.buffer)\n",
    "        if sequential: # 顺序采样\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
    "            batch = [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
    "            return zip(*batch)\n",
    "        else: # 随机采样\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "            return zip(*batch)\n",
    "    def clear(self):\n",
    "        ''' 清空经验回放\n",
    "        '''\n",
    "        self.buffer.clear()\n",
    "    def __len__(self):\n",
    "        ''' 返回当前存储的量\n",
    "        '''\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义 DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy_net 即 actor\n",
    "- target_net 即 critic (不参与优化)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class DQN:\n",
    "    def __init__(self, model, memory, cfg):\n",
    "        # agent\n",
    "        self.device = torch.device(cfg.device)\n",
    "        self.policy_net = model.to(self.device)\n",
    "        self.target_net = model.to(self.device)\n",
    "\n",
    "        # memory\n",
    "        self.memory = memory\n",
    "\n",
    "        # cfg\n",
    "        self.num_actions = cfg.num_actions\n",
    "        self.num_states = cfg.num_states\n",
    "        self.batch_size = cfg.batch_size\n",
    "        self.gamma = cfg.gamma\n",
    "        self.device = cfg.device\n",
    "        # ε-greedy\n",
    "        self.epsilon = cfg.epsilon\n",
    "        self.epsilon_decay = cfg.epsilon_decay\n",
    "        self.epsilon_start = cfg.epsilon_start\n",
    "        self.epsilon_end = cfg.epsilon_end\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=cfg.lr)\n",
    "\n",
    "        self.sample_count = 0\n",
    "\n",
    "        # copy\n",
    "        for tp, pp in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "            tp.data.copy_(pp.data)\n",
    "        self.policy_net.to(self.device)\n",
    "        self.target_net.to(self.device)\n",
    "        \n",
    "    def sample(self, state):\n",
    "        ''' 采样动作\n",
    "        '''\n",
    "        self.sample_count += 1\n",
    "        # epsilon指数衰减\n",
    "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "            math.exp(-1. * self.sample_count / self.epsilon_decay) \n",
    "        if random.random() > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "                q_values = self.policy_net(state)\n",
    "                action = q_values.max(1)[1].item() # choose action corresponding to the maximum q value\n",
    "        else:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        return action\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size: # 当经验回放中不满足一个批量时，不更新策略\n",
    "            return\n",
    "        # 从经验回放中随机采样一个批量的转移(transition)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(\n",
    "            self.batch_size)\n",
    "        # 将数据转换为tensor\n",
    "        state_batch = torch.tensor(np.array(state_batch), device=self.device, dtype=torch.float)\n",
    "        action_batch = torch.tensor(action_batch, device=self.device).unsqueeze(1)  \n",
    "        reward_batch = torch.tensor(reward_batch, device=self.device, dtype=torch.float)  \n",
    "        next_state_batch = torch.tensor(np.array(next_state_batch), device=self.device, dtype=torch.float)\n",
    "        done_batch = torch.tensor(np.float32(done_batch), device=self.device)\n",
    "        q_values = self.policy_net(state_batch).gather(dim=1, index=action_batch) # 计算当前状态(s_t,a)对应的Q(s_t, a)\n",
    "        next_q_values = self.target_net(next_state_batch).max(1)[0].detach() # 计算下一时刻的状态(s_t_,a)对应的Q值\n",
    "        # 计算期望的Q值，对于终止状态，此时done_batch[0]=1, 对应的expected_q_value等于reward\n",
    "        expected_q_values = reward_batch + self.gamma * next_q_values * (1-done_batch)\n",
    "        loss = nn.MSELoss()(q_values, expected_q_values.unsqueeze(1))  # 计算均方根损失\n",
    "        # 优化更新模型\n",
    "        self.optimizer.zero_grad()  \n",
    "        loss.backward()\n",
    "        # clip防止梯度爆炸\n",
    "        for param in self.policy_net.parameters():  \n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step() \n",
    "\n",
    "        with torch.no_grad():\n",
    "            return loss.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg, env, agent):\n",
    "    ''' 训练\n",
    "    '''\n",
    "    print(\"开始训练！\")\n",
    "    rewards = []  # 记录所有回合的奖励\n",
    "    steps = []\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        ep_reward = 0  # 记录一回合内的奖励\n",
    "        ep_step = 0\n",
    "        state = env.reset()  # 重置环境，返回初始状态\n",
    "        loss = -1\n",
    "        for _ in range(cfg.ep_max_steps):\n",
    "            ep_step += 1\n",
    "            action = agent.sample(state)  # 选择动作\n",
    "            next_state, reward, done, _ = env.step(action)  # 更新环境，返回transition\n",
    "            agent.memory.push((state, action, reward,next_state, done))  # 保存transition\n",
    "            state = next_state  # 更新下一个状态\n",
    "            loss = agent.update()  # 更新智能体\n",
    "            ep_reward += reward  # 累加奖励\n",
    "            if done:\n",
    "                break\n",
    "        if (i_ep + 1) % cfg.target_update == 0:  # 智能体目标网络更新\n",
    "            agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "        steps.append(ep_step)\n",
    "        rewards.append(ep_reward)\n",
    "        if (i_ep + 1) % 10 == 0:\n",
    "            print(f\"回合：{i_ep+1}/{cfg.train_eps}，奖励：{ep_reward:.2f}，Epislon：{agent.epsilon:.3f}, loss: {loss:.3f}\")\n",
    "    print(\"完成训练！\")\n",
    "    env.close()\n",
    "    return {'rewards':rewards}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def all_seed(env,seed = 1):\n",
    "    ''' 万能的seed函数\n",
    "    '''\n",
    "    env.seed(seed) # env config\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed) # config for CPU\n",
    "    torch.cuda.manual_seed(seed) # config for GPU\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # config for python scripts\n",
    "    # config for cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "def env_agent_config(cfg):\n",
    "    env = gym.make(cfg.env_name) # 创建环境\n",
    "    if cfg.seed !=0:\n",
    "        all_seed(env,seed=cfg.seed)\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    model = MLP(n_states, n_actions, hidden_dim = cfg.hidden_dim) # 创建模型\n",
    "    memory = ReplayBuffer(cfg.memory_capacity)\n",
    "    agent = DQN(model,memory,cfg)\n",
    "    return env,agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.seed = 10\n",
    "        self.hidden_dim = 256\n",
    "        self.env_name = \"CartPole-v0\"\n",
    "        self.env = gym.make(self.env_name)\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.num_states = self.env.observation_space.shape[0]\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.gamma = 0.95\n",
    "        self.lr = 0.0001\n",
    "        self.batch_size = 64\n",
    "        self.num_episodes = 200\n",
    "        self.epsilon = 0.95\n",
    "        self.epsilon_decay = 500\n",
    "        self.epsilon_end = 0.01\n",
    "        self.epsilon_start = 0.95\n",
    "\n",
    "        self.memory_size = 100000\n",
    "        self.memory_capacity = 100000\n",
    "\n",
    "        self.train_eps = 200\n",
    "        self.test_eps = 10\n",
    "        self.ep_max_steps = 100000\n",
    "        self.target_update = 4\n",
    "\n",
    "        self.optimizer = optim.Adam\n",
    "        self.lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练！\n",
      "回合：10/200，奖励：14.00，Epislon：0.611, loss: 5.520\n",
      "回合：20/200，奖励：10.00，Epislon：0.470, loss: 10.752\n",
      "回合：30/200，奖励：11.00，Epislon：0.372, loss: 5.159\n",
      "回合：40/200，奖励：18.00，Epislon：0.302, loss: 2.935\n",
      "回合：50/200，奖励：15.00，Epislon：0.228, loss: 1.488\n",
      "回合：60/200，奖励：62.00，Epislon：0.121, loss: 4.113\n",
      "回合：70/200，奖励：128.00，Epislon：0.039, loss: 0.557\n",
      "回合：80/200，奖励：200.00，Epislon：0.011, loss: 2.696\n",
      "回合：90/200，奖励：200.00，Epislon：0.010, loss: 5.457\n",
      "回合：100/200，奖励：200.00，Epislon：0.010, loss: 10.728\n",
      "回合：110/200，奖励：200.00，Epislon：0.010, loss: 1.225\n",
      "回合：120/200，奖励：200.00，Epislon：0.010, loss: 0.744\n",
      "回合：130/200，奖励：200.00，Epislon：0.010, loss: 7.263\n",
      "回合：140/200，奖励：200.00，Epislon：0.010, loss: 9.195\n",
      "回合：150/200，奖励：200.00，Epislon：0.010, loss: 0.345\n",
      "回合：160/200，奖励：200.00，Epislon：0.010, loss: 0.986\n",
      "回合：170/200，奖励：200.00，Epislon：0.010, loss: 0.388\n",
      "回合：180/200，奖励：200.00，Epislon：0.010, loss: 8.524\n",
      "回合：190/200，奖励：200.00，Epislon：0.010, loss: 0.115\n",
      "回合：200/200，奖励：200.00，Epislon：0.010, loss: 0.418\n",
      "完成训练！\n"
     ]
    }
   ],
   "source": [
    "cfg = Config()\n",
    "env, agent = env_agent_config(cfg)\n",
    "\n",
    "rewards = train(cfg,env,agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**如果遇到轮次奖励忽大忽小的问题：**\n",
    "\n",
    "很有可能是：\n",
    "1. 学习率过高（最有可能）\n",
    "2. $\\epsilon$ 值不恰当\n",
    "3. 神经网络模型太简单"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关键词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**状态价值网络**\n",
    "\n",
    "演员在对应状态下，到结束时段内所能获得的期望价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 函数**\n",
    "\n",
    "动作价值函数，输入 s-a 对。假设都是用这一策略，那么 Q 函数就是这一策略累计奖励的期望。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 针对连续动作的 DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：**\n",
    "\n",
    "对于连续动作 $a$，他是一个连续的向量，可能性是无限的，我们无法穷举所有的可能值来求出最值点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解决方案：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案1. 动作采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一次采样多个可能的 $a：\\{a_1,a_2,...,a_N\\}$ \n",
    "- 分别代入 $Q$ 函数\n",
    "- 选择最大的 $Q$ 函数值对应的 $a$\n",
    "\n",
    "这样近似出来的结果不是非常精确"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案2. 梯度上升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 $a$ 视为参数，使用梯度上升去优化\n",
    "\n",
    "但是相当于每次都要多训练一个网络，运算量过大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案3. 设计网络架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在原始的DQN中，用于近似Q函数的深度模型接受输入状态s，输出所有动作a的Q值。\n",
    "\n",
    "针对连续动作的深度模型需要特别设计，目的是简化 argmax 操作。\n",
    "\n",
    "例如：\n",
    "\n",
    "根据输入的 $s$ ，输出一个向量 $\\mu(s)$ 和一个矩阵 $\\Sigma(s)$ 、 一个标量 $V(s)$ ，使：\n",
    "\n",
    "$$\n",
    "Q(s,a) = -(a-\\mu(s))^T\\Sigma(s)(a-\\mu(s)) + V(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可知：\n",
    "\n",
    "$a=\\mu(s)$ 时能够取到Q的最大值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练时：\n",
    "- 设计网络架构，使能输出向量$\\mu(s)$、矩阵$\\Sigma(s)$和向量$V(s)$。\n",
    "- 训练网络，按照上面公式得到 Q 值。\n",
    "- 优化参数，使 Q 值最大化。\n",
    "- 最后得到的 $\\mu(s)$ 就是 $a$ 的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案4. 不使用DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合 PPO 与 DQN\n",
    "\n",
    "得到 演员-评论员算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 演员-评论员算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**演员：**\n",
    "\n",
    "策略函数 $\\pi_\\theta(a|s)$\n",
    "\n",
    "**评论员：（实现单步价值更新，不需要等一条完整的轨迹）**\n",
    "\n",
    "价值函数 $V_\\pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优势演员-评论员算法 A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C 结合了 PPO 和 DQN 的思想，重点在于两个函数：Q函数和V函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优势函数替换策略期望"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PPO 中，我们对一条轨迹的回报求梯度，然后使用梯度上升更新策略网络：\n",
    "\n",
    "$$\n",
    "\\nabla R_\\theta = \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} ( 带有折扣因子的轨迹回报 - 基线 b ) \\nabla \\log p_\\pi(a_t|s_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 发现： $Q$ 函数就是回报的期望，如果将价值函数的值作为基线，那么上述公式中括号内的部分就是\n",
    "   **优势函数**：\n",
    "\n",
    "$$\n",
    "A^\\theta(s, a) = Q^\\theta(s, a) - V^\\theta(s)\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 要避免训练两个网络（Q和V），可以将Q函数用V函数表示：\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) = r_t + V_\\pi(s_{t+1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 使用价值函数替换后：\n",
    "\n",
    "$$\n",
    "\\nabla R_\\theta = \\frac{1}{N} \\sum_{i=1}^N  \\sum_{t=1}^{T_n} ( r_t^n + V_\\pi(s_{t+1}^n) - V_\\pi(s_t^n) ) \\nabla \\log p_\\theta(a_t^n|s_t^n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 算法流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 演员按照策略 $\\pi$ 与环境进行交互，得到记录\n",
    "- 采用时序差分和蒙特卡洛方法，学习价值函数 $V_\\pi(s)$\n",
    "- 基于价值函数 $V_\\pi(s)$ 更新策略 $\\pi$\n",
    "- 重复上述过程，直到策略 $\\pi$ 收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from multiprocessing import Process, Pipe\n",
    "import argparse\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 神经网络模型\n",
    "\n",
    "- **Actor** \n",
    "  - 输入 状态 s\n",
    "  - 输出 动作 a 的概率分布\n",
    "- **Critic** \n",
    "  - 输入 状态 s\n",
    "  - 输出当前状态下的价值 V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # num_inputs: shape of state\n",
    "        # critic nn: state => value\n",
    "        self.critic_ln1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic_ln2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # actor nn: state => action, policy\n",
    "        self.actor_ln1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor_ln2 = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        # (4, ) => (1, 4)\n",
    "        # ndarray => Variable\n",
    "        # state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        state = torch.tensor(state, requires_grad=True, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # forward of critic network\n",
    "        # (1, 4) => (1, 256)\n",
    "        value = F.relu(self.critic_ln1(state))\n",
    "        # (1, 256) => (1, 1)\n",
    "        value = self.critic_ln2(value)\n",
    "        \n",
    "        # (1, 4) => (1, 256)\n",
    "        policy_dist = F.relu(self.actor_ln1(state))\n",
    "        # (1, 256) => (1, 2)\n",
    "        policy_dist = F.softmax(self.actor_ln2(policy_dist), dim=1)\n",
    "        return value, policy_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 环境和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02520598, -0.01074513, -0.04647208,  0.04307167], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 256\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "GAMMA = 0.99\n",
    "num_steps = 300\n",
    "max_episodes = 3000\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# 4-d 连续\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "# 左右离散\n",
    "num_actions = env.action_space.n\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = ActorCritic(num_inputs, num_actions, hidden_size, )  \n",
    "ac_opt = optim.Adam(ac.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现 A2C 算法：\n",
    "- **Actor：** 进行探索，输出动作概率。\n",
    "- **Critic：** 评估当前状态，输出 V 值。\n",
    "- 根据探索得到的 reward 和 V 值计算优势函数值 A(s, a)\n",
    "  - 计算 Actor 的 loss\n",
    "    - $ loss_{actor} = A(s,a) · \\log p(a|s) $\n",
    "  - 计算 Critic 的 loss\n",
    "    - $ loss_{critic} = (V(s) - G_t)^2 $\n",
    "- 根据 loss 更新参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, reward: 17.0, steps: 16, average steps: 16.0\n",
      "episode: 100, reward: 14.0, steps: 13, average steps: 39.1\n",
      "episode: 200, reward: 71.0, steps: 70, average steps: 37.9\n",
      "episode: 300, reward: 36.0, steps: 35, average steps: 37.6\n",
      "episode: 400, reward: 108.0, steps: 107, average steps: 65.5\n",
      "episode: 500, reward: 53.0, steps: 52, average steps: 63.1\n",
      "episode: 600, reward: 30.0, steps: 29, average steps: 56.0\n",
      "episode: 700, reward: 45.0, steps: 44, average steps: 79.8\n",
      "episode: 800, reward: 77.0, steps: 76, average steps: 127.6\n",
      "episode: 900, reward: 27.0, steps: 26, average steps: 96.5\n",
      "episode: 1000, reward: 129.0, steps: 128, average steps: 140.3\n",
      "episode: 1100, reward: 98.0, steps: 97, average steps: 148.6\n",
      "episode: 1200, reward: 187.0, steps: 186, average steps: 156.9\n",
      "episode: 1300, reward: 200.0, steps: 199, average steps: 181.1\n",
      "episode: 1400, reward: 169.0, steps: 168, average steps: 131.7\n",
      "episode: 1500, reward: 200.0, steps: 199, average steps: 156.5\n",
      "episode: 1600, reward: 80.0, steps: 79, average steps: 180.8\n",
      "episode: 1700, reward: 164.0, steps: 163, average steps: 142.5\n",
      "episode: 1800, reward: 200.0, steps: 199, average steps: 186.9\n",
      "episode: 1900, reward: 113.0, steps: 112, average steps: 129.9\n",
      "episode: 2000, reward: 200.0, steps: 199, average steps: 191.2\n",
      "episode: 2100, reward: 200.0, steps: 199, average steps: 194.4\n",
      "episode: 2200, reward: 200.0, steps: 199, average steps: 178.3\n",
      "episode: 2300, reward: 200.0, steps: 199, average steps: 196.5\n",
      "episode: 2400, reward: 188.0, steps: 187, average steps: 172.3\n",
      "episode: 2500, reward: 200.0, steps: 199, average steps: 196.4\n",
      "episode: 2600, reward: 149.0, steps: 148, average steps: 156.8\n",
      "episode: 2700, reward: 140.0, steps: 139, average steps: 121.2\n",
      "episode: 2800, reward: 195.0, steps: 194, average steps: 175.6\n",
      "episode: 2900, reward: 200.0, steps: 199, average steps: 193.4\n"
     ]
    }
   ],
   "source": [
    "all_rewards = []\n",
    "all_steps = []\n",
    "ma_steps = []\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    \n",
    "    # same length\n",
    "    # index means timestamp: t\n",
    "    log_probs = [] \n",
    "    values = []\n",
    "    rewards = []\n",
    "    entropy_term = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        value, policy_dist = ac(state)\n",
    "        \n",
    "#         print(value.shape, value)\n",
    "#         print(policy_dist.shape, policy_dist)\n",
    "        \n",
    "        value = value.detach().numpy()[0, 0]\n",
    "        dist = policy_dist.detach().numpy()\n",
    "        \n",
    "#         print(value.shape, value)\n",
    "#         print(dist.shape, dist)\n",
    "        \n",
    "        # 概率化地选择，[0/1]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(dist))\n",
    "        log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "        \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        entropy_term += -np.sum(dist * np.log(dist))\n",
    "#         print(dist, np.log(dist), entropy_term)\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done or step == num_steps - 1:\n",
    "            q_value, _ = ac(new_state)\n",
    "            q_value = q_value.detach().numpy()[0, 0]\n",
    "            all_rewards.append(np.sum(rewards))\n",
    "            all_steps.append(step)\n",
    "            ma_steps.append(np.mean(all_steps[-10:]))\n",
    "            if episode % 100 == 0:\n",
    "                print(f'episode: {episode}, reward: {np.sum(rewards)}, steps: {step}, average steps: {ma_steps[-1]}')\n",
    "            break\n",
    "                \n",
    "    # 收集训练数据\n",
    "    q_values = np.zeros_like(values)\n",
    "    for t in range(len(values))[::-1]:\n",
    "        # discounted reward\n",
    "        # 只跟 final value 有关\n",
    "        q_value = rewards[-1] + GAMMA * q_value\n",
    "        q_values[t] = q_value\n",
    "    \n",
    "    values = torch.FloatTensor(values)\n",
    "    q_values = torch.FloatTensor(q_values)\n",
    "    log_probs = torch.stack(log_probs)\n",
    "#     print(values.shape, q_values.shape, log_probs.shape)\n",
    "    advantage = q_values - values\n",
    "    \n",
    "    # scalar objective to minimize\n",
    "    actor_loss = -(log_probs*advantage).mean()\n",
    "    critic_loss = 0.5*advantage.pow(2).mean()\n",
    "    loss = actor_loss + critic_loss + 0.001*entropy_term\n",
    "    \n",
    "    ac_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    ac_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b9cc097a30>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "ma_rewards = pd.Series.rolling(pd.Series(all_rewards), 10).mean()\n",
    "plt.plot([r for r in ma_rewards])\n",
    "# plt.plot(all_rewards)\n",
    "# plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异步 A3C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "异步优势演员-评论员算法同时使用很多个进程（worker），\n",
    "\n",
    "如果我们没有很多 CPU，不好实现异步优势演员-评论员算法，\n",
    "\n",
    "但可以实现优势演员-评论员算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一个全局网络，包含：\n",
    "  - Q 函数网络\n",
    "  - V 函数网络\n",
    "  - 策略\n",
    "  - 调度\n",
    "- 多个进程，每个进程：\n",
    "  - 互相平行地执行\n",
    "  - 先向全局网络复制一份参数\n",
    "  - 执行一个训练步骤（采样、计算、优化）\n",
    "  - 使用训练后的参数更新全局网络（覆盖）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 路径衍生策略梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评论员直接告诉演员应当选择什么样的动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 Q 学习来求解连续动作，评论员对演员提供价值最大的动作\n",
    "\n",
    "类似一个生成对抗网络\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 插件方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Actor - Critic**\n",
    "\n",
    "训练两个网络，一个用于探索，一个用于评价。\n",
    "\n",
    "**2. 经验回放池**\n",
    "\n",
    "每条经验包括 (state,action,log_prob,reward,done)\n",
    "\n",
    "每次随机采样一个批次的经验，用以计算loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 算法对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. PPO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actor** \n",
    "  - 探索、优化\n",
    "- **Critic** \n",
    "  - 评估\n",
    "- **经验回放** \n",
    "  - PPO 优化时需要一整条轨迹，因此经验回放池每次采样都是直接返回所有的经验\n",
    "- **Loss** \n",
    "  - PPO2优化裁剪 \n",
    "  - $loss_{actor} = min( A · ratio , clamp(ratio,1-\\epsilon,1+\\epsilon)·A )$\n",
    "- **训练**\n",
    "  - 探索并写入经验池\n",
    "  - 满足一定轮数：采样，优化更新参数\n",
    "  - 满足一定轮数：\n",
    "    - 探索一条轨迹\n",
    "    - 计算回报\n",
    "    - 比较回报与评估，判断是否更新 agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. DQN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actor**\n",
    "  - policy_net 探索，获取经验\n",
    "- **Critic**\n",
    "  - target_net 目标网络，更新,评估期望Q值\n",
    "- **经验池**\n",
    "  - 存储经验\n",
    "  - 随机采样指定批次\n",
    "- **Loss**\n",
    "  - 探索得到的 Q 与目标网络计算出来的期望 Q 的均方误差\n",
    "  - $ Q = PolicyNet(state)$\n",
    "  - $Q_{exp} = r + \\gamma · Q_{next}$\n",
    "  - $ loss = MSELoss(Q,Q_{exp})$\n",
    "- **训练**\n",
    "  - 探索并写入经验池\n",
    "  - 满足一定轮次时：采样并优化\n",
    "  - 满足一定轮次时：更新目标网络（使用actor覆盖）\n",
    "  \n",
    "$$\n",
    "Q_\\pi(s_t,a_t) = r_t + Q_\\pi(s_{t+1},\\pi(s_{t+1}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. A2C**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actor** \n",
    "  - 输入 状态 s\n",
    "  - 输出 动作 a 的概率分布\n",
    "- **Critic** \n",
    "  - 输入 状态 s\n",
    "  - 输出当前状态下的价值 V\n",
    "- **Loss**\n",
    "  - $loss_{actor} = -\\log(probs)·A$\n",
    "  - $loss_{critic} = MSELoss(A)$\n",
    "  - $loss = k_1 · loss_{actor} + k_2 · loss_{critic} + k_3 · entropy$\n",
    "\n",
    "$$\n",
    "\\nabla R_\\theta = \\frac{1}{N} \\sum_{i=1}^N  \\sum_{t=1}^{T_n} ( r_t^n + V_\\pi(s_{t+1}^n) - V_\\pi(s_t^n) ) \\nabla \\log p_\\theta(a_t^n|s_t^n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 深度确定性策略梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 离散动作与连续动作的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 离散动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可数的，有限个动作\n",
    "- Q学习、DQN等 模型输出的是动作的概率\n",
    "- agent 执行动作时采用的是随机性策略，依据模型输出的概率分布决定执行哪个动作\n",
    "- 要输出离散动作：神经网络上加一层softmax，保证输出的是和为1的概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 连续动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不可数的，有无限个选择\n",
    "- 输出的是一个具体的值，代表执行的策略\n",
    "- agent 执行动作时采用的是确定性策略，即按照模型给定的值进行动作\n",
    "- 要输出连续动作：在输出层加一层 tanh 函数，将输出限制在 [-1,1]，在根据环境要求按比例扩大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度确定性策略梯度 DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**深度：**\n",
    "\n",
    "采用了神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**确定性：**\n",
    "\n",
    "输出的是一个确定的动作，而非概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**策略梯度：**\n",
    "\n",
    "是基于策略的，每一步更新一次策略网络（单步更新）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**采用技巧：**\n",
    "\n",
    "- 目标网络（a-c结构）\n",
    "  - 策略网络 $\\pi_\\theta$\n",
    "  - Q网络（评论员）$Q_\\theta$\n",
    "  - 两个网络都要学习和优化\n",
    "    - 策略网络的优化目标是靠近目标网络,即最大化Q值，loss就是Q取负数\n",
    "    - 目标网络的优化目标是拟合 $Q = r + Q_{next}$ ，loss是MSE(Q,Q')\n",
    "  - 为了使学习更加稳定，为两个网络都添加一个目标网络\n",
    "- 经验回放\n",
    "  - (s,a,r,s')\n",
    "- 添加噪声"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actor\n",
    "  - 三层感知机 输入：状态，输出：动作\n",
    "  - 针对连续动作：输出的值就是确定的动作\n",
    "  - tanh 控制输出范围[-1,1]，按比例放大（x max_action）就得到了真正的动作值\n",
    "- Critic\n",
    "  - 输入：状态，输出：Q值\n",
    "  - 中间层还要加上动作数：Q(s,a) Q 与 s,a 都有关\n",
    "  - Q = Critic(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action):\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, 400)\n",
    "\t\tself.l2 = nn.Linear(400, 300)\n",
    "\t\tself.l3 = nn.Linear(300, action_dim)\n",
    "\t\t\n",
    "\t\tself.max_action = max_action\n",
    "\n",
    "\t\n",
    "\tdef forward(self, state):\n",
    "\t\ta = F.relu(self.l1(state))\n",
    "\t\ta = F.relu(self.l2(a))\n",
    "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim):\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, 400)\n",
    "\t\tself.l2 = nn.Linear(400 + action_dim, 300)\n",
    "\t\tself.l3 = nn.Linear(300, 1)\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\tq = F.relu(self.l1(state))\n",
    "\t\tq = F.relu(self.l2(torch.cat([q, action], 1)))\n",
    "\t\treturn self.l3(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDPG 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为了保证Q的稳定，为Q网络和策略网络都搭建了目标网络\n",
    "- 智能体的动作选择就是 actor(state) 的结果\n",
    "- 评论员做评估时，Q = critic(state,action)\n",
    "- 训练：\n",
    "  - 采用经验回放，随机从历史数据中采样 (state,action,next_state,reward,done)\n",
    "  - 计算 target_Q = reward + gamma * critic_target(next_state,actor_target(next_state))\n",
    "  - 计算 current_Q = critic(state,action)\n",
    "  - 计算 critic_loss = MSE(target_Q,current_Q)\n",
    "  - 计算 actor_loss = -current_Q\n",
    "  - 分别优化\n",
    "  - **更新参数**\n",
    "    - `target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)`\n",
    "    - 使目标网络的参数 缓慢地 向原始网络参数靠近，使得Q网络和策略网络更稳定 变化更平滑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action, discount=0.99, tau=0.001):\n",
    "\t\tself.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "\n",
    "\t\tself.critic = Critic(state_dim, action_dim).to(device)\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), weight_decay=1e-2)\n",
    "\n",
    "\t\tself.discount = discount\n",
    "\t\tself.tau = tau\n",
    "\n",
    "\n",
    "\tdef select_action(self, state):\n",
    "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "\tdef train(self, replay_buffer, batch_size=64):\n",
    "\t\t# Sample replay buffer \n",
    "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "\t\t# Compute the target Q value\n",
    "\t\ttarget_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "\t\ttarget_Q = reward + (not_done * self.discount * target_Q).detach()\n",
    "\n",
    "\t\t# Get current Q estimate\n",
    "\t\tcurrent_Q = self.critic(state, action)\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "\t\tcritic_loss = F.mse_loss(current_Q, target_Q)\n",
    "\n",
    "\t\t# Optimize the critic\n",
    "\t\tself.critic_optimizer.zero_grad()\n",
    "\t\tcritic_loss.backward()\n",
    "\t\tself.critic_optimizer.step()\n",
    "\n",
    "\t\t# Compute actor loss\n",
    "\t\tactor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\t\t\n",
    "\t\t# Optimize the actor \n",
    "\t\tself.actor_optimizer.zero_grad()\n",
    "\t\tactor_loss.backward()\n",
    "\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t# Update the frozen target models\n",
    "\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 双延迟深度确定性策略梯度 TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG 因为采用Q网络作为评论员，容易导致训练好的Q网络开始显著高估的问题\n",
    "\n",
    "TD3 使用如下技巧来解决这一问题："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 截断的双Q学习**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD3 学习两个 Q 函数 $Q_{\\theta1}$ $Q_{\\theta2}$ , 选取其中较小的值作为 $Q_{target}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 延迟的策略更新**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以较低频率更新动作网络\n",
    "\n",
    "以较高频率更新评价网络\n",
    "\n",
    "一般每更新两次评价网络，更新一次动作网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 正则化：目标策略平滑**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在目标动作中加入噪声\n",
    "\n",
    "平滑 Q 沿动作的变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD3 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action):\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, action_dim)\n",
    "\t\t\n",
    "\t\tself.max_action = max_action\n",
    "\t\t\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\ta = F.relu(self.l1(state))\n",
    "\t\ta = F.relu(self.l2(a))\n",
    "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim):\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\n",
    "\t\t# Q1 architecture\n",
    "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, 1)\n",
    "\n",
    "\t\t# Q2 architecture\n",
    "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l5 = nn.Linear(256, 256)\n",
    "\t\tself.l6 = nn.Linear(256, 1)\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\n",
    "\t\tq2 = F.relu(self.l4(sa))\n",
    "\t\tq2 = F.relu(self.l5(q2))\n",
    "\t\tq2 = self.l6(q2)\n",
    "\t\treturn q1, q2\n",
    "\n",
    "\n",
    "\tdef Q1(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\t\treturn q1\n",
    "\n",
    "\n",
    "class TD3(object):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tstate_dim,\n",
    "\t\taction_dim,\n",
    "\t\tmax_action,\n",
    "\t\tdiscount=0.99,\n",
    "\t\ttau=0.005,\n",
    "\t\tpolicy_noise=0.2,\n",
    "\t\tnoise_clip=0.5,\n",
    "\t\tpolicy_freq=2\n",
    "\t):\n",
    "\n",
    "\t\tself.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.critic = Critic(state_dim, action_dim).to(device)\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.max_action = max_action\n",
    "\t\tself.discount = discount\n",
    "\t\tself.tau = tau\n",
    "\t\tself.policy_noise = policy_noise\n",
    "\t\tself.noise_clip = noise_clip\n",
    "\t\tself.policy_freq = policy_freq\n",
    "\n",
    "\t\tself.total_it = 0\n",
    "\n",
    "\n",
    "\tdef select_action(self, state):\n",
    "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "\tdef train(self, replay_buffer, batch_size=256):\n",
    "\t\tself.total_it += 1\n",
    "\n",
    "\t\t# Sample replay buffer \n",
    "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Select action according to policy and add clipped noise\n",
    "\t\t\tnoise = (\n",
    "\t\t\t\ttorch.randn_like(action) * self.policy_noise\n",
    "\t\t\t).clamp(-self.noise_clip, self.noise_clip)\n",
    "\t\t\t\n",
    "\t\t\tnext_action = (\n",
    "\t\t\t\tself.actor_target(next_state) + noise\n",
    "\t\t\t).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "\t\t\t# Compute the target Q value\n",
    "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
    "\t\t\ttarget_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "\t\t# Get current Q estimates\n",
    "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "\t\t# Optimize the critic\n",
    "\t\tself.critic_optimizer.zero_grad()\n",
    "\t\tcritic_loss.backward()\n",
    "\t\tself.critic_optimizer.step()\n",
    "\n",
    "\t\t# Delayed policy updates\n",
    "\t\tif self.total_it % self.policy_freq == 0:\n",
    "\n",
    "\t\t\t# Compute actor losse\n",
    "\t\t\tactor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "\t\t\t\n",
    "\t\t\t# Optimize the actor \n",
    "\t\t\tself.actor_optimizer.zero_grad()\n",
    "\t\t\tactor_loss.backward()\n",
    "\t\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t\t# Update the frozen target models\n",
    "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dailly38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
