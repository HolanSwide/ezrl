{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EazyRL_v1.0.6 笔记\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 绪论\n",
    "> 9/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强化学习 RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 机器学习的一个分支\n",
    "- 研究智能主体在环境中如何采取行动以最大化累积奖励\n",
    "- 基本思想是通过智能体（Agent）与环境（Environment）的**交互**来**学习最优策略**，使得智能体能够采取一系列**动作**以**获取最大的长期奖励**\n",
    "- 目标是寻找最优策略\n",
    "\n",
    "![](./img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 产生 action 作用于 env\n",
    "\n",
    "env 根据 action 产生 reward 和 next_state\n",
    "\n",
    "agent 根据 reward 和 next_state 进行更新\n",
    "\n",
    "循环上述过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**与监督学习对比**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督学习：\n",
    "- 样本互不相关，是独立同分布的\n",
    "- 根据监督者给出的 label 确定正误\n",
    "- 通过反向传播更新模型参数，实现学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习：\n",
    "- 输入的是序列数据，数据之间有很强的关联\n",
    "- 不给出正确的动作，需要学习器不断尝试\n",
    "- 延迟奖励：奖励信号不是立即给出的，而是延迟一段时间\n",
    "- 动作会影响后面的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度强化学习：省去特征提取，实现端到端"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 序列决策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在与环境的交互过程中，智能体会获得很多观测\n",
    "\n",
    "每一个观测 - 采取一个动作 - 得到一个奖励\n",
    "\n",
    "**历史是观测 动作 奖励的序列**\n",
    "\n",
    "智能体采取**当前动作** 依赖于 **得到的历史**\n",
    "\n",
    "**状态是历史的函数**\n",
    "\n",
    "基本问题：**学习与规划**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体能够观察环境所有状态（上帝视角） - 完全可观测的环境 - 马尔可夫决策过程 MDP\n",
    "\n",
    "智能体只能看到部分的观测状态（有限信息） - 部分可观测的环境 - 部分可观测马尔可夫决策过程 POMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动作空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定环境中 有效动作的集合\n",
    "\n",
    "- 离散动作空间：动作数量有限，如围棋\n",
    "- 连续动作空间：动作是实值的向量，如智能驾驶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 智能体组成\n",
    "\n",
    "> 策略，价值函数，模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **策略**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入状态到动作的函数\n",
    "\n",
    "- 随机性策略：概率分布采样\n",
    "- 确定性策略：直接选择最有可能的动作（可能性最大的动作）\n",
    "\n",
    "通常采用随机性策略，避免出现局部最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **价值函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数值是对**未来奖励**的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**折扣因子**\n",
    "\n",
    "折扣因子γ（gamma） 反映了决策者对未来收益相对于当前收益的重视程度。\n",
    "\n",
    "折扣因子越大，表示决策者越看重未来的收益；\n",
    "\n",
    "折扣因子越小，表示决策者越倾向于即时满足，即更看重当前的收益。\n",
    "\n",
    "\n",
    "\n",
    "![](./img/2.png)\n",
    "\n",
    "pi策略 gamma折扣因子 r回报 s状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 函数**\n",
    "\n",
    "包含 状态 和 动作 两个变量，未来可以获得奖励的期望取决于当前的状态和当前的动作。\n",
    "\n",
    "![](./img/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 状态转移概率\n",
    "- 奖励函数：当前状态采取某个动作可以得到的奖励（价值函数是预测）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 智能体类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **基于价值 / 基于策略**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于价值：学习价值函数 （QL,Sarsa）\n",
    "- 基于策略：学习策略，通过状态给出动作概率 (PG)\n",
    "- 结合两者：演员（策略）- 评论员（价值）智能体\n",
    "\n",
    "基于价值迭代的方法只能应用在不连续的、离散的环境下，而基于策略迭代的方法可以应用在连续的环境中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **有模型 / 免模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是否需要对真实环境进行建模：\n",
    "\n",
    "- 有模型：学习状态转移来采取动作，如状态转移和奖励函数都是**已知**的，能知道某一状态采取某一决策后的奖励和状态（动规问题）\n",
    "- 免模型：学习价值函数和策略函数进行决策，没有环境转移函数和奖励函数。没有对真实环境进行建模，智能体只能在真实环境中通过一定的策略来执行动作，**等待**奖励和状态迁移，然后根据这些反馈信息来**更新**动作策略，这样反复迭代直到学习到最优策略。\n",
    "\n",
    "有模型同时在真实环境与虚拟世界学习\n",
    "\n",
    "免模型直接与真实环境交互来学习 一般需要大量数据样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索与利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单步强化学习任务 - K-臂赌博机\n",
    "\n",
    "- 仅探索：机会平均分配，最后计算期望\n",
    "- 仅利用：贪心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CartPole-v0 环境**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info, others = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态信息 observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.2427716, 3.7990518, 6.626667 , 4.7200627], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奖励 reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "游戏是否完成了 done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调试信息 info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`step()` 完成了一个完整的 S-A-R-S' 迭代过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看 gym 库中的环境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CartPole-v0',\n",
       " 'CartPole-v1',\n",
       " 'MountainCar-v0',\n",
       " 'MountainCarContinuous-v0',\n",
       " 'Pendulum-v1',\n",
       " 'Acrobot-v1',\n",
       " 'LunarLander-v2',\n",
       " 'LunarLanderContinuous-v2',\n",
       " 'BipedalWalker-v3',\n",
       " 'BipedalWalkerHardcore-v3',\n",
       " 'CarRacing-v2',\n",
       " 'Blackjack-v1',\n",
       " 'FrozenLake-v1',\n",
       " 'FrozenLake8x8-v1',\n",
       " 'CliffWalking-v0',\n",
       " 'Taxi-v3',\n",
       " 'Reacher-v2',\n",
       " 'Reacher-v4',\n",
       " 'Pusher-v2',\n",
       " 'Pusher-v4',\n",
       " 'InvertedPendulum-v2',\n",
       " 'InvertedPendulum-v4',\n",
       " 'InvertedDoublePendulum-v2',\n",
       " 'InvertedDoublePendulum-v4',\n",
       " 'HalfCheetah-v2',\n",
       " 'HalfCheetah-v3',\n",
       " 'HalfCheetah-v4',\n",
       " 'Hopper-v2',\n",
       " 'Hopper-v3',\n",
       " 'Hopper-v4',\n",
       " 'Swimmer-v2',\n",
       " 'Swimmer-v3',\n",
       " 'Swimmer-v4',\n",
       " 'Walker2d-v2',\n",
       " 'Walker2d-v3',\n",
       " 'Walker2d-v4',\n",
       " 'Ant-v2',\n",
       " 'Ant-v3',\n",
       " 'Ant-v4',\n",
       " 'Humanoid-v2',\n",
       " 'Humanoid-v3',\n",
       " 'Humanoid-v4',\n",
       " 'HumanoidStandup-v2',\n",
       " 'HumanoidStandup-v4']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym import envs\n",
    "\n",
    "specs = envs.registry.values() # 原书代码有误\n",
    "ids = [spec.id for spec in specs]\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 与 `gym` 库交互"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例：小车上山"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<MountainCarEnv<MountainCar-v0>>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观测空间：连续空间用 `gym.spaces.Box` 类 表示范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作空间：离散空间用 `gym.spaces.Discrete` 类 表示可取的数量，即动作数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现智能体，控制小车移动："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "    def decide(self, observation):\n",
    "        pos,vel = observation\n",
    "        lb = min( -0.09 * (pos + 0.25) ** 2 + 0.03,\n",
    "                0.3 * (pos + 0.9) ** 4 - 0.008 )\n",
    "        ub = -0.07 * (pos + 0.38) ** 2 + 0.07\n",
    "        if lb < vel < ub:\n",
    "            return  2\n",
    "        else: return 0\n",
    "    def learn(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SimpleAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与环境交互："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env,agent,render=False,train=False):\n",
    "    episode_reward = 0 # 回合总奖励\n",
    "    observation = env.reset() \n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.decide(observation)\n",
    "        next_obs,reward,done,_ = env.step(action) \n",
    "        episode_reward += reward\n",
    "        if train:\n",
    "            agent.learn(observation,action,reward,done)\n",
    "        if done:\n",
    "            break\n",
    "        observation = next_obs\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交互一回合：\n",
    "> !TODO notebook 中无法显示 gym 窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-105.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.seed(7) # 随机种子 v0.26 已经去掉了这个方法\n",
    "episode_reward= play(env,agent)\n",
    "episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估智能体的性能，需要计算出连续交互 100 回合的平均回合奖励："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-106.13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_rewards = [ play(env,agent) for _ in range(100) ]\n",
    "import numpy as np\n",
    "np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **总结**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('') # 取环境\n",
    "obs = env.reset()  # 初始化环境\n",
    "action = 1         # 随机动作\n",
    "next_obs,reward,done,info = env.step(action)   # 执行动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**习题**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 强化学习基本结构？**\n",
    "\n",
    "agent, environment, reward, value function, policy/action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 比监督学习训练过程困难的原因**\n",
    "\n",
    "延迟奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 基本特征**\n",
    "\n",
    "exploration, exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. 状态和观测？**\n",
    "\n",
    "状态：客观世界中事物的属性，是事物内部特征的集合，是全集\n",
    "\n",
    "观测：是状态的一种表现，是外界对状态的一种观察，是子集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. 智能体组成**\n",
    "\n",
    "policy, value function, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. 分类？**\n",
    "\n",
    "policy_based, value_based\n",
    "\n",
    "model_free, model_based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. 基于策略迭代和基于价值迭代的强化学习方法有什么区别**\n",
    "\n",
    "策略：通过策略给出概率，连续的环境\n",
    "\n",
    "价值：学习价值函数，离散的环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. 有模型学习和免模型学习有什么区别**\n",
    "\n",
    "有：奖励函数 / 价值函数 都是明确的，可以建立清晰的模型\n",
    "\n",
    "无：现实情况，无需模型。通过智能体与环境的交互来学习。需要较大的数据样本量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. 强化学习？**\n",
    "\n",
    "通过智能体与环境的不断交互，优化调整策略，以期达到最优奖励\n",
    "\n",
    "输入是一串序列，样本之间具有相关性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 马尔可夫决策过程\n",
    "\n",
    "> 9/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体与环境的交互过程 可以用 马尔可夫决策过程 表示，马尔可夫决策过程是强化学习的基本框架\n",
    "\n",
    "在马尔可夫决策过程中，它的环境是**全部可观测**的。\n",
    "\n",
    "但是很多时候环境里面有些量是不可观测的，但是这个部分观测的问题也可以转换成马尔可夫决策过程的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫过程 MP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **马尔可夫性质**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机过程 在某一时刻的状态 只与它前一个时刻的状态有关，而与它之前所有状态都无关的性质\n",
    "\n",
    "即 **未来的转移与过去的是独立的，无记忆性**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **马尔可夫链**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "马尔可夫过程：一组具有马尔可夫性质的随机变量序列\n",
    "\n",
    "马尔可夫链：离散时间的马尔可夫过程，状态是有限的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用状态转移矩阵来描述状态转移\n",
    "\n",
    "`a[i][j]` 表示 状态 `si` 到 状态 `sj` 的转移概率\n",
    "\n",
    "![](./img/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫奖励过程 MRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "马尔可夫链 + 奖励函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **回报和价值函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回报 return** 时刻t的回报是当前获得的所有奖励的和 (单个轨迹)\n",
    "\n",
    "**折扣回报 discounted return** 具有折扣因子的回报函数，折扣因子作为超参数，可以调整出不同偏好（长短期收益）下的 agent\n",
    "\n",
    "**价值函数** 状态价值函数：在某状态下，agent能获得的回报期望 v(pi,s) （所有轨迹计算后的期望）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **贝尔曼方程**\n",
    "\n",
    "> [推导](https://www.bilibili.com/video/BV1sd4y167NS?p=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单地说就是： 状态价值 = 即时奖励 + 未来奖励的折扣和， state value = immediate reward + sum( discount factor * next state value )\n",
    "\n",
    "![](./img/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展开来看，对于指定的策略，每个状态下的价值函数：\n",
    "\n",
    "![](./img/bellmaneq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多个状态的价值函数联立，就能得到状态转移概率矩阵：\n",
    "\n",
    "![](./img/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **计算状态价值**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学的方法需要求矩阵的逆，对于大型矩阵比较困难。可以通过迭代方法来求解。\n",
    "\n",
    "- 动态规划 bootstrapping方法\n",
    "- 蒙特卡洛\n",
    "- 时序差分学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫决策过程 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在奖励过程上添加了决策条件\n",
    "\n",
    "![](./img/7.png)\n",
    "\n",
    "在马尔可夫决策过程中，动作是由智能体决定的，智能体会采取动作来决定未来的状态转移"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q 函数 （动作价值函数）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即添加了 决策条件 后的状态价值函数，通过某一状态下采取某一动作，得出回报的期望\n",
    "\n",
    "![](./img/8.png)\n",
    "\n",
    "该状态下 每个决策的Q函数值 的加权平均就是 该状态的状态价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于 Q 函数的贝尔曼方程同理，只是增加了策略条件。整理得到的就是Q函数的**贝尔曼期望方程**：\n",
    "\n",
    "![](./img/9.png)\n",
    "\n",
    "即： 当前价值 = 期望(即时奖励 + 折扣 * 未来价值)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于某一个状态，它的当前价值是与它的未来价值线性相关的\n",
    "\n",
    "当前价值 = 即时奖励 + 折扣因子 * 未来价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **预测和控制**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测 就是评估决策的价值\n",
    "\n",
    "控制 就是搜索最优决策\n",
    "\n",
    "预测和控制是马尔可夫决策过程的两个重要方面\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过策略迭代和价值迭代来解决马尔可夫决策过程的控制问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **策略迭代**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包括：\n",
    "\n",
    "- **策略评估** 根据当前策略估算价值函数\n",
    "- **策略改进** 得到价值函数后 推算其Q函数，做贪心搜索\n",
    "\n",
    "两个步骤迭代进行，直到收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **价值迭代**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多次迭代贝尔曼最优方程，直到价值函数收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代算法过程：\n",
    "1. 初始化，所有状态的价值都设0\n",
    "2. 重复以下操作，直到所有状态的价值不再变化\n",
    "   - 计算每个状态下每个动作的Q值\n",
    "   - 利用Q值更新状态价值和动作条件（选取最大项）\n",
    "3. 收敛后，提取最优策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 折扣因子作用**\n",
    "\n",
    "超参数 调节智能体远视/近视 视野 避免无穷奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 解析解难求得**\n",
    "\n",
    "需要计算状态转移概率矩阵的逆 大型矩阵的逆很难求得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 贝尔曼方程求解**\n",
    "\n",
    "蒙特卡洛 动态规划 时序差分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 奖励过程与决策过程的区别**\n",
    "\n",
    "奖励过程只根据奖励信号来调整行为，而决策过程通过 agent 的介入具有了决策的自主性，agent 可以根据环境的变化做出相应的调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. 寻找最优策略？**\n",
    "\n",
    "策略迭代 / 价值迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 表格型方法\n",
    "\n",
    "> 9/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是查表\n",
    "\n",
    "- 蒙特卡洛\n",
    "- Q-learning\n",
    "- Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态、动作、状态转移概率和奖励 `(S、A、P 、R)`，这 4 个合集就构成了强化学习马尔可夫决策过程的四元组，后面也可能会再加上折扣因子构成五元组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习用价值函数 `V (S)` 来表示状态是好的还是坏的，\n",
    "\n",
    "用 Q 函数来判断在什么状态下采取什么动作能够取得最大奖励，即用Q 函数来表示状态-动作值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 免模型预测\n",
    "\n",
    "在无法获取环境模型的情况下，通过 **蒙特卡洛方法** 和 **时序差分方法** 估计某个策略的价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **蒙特卡洛策略评估**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采样大量轨迹、计算所有轨迹的真实回报、计算平均值\n",
    "\n",
    "局限性：只能用于 **有终止** 的马尔可夫决策过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**增量式蒙特卡洛方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获得 新的轨迹 后\n",
    "\n",
    "将原本蒙特卡洛的经验均值转换成增量均值\n",
    "\n",
    "用上一时刻的值和当前时刻的增量 更新现在的值\n",
    "\n",
    "![](./img/10.png)\n",
    "\n",
    "- $\\alpha$ 是一个超参数，用来控制更新速度\n",
    "- $G_t$ 代表当前时刻的回报，如 $G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma ^2 r_{t+3} + ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 增量式蒙特卡洛方法只需要一条轨迹就能更新轨迹上的所有状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **时序差分**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态价值迭代：下一个状态会影响上一个状态（例子：巴普洛夫的狗、多级条件反射）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 免模型的\n",
    "- 可以从不完整的回合中学习\n",
    "- 结合了自举的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于给定的策略 $\\pi$ ,一步一步地计算价值函数 $v_{\\pi}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一步时序差分 TD(0)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每往前走一步，就做一步自举，更新上一步的值：\n",
    "\n",
    "![](./img/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 免模型控制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\epsilon -greedy$ 探索**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有 $1-\\epsilon$ 概率按照Q函数来决定动作（取最大值）\n",
    "\n",
    "一般取 $\\epsilon = 0.1$ 在实现上会让 $\\epsilon$ 随时间递减\n",
    "\n",
    "> 最初的时候不知道那个动作较好，引入随机性实现探索；随着探索次数增加，随机性减小，就减少探索，选择最优动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sarsa 同策略时序差分控制**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用时序差分方法 估计 Q函数 (把时序的 V 改成 Q函数)\n",
    "\n",
    "![](./img/12.png)\n",
    "\n",
    "State Action Reward State' Action' SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码实现：\n",
    "- 根据 Q 表格选择动作 （$\\epsilon-greedy$ 策略）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_action(action_space, epsilon, pi, Q, s):\n",
    "    if np.random.rand() < epsilon: # explore\n",
    "        return np.random.choice(action_space,p=pi)\n",
    "    else : #  exploit\n",
    "        return np.nanargmax( Q[s,:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 获取 $s_t,a_t,r_{t+1},s_{t+1},a_{t+1}$ 更新 Q 表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(s,a,r,s_next,a_next,alpha,gamma,Q):\n",
    "    if s_next == 'terminal': # 结束状态\n",
    "        Q[s,a] = Q[s,a] + alpha * (r-Q[s,a])\n",
    "    else :\n",
    "        Q[s,a] = Q[s,a] + alpha * (r + gamma*Q[s_next,a_next]-Q[s,a]) # 时序差分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarsa 优化的是其实际执行的策略，而 Q-learning 优化的是其估计的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q学习 异策略时序差分控制**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 学习的下一个动作都是通过 argmax 函数来选择的\n",
    "\n",
    "![](./img/13.png)\n",
    "\n",
    "二者的更新公式是一样的，区别在于 Q 学习采取的动作是 argmax 选择的动作，Q 学习只需要接收 当前状态 s 和 动作 a 的 Q 值，然后根据 Q 值选择动作即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(s,a,r,s_next,gamma,alpha,Q):\n",
    "    Q[s,a] = Q[s,a] + alpha*(r + gamma*Q[s_next, np.nanargmax(Q[s_next,:]) ] - Q[s,a]) # 不考虑终点状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 学习不需要知道 a_next，其探索更加大胆"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dailly38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
