{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EazyRL_v1.0.6 笔记\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 绪论\n",
    "> 9/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强化学习 RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 机器学习的一个分支\n",
    "- 研究智能主体在环境中如何采取行动以最大化累积奖励\n",
    "- 基本思想是通过智能体（Agent）与环境（Environment）的**交互**来**学习最优策略**，使得智能体能够采取一系列**动作**以**获取最大的长期奖励**\n",
    "- 目标是寻找最优策略\n",
    "\n",
    "![](./img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 产生 action 作用于 env\n",
    "\n",
    "env 根据 action 产生 reward 和 next_state\n",
    "\n",
    "agent 根据 reward 和 next_state 进行更新\n",
    "\n",
    "循环上述过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**与监督学习对比**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督学习：\n",
    "- 样本互不相关，是独立同分布的\n",
    "- 根据监督者给出的 label 确定正误\n",
    "- 通过反向传播更新模型参数，实现学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习：\n",
    "- 输入的是序列数据，数据之间有很强的关联\n",
    "- 不给出正确的动作，需要学习器不断尝试\n",
    "- 延迟奖励：奖励信号不是立即给出的，而是延迟一段时间\n",
    "- 动作会影响后面的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度强化学习：省去特征提取，实现端到端"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 序列决策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在与环境的交互过程中，智能体会获得很多观测\n",
    "\n",
    "每一个观测 - 采取一个动作 - 得到一个奖励\n",
    "\n",
    "**历史是观测 动作 奖励的序列**\n",
    "\n",
    "智能体采取**当前动作** 依赖于 **得到的历史**\n",
    "\n",
    "**状态是历史的函数**\n",
    "\n",
    "基本问题：**学习与规划**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体能够观察环境所有状态（上帝视角） - 完全可观测的环境 - 马尔可夫决策过程 MDP\n",
    "\n",
    "智能体只能看到部分的观测状态（有限信息） - 部分可观测的环境 - 部分可观测马尔可夫决策过程 POMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动作空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定环境中 有效动作的集合\n",
    "\n",
    "- 离散动作空间：动作数量有限，如围棋\n",
    "- 连续动作空间：动作是实值的向量，如智能驾驶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 智能体组成\n",
    "\n",
    "> 策略，价值函数，模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **策略**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入状态到动作的函数\n",
    "\n",
    "- 随机性策略：概率分布采样\n",
    "- 确定性策略：直接选择最有可能的动作（可能性最大的动作）\n",
    "\n",
    "通常采用随机性策略，避免出现局部最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **价值函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数值是对**未来奖励**的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**折扣因子**\n",
    "\n",
    "折扣因子γ（gamma） 反映了决策者对未来收益相对于当前收益的重视程度。\n",
    "\n",
    "折扣因子越大，表示决策者越看重未来的收益；\n",
    "\n",
    "折扣因子越小，表示决策者越倾向于即时满足，即更看重当前的收益。\n",
    "\n",
    "\n",
    "\n",
    "![](./img/2.png)\n",
    "\n",
    "pi策略 gamma折扣因子 r回报 s状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 函数**\n",
    "\n",
    "包含 状态 和 动作 两个变量，未来可以获得奖励的期望取决于当前的状态和当前的动作。\n",
    "\n",
    "![](./img/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 状态转移概率\n",
    "- 奖励函数：当前状态采取某个动作可以得到的奖励（价值函数是预测）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 智能体类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **基于价值 / 基于策略**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于价值：学习价值函数 （QL,Sarsa）\n",
    "- 基于策略：学习策略，通过状态给出动作概率 (PG)\n",
    "- 结合两者：演员（策略）- 评论员（价值）智能体\n",
    "\n",
    "基于价值迭代的方法只能应用在不连续的、离散的环境下，而基于策略迭代的方法可以应用在连续的环境中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **有模型 / 免模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是否需要对真实环境进行建模：\n",
    "\n",
    "- 有模型：学习状态转移来采取动作，如状态转移和奖励函数都是**已知**的，能知道某一状态采取某一决策后的奖励和状态（动规问题）\n",
    "- 免模型：学习价值函数和策略函数进行决策，没有环境转移函数和奖励函数。没有对真实环境进行建模，智能体只能在真实环境中通过一定的策略来执行动作，**等待**奖励和状态迁移，然后根据这些反馈信息来**更新**动作策略，这样反复迭代直到学习到最优策略。\n",
    "\n",
    "有模型同时在真实环境与虚拟世界学习\n",
    "\n",
    "免模型直接与真实环境交互来学习 一般需要大量数据样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索与利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单步强化学习任务 - K-臂赌博机\n",
    "\n",
    "- 仅探索：机会平均分配，最后计算期望\n",
    "- 仅利用：贪心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CartPole-v0 环境**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info, others = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态信息 observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.2427716, 3.7990518, 6.626667 , 4.7200627], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奖励 reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "游戏是否完成了 done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调试信息 info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`step()` 完成了一个完整的 S-A-R-S' 迭代过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看 gym 库中的环境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CartPole-v0',\n",
       " 'CartPole-v1',\n",
       " 'MountainCar-v0',\n",
       " 'MountainCarContinuous-v0',\n",
       " 'Pendulum-v1',\n",
       " 'Acrobot-v1',\n",
       " 'LunarLander-v2',\n",
       " 'LunarLanderContinuous-v2',\n",
       " 'BipedalWalker-v3',\n",
       " 'BipedalWalkerHardcore-v3',\n",
       " 'CarRacing-v2',\n",
       " 'Blackjack-v1',\n",
       " 'FrozenLake-v1',\n",
       " 'FrozenLake8x8-v1',\n",
       " 'CliffWalking-v0',\n",
       " 'Taxi-v3',\n",
       " 'Reacher-v2',\n",
       " 'Reacher-v4',\n",
       " 'Pusher-v2',\n",
       " 'Pusher-v4',\n",
       " 'InvertedPendulum-v2',\n",
       " 'InvertedPendulum-v4',\n",
       " 'InvertedDoublePendulum-v2',\n",
       " 'InvertedDoublePendulum-v4',\n",
       " 'HalfCheetah-v2',\n",
       " 'HalfCheetah-v3',\n",
       " 'HalfCheetah-v4',\n",
       " 'Hopper-v2',\n",
       " 'Hopper-v3',\n",
       " 'Hopper-v4',\n",
       " 'Swimmer-v2',\n",
       " 'Swimmer-v3',\n",
       " 'Swimmer-v4',\n",
       " 'Walker2d-v2',\n",
       " 'Walker2d-v3',\n",
       " 'Walker2d-v4',\n",
       " 'Ant-v2',\n",
       " 'Ant-v3',\n",
       " 'Ant-v4',\n",
       " 'Humanoid-v2',\n",
       " 'Humanoid-v3',\n",
       " 'Humanoid-v4',\n",
       " 'HumanoidStandup-v2',\n",
       " 'HumanoidStandup-v4']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym import envs\n",
    "\n",
    "specs = envs.registry.values() # 原书代码有误\n",
    "ids = [spec.id for spec in specs]\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 与 `gym` 库交互"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例：小车上山"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<MountainCarEnv<MountainCar-v0>>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观测空间：连续空间用 `gym.spaces.Box` 类 表示范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作空间：离散空间用 `gym.spaces.Discrete` 类 表示可取的数量，即动作数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现智能体，控制小车移动："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "    def decide(self, observation):\n",
    "        pos,vel = observation\n",
    "        lb = min( -0.09 * (pos + 0.25) ** 2 + 0.03,\n",
    "                0.3 * (pos + 0.9) ** 4 - 0.008 )\n",
    "        ub = -0.07 * (pos + 0.38) ** 2 + 0.07\n",
    "        if lb < vel < ub:\n",
    "            return  2\n",
    "        else: return 0\n",
    "    def learn(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SimpleAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与环境交互："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env,agent,render=False,train=False):\n",
    "    episode_reward = 0 # 回合总奖励\n",
    "    observation = env.reset() \n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.decide(observation)\n",
    "        next_obs,reward,done,_ = env.step(action) \n",
    "        episode_reward += reward\n",
    "        if train:\n",
    "            agent.learn(observation,action,reward,done)\n",
    "        if done:\n",
    "            break\n",
    "        observation = next_obs\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交互一回合：\n",
    "> !TODO notebook 中无法显示 gym 窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-105.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.seed(7) # 随机种子 v0.26 已经去掉了这个方法\n",
    "episode_reward= play(env,agent)\n",
    "episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估智能体的性能，需要计算出连续交互 100 回合的平均回合奖励："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-106.13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_rewards = [ play(env,agent) for _ in range(100) ]\n",
    "import numpy as np\n",
    "np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **总结**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('') # 取环境\n",
    "obs = env.reset()  # 初始化环境\n",
    "action = 1         # 随机动作\n",
    "next_obs,reward,done,info = env.step(action)   # 执行动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**习题**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 强化学习基本结构？**\n",
    "\n",
    "agent, environment, reward, value function, policy/action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 比监督学习训练过程困难的原因**\n",
    "\n",
    "延迟奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 基本特征**\n",
    "\n",
    "exploration, exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. 状态和观测？**\n",
    "\n",
    "状态：客观世界中事物的属性，是事物内部特征的集合，是全集\n",
    "\n",
    "观测：是状态的一种表现，是外界对状态的一种观察，是子集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. 智能体组成**\n",
    "\n",
    "policy, value function, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. 分类？**\n",
    "\n",
    "policy_based, value_based\n",
    "\n",
    "model_free, model_based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. 基于策略迭代和基于价值迭代的强化学习方法有什么区别**\n",
    "\n",
    "策略：通过策略给出概率，连续的环境\n",
    "\n",
    "价值：学习价值函数，离散的环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. 有模型学习和免模型学习有什么区别**\n",
    "\n",
    "有：奖励函数 / 价值函数 都是明确的，可以建立清晰的模型\n",
    "\n",
    "无：现实情况，无需模型。通过智能体与环境的交互来学习。需要较大的数据样本量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. 强化学习？**\n",
    "\n",
    "通过智能体与环境的不断交互，优化调整策略，以期达到最优奖励\n",
    "\n",
    "输入是一串序列，样本之间具有相关性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 马尔可夫决策过程\n",
    "\n",
    "> 9/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体与环境的交互过程 可以用 马尔可夫决策过程 表示，马尔可夫决策过程是强化学习的基本框架\n",
    "\n",
    "在马尔可夫决策过程中，它的环境是**全部可观测**的。\n",
    "\n",
    "但是很多时候环境里面有些量是不可观测的，但是这个部分观测的问题也可以转换成马尔可夫决策过程的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫过程 MP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **马尔可夫性质**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机过程 在某一时刻的状态 只与它前一个时刻的状态有关，而与它之前所有状态都无关的性质\n",
    "\n",
    "即 **未来的转移与过去的是独立的，无记忆性**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **马尔可夫链**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "马尔可夫过程：一组具有马尔可夫性质的随机变量序列\n",
    "\n",
    "马尔可夫链：离散时间的马尔可夫过程，状态是有限的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用状态转移矩阵来描述状态转移\n",
    "\n",
    "`a[i][j]` 表示 状态 `si` 到 状态 `sj` 的转移概率\n",
    "\n",
    "![](./img/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫奖励过程 MRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "马尔可夫链 + 奖励函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **回报和价值函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回报 return** 时刻t的回报是当前获得的所有奖励的和 (单个轨迹)\n",
    "\n",
    "**折扣回报 discounted return** 具有折扣因子的回报函数，折扣因子作为超参数，可以调整出不同偏好（长短期收益）下的 agent\n",
    "\n",
    "**价值函数** 状态价值函数：在某状态下，agent能获得的回报期望 v(pi,s) （所有轨迹计算后的期望）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **贝尔曼方程**\n",
    "\n",
    "> [推导](https://www.bilibili.com/video/BV1sd4y167NS?p=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单地说就是： 状态价值 = 即时奖励 + 未来奖励的折扣和， state value = immediate reward + sum( discount factor * next state value )\n",
    "\n",
    "![](./img/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展开来看，对于指定的策略，每个状态下的价值函数：\n",
    "\n",
    "![](./img/bellmaneq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多个状态的价值函数联立，就能得到状态转移概率矩阵：\n",
    "\n",
    "![](./img/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **计算状态价值**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学的方法需要求矩阵的逆，对于大型矩阵比较困难。可以通过迭代方法来求解。\n",
    "\n",
    "- 动态规划 bootstrapping方法\n",
    "- 蒙特卡洛\n",
    "- 时序差分学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫决策过程 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在奖励过程上添加了决策条件\n",
    "\n",
    "![](./img/7.png)\n",
    "\n",
    "在马尔可夫决策过程中，动作是由智能体决定的，智能体会采取动作来决定未来的状态转移"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q 函数 （动作价值函数）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即添加了 决策条件 后的状态价值函数，通过某一状态下采取某一动作，得出回报的期望\n",
    "\n",
    "![](./img/8.png)\n",
    "\n",
    "该状态下 每个决策的Q函数值 的加权平均就是 该状态的状态价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于 Q 函数的贝尔曼方程同理，只是增加了策略条件。整理得到的就是Q函数的**贝尔曼期望方程**：\n",
    "\n",
    "![](./img/9.png)\n",
    "\n",
    "即： 当前价值 = 期望(即时奖励 + 折扣 * 未来价值)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于某一个状态，它的当前价值是与它的未来价值线性相关的\n",
    "\n",
    "当前价值 = 即时奖励 + 折扣因子 * 未来价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **预测和控制**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测 就是评估决策的价值\n",
    "\n",
    "控制 就是搜索最优决策\n",
    "\n",
    "预测和控制是马尔可夫决策过程的两个重要方面\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过策略迭代和价值迭代来解决马尔可夫决策过程的控制问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **策略迭代**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包括：\n",
    "\n",
    "- **策略评估** 根据当前策略估算价值函数\n",
    "- **策略改进** 得到价值函数后 推算其Q函数，做贪心搜索\n",
    "\n",
    "两个步骤迭代进行，直到收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **价值迭代**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多次迭代贝尔曼最优方程，直到价值函数收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代算法过程：\n",
    "1. 初始化，所有状态的价值都设0\n",
    "2. 重复以下操作，直到所有状态的价值不再变化\n",
    "   - 计算每个状态下每个动作的Q值\n",
    "   - 利用Q值更新状态价值和动作条件（选取最大项）\n",
    "3. 收敛后，提取最优策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 折扣因子作用**\n",
    "\n",
    "超参数 调节智能体远视/近视 视野 避免无穷奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 解析解难求得**\n",
    "\n",
    "需要计算状态转移概率矩阵的逆 大型矩阵的逆很难求得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 贝尔曼方程求解**\n",
    "\n",
    "蒙特卡洛 动态规划 时序差分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 奖励过程与决策过程的区别**\n",
    "\n",
    "奖励过程只根据奖励信号来调整行为，而决策过程通过 agent 的介入具有了决策的自主性，agent 可以根据环境的变化做出相应的调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. 寻找最优策略？**\n",
    "\n",
    "策略迭代 / 价值迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 表格型方法\n",
    "\n",
    "> 9/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是查表\n",
    "\n",
    "- 蒙特卡洛\n",
    "- Q-learning\n",
    "- Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态、动作、状态转移概率和奖励 `(S、A、P 、R)`，这 4 个合集就构成了强化学习马尔可夫决策过程的四元组，后面也可能会再加上折扣因子构成五元组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习用价值函数 `V (S)` 来表示状态是好的还是坏的，\n",
    "\n",
    "用 Q 函数来判断在什么状态下采取什么动作能够取得最大奖励，即用Q 函数来表示状态-动作值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 免模型预测\n",
    "\n",
    "在无法获取环境模型的情况下，通过 **蒙特卡洛方法** 和 **时序差分方法** 估计某个策略的价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **蒙特卡洛策略评估**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采样大量轨迹、计算所有轨迹的真实回报、计算平均值\n",
    "\n",
    "局限性：只能用于 **有终止** 的马尔可夫决策过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**增量式蒙特卡洛方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获得 新的轨迹 后\n",
    "\n",
    "将原本蒙特卡洛的经验均值转换成增量均值\n",
    "\n",
    "用上一时刻的值和当前时刻的增量 更新现在的值\n",
    "\n",
    "![](./img/10.png)\n",
    "\n",
    "- $\\alpha$ 是一个超参数，用来控制更新速度\n",
    "- $G_t$ 代表当前时刻的回报，如 $G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma ^2 r_{t+3} + ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 增量式蒙特卡洛方法只需要一条轨迹就能更新轨迹上的所有状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **时序差分**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态价值迭代：下一个状态会影响上一个状态（例子：巴普洛夫的狗、多级条件反射）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 免模型的\n",
    "- 可以从不完整的回合中学习\n",
    "- 结合了自举的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于给定的策略 $\\pi$ ,一步一步地计算价值函数 $v_{\\pi}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一步时序差分 TD(0)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每往前走一步，就做一步自举，更新上一步的值：\n",
    "\n",
    "![](./img/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 免模型控制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\epsilon -greedy$ 探索**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有 $1-\\epsilon$ 概率按照Q函数来决定动作（取最大值）\n",
    "\n",
    "一般取 $\\epsilon = 0.1$ 在实现上会让 $\\epsilon$ 随时间递减\n",
    "\n",
    "> 最初的时候不知道那个动作较好，引入随机性实现探索；随着探索次数增加，随机性减小，就减少探索，选择最优动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sarsa 同策略时序差分控制**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用时序差分方法 估计 Q函数 (把时序的 V 改成 Q函数)\n",
    "\n",
    "![](./img/12.png)\n",
    "\n",
    "State Action Reward State' Action' SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码实现：\n",
    "- 根据 Q 表格选择动作 （$\\epsilon-greedy$ 策略）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_action(action_space, epsilon, pi, Q, s):\n",
    "    if np.random.rand() < epsilon: # explore\n",
    "        return np.random.choice(action_space,p=pi)\n",
    "    else : #  exploit\n",
    "        return np.nanargmax( Q[s,:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 获取 $s_t,a_t,r_{t+1},s_{t+1},a_{t+1}$ 更新 Q 表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(s,a,r,s_next,a_next,alpha,gamma,Q):\n",
    "    if s_next == 'terminal': # 结束状态\n",
    "        Q[s,a] = Q[s,a] + alpha * (r-Q[s,a])\n",
    "    else :\n",
    "        Q[s,a] = Q[s,a] + alpha * (r + gamma*Q[s_next,a_next]-Q[s,a]) # 时序差分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarsa 优化的是其实际执行的策略，而 Q-learning 优化的是其估计的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q学习 异策略时序差分控制**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 学习的下一个动作都是通过 argmax 函数来选择的\n",
    "\n",
    "![](./img/13.png)\n",
    "\n",
    "二者的更新公式是一样的，区别在于 Q 学习采取的动作是 argmax 选择的动作，Q 学习只需要接收 当前状态 s 和 动作 a 的 Q 值，然后根据 Q 值选择动作即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(s,a,r,s_next,gamma,alpha,Q):\n",
    "    Q[s,a] = Q[s,a] + alpha*(r + gamma*Q[s_next, np.nanargmax(Q[s_next,:]) ] - Q[s,a]) # 不考虑终点状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 学习不需要知道 a_next，其探索更加大胆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 学习解决悬崖寻路问题\n",
    "\n",
    "> 9/11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**环境：`CliffWalking-v0`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 每走一步 收到 -1 奖励\n",
    "- 掉入悬崖 返回起点并收到 -100 奖励\n",
    "- 超出边界 不会移动 但会收到 -1 奖励\n",
    "- 目标是以最少的步数到达终点\n",
    "\n",
    "![](./img/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化环境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import turtle\n",
    "import numpy as np\n",
    "\n",
    "# turtle tutorial : https://docs.python.org/3.3/library/turtle.html\n",
    "\n",
    "class CliffWalkingWapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.t = None\n",
    "        self.unit = 50\n",
    "        self.max_x = 12\n",
    "        self.max_y = 4\n",
    "\n",
    "    def draw_x_line(self, y, x0, x1, color='gray'):\n",
    "        assert x1 > x0\n",
    "        self.t.color(color)\n",
    "        self.t.setheading(0)\n",
    "        self.t.up()\n",
    "        self.t.goto(x0, y)\n",
    "        self.t.down()\n",
    "        self.t.forward(x1 - x0)\n",
    "\n",
    "    def draw_y_line(self, x, y0, y1, color='gray'):\n",
    "        assert y1 > y0\n",
    "        self.t.color(color)\n",
    "        self.t.setheading(90)\n",
    "        self.t.up()\n",
    "        self.t.goto(x, y0)\n",
    "        self.t.down()\n",
    "        self.t.forward(y1 - y0)\n",
    "\n",
    "    def draw_box(self, x, y, fillcolor='', line_color='gray'):\n",
    "        self.t.up()\n",
    "        self.t.goto(x * self.unit, y * self.unit)\n",
    "        self.t.color(line_color)\n",
    "        self.t.fillcolor(fillcolor)\n",
    "        self.t.setheading(90)\n",
    "        self.t.down()\n",
    "        self.t.begin_fill()\n",
    "        for i in range(4):\n",
    "            self.t.forward(self.unit)\n",
    "            self.t.right(90)\n",
    "        self.t.end_fill()\n",
    "\n",
    "    def move_player(self, x, y):\n",
    "        self.t.up()\n",
    "        self.t.setheading(90)\n",
    "        self.t.fillcolor('red')\n",
    "        self.t.goto((x + 0.5) * self.unit, (y + 0.5) * self.unit)\n",
    "\n",
    "    def render(self):\n",
    "        if self.t == None:\n",
    "            self.t = turtle.Turtle()\n",
    "            self.wn = turtle.Screen()\n",
    "            self.wn.setup(self.unit * self.max_x + 100,\n",
    "                          self.unit * self.max_y + 100)\n",
    "            self.wn.setworldcoordinates(0, 0, self.unit * self.max_x,\n",
    "                                        self.unit * self.max_y)\n",
    "            self.t.shape('circle')\n",
    "            self.t.width(2)\n",
    "            self.t.speed(0)\n",
    "            self.t.color('gray')\n",
    "            for _ in range(2):\n",
    "                self.t.forward(self.max_x * self.unit)\n",
    "                self.t.left(90)\n",
    "                self.t.forward(self.max_y * self.unit)\n",
    "                self.t.left(90)\n",
    "            for i in range(1, self.max_y):\n",
    "                self.draw_x_line(\n",
    "                    y=i * self.unit, x0=0, x1=self.max_x * self.unit)\n",
    "            for i in range(1, self.max_x):\n",
    "                self.draw_y_line(\n",
    "                    x=i * self.unit, y0=0, y1=self.max_y * self.unit)\n",
    "\n",
    "            for i in range(1, self.max_x - 1):\n",
    "                self.draw_box(i, 0, 'black')\n",
    "            self.draw_box(self.max_x - 1, 0, 'yellow')\n",
    "            self.t.shape('turtle')\n",
    "\n",
    "        x_pos = self.s % self.max_x\n",
    "        y_pos = self.max_y - 1 - int(self.s / self.max_x)\n",
    "        self.move_player(x_pos, y_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 4)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env_name = 'CliffWalking-v0'\n",
    "env = gym.make(env_name)\n",
    "env = CliffWalkingWapper(env)\n",
    "env.seed(1)\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "num_states, num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "a = env.action_space.sample()\n",
    "\n",
    "s,a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "环境初始化状态："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **强化学习基本过程**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 初始化 agent env\n",
    "2. agent.get_action(state)\n",
    "3. 执行 action , 返回 reward, new_state, done\n",
    "4. 学习，即进行策略更新\n",
    "5. 循环 2-4 直到算法收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**如何确定算法收敛：滑动平均奖励**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "滑动平均量 反映了奖励变化的趋势："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.9500000000000001,\n",
       " 0.8883333343267442,\n",
       " 0.8245000008940698,\n",
       " 0.7620500011026861,\n",
       " 0.7025116681557896,\n",
       " 0.6465462162645461,\n",
       " 0.5943915946380914,\n",
       " 0.5460635463681776,\n",
       " 0.5014571918803714,\n",
       " 0.4604023820541736,\n",
       " 0.4226954774304423,\n",
       " 0.3881182376662666,\n",
       " 0.35644927136180765,\n",
       " 0.3274710112399873,\n",
       " 0.30097391011598856,\n",
       " 0.2767588720674796,\n",
       " 0.2546385404576793,\n",
       " 0.23443784434586182,\n",
       " 0.21599405998578144,\n",
       " 0.19915655883780545,\n",
       " 0.18378635763494455,\n",
       " 0.16975554803939122,\n",
       " 0.15694666002629512,\n",
       " 0.14525199393425864,\n",
       " 0.13457294853026702,\n",
       " 0.12481935740853876,\n",
       " 0.11590885039876876,\n",
       " 0.10776624120811502,\n",
       " 0.10032295059448373,\n",
       " 0.09351646189051174,\n",
       " 0.08728981570146058,\n",
       " 0.08159113725192763,\n",
       " 0.07637320000827984,\n",
       " 0.07159302288056024,\n",
       " 0.06721149839097806,\n",
       " 0.06319305132002725,\n",
       " 0.05950532515499974,\n",
       " 0.05611889523703443,\n",
       " 0.05300700575058389,\n",
       " 0.05014532947490866,\n",
       " 0.047511748952718875,\n",
       " 0.04508615544413236,\n",
       " 0.042850267240178944,\n",
       " 0.04078746279219302,\n",
       " 0.03888262959694427,\n",
       " 0.037122026144345656,\n",
       " 0.0354931569253326,\n",
       " 0.03398465751751547,\n",
       " 0.03258619172106044,\n",
       " 0.031288356932072554,\n",
       " 0.03008259823358242,\n",
       " 0.028961130891169765,\n",
       " 0.027916869667702012,\n",
       " 0.026943364461540963,\n",
       " 0.026034742380928802,\n",
       " 0.025185654120819398,\n",
       " 0.024391226633349028,\n",
       " 0.023647019227408438,\n",
       " 0.022948984058257703,\n",
       " 0.022293429823121515,\n",
       " 0.021676990018547556,\n",
       " 0.021096592695648356,\n",
       " 0.02054943342608352,\n",
       " 0.020032951623369512,\n",
       " 0.01954480802133911,\n",
       " 0.019082864497887195,\n",
       " 0.018645166288870963,\n",
       " 0.018229925049297567,\n",
       " 0.017835503980922002,\n",
       " 0.017460404260820715,\n",
       " 0.017103252733975564,\n",
       " 0.01676279046407036,\n",
       " 0.01643786280173682,\n",
       " 0.01612740988718232,\n",
       " 0.015830458381951695,\n",
       " 0.015546113841248314,\n",
       " 0.01527355375589081,\n",
       " 0.01501202119694187,\n",
       " 0.014760819095874135,\n",
       " 0.014519305096719538,\n",
       " 0.014286886736739168,\n",
       " 0.014063017295290671,\n",
       " 0.013847191778412142,\n",
       " 0.013638943211815368,\n",
       " 0.013437839583976516,\n",
       " 0.013243480908653245,\n",
       " 0.013055496488017833,\n",
       " 0.012873542356344756,\n",
       " 0.012697299258726267,\n",
       " 0.012526470459387387,\n",
       " 0.012360779955433922,\n",
       " 0.01219997077609341,\n",
       " 0.012043803452031975,\n",
       " 0.011892054730871765,\n",
       " 0.011744515955495341,\n",
       " 0.011600992163313185,\n",
       " 0.011461301089339931,\n",
       " 0.011325271989566219,\n",
       " 0.011192744768257856,\n",
       " 0.011063569285657333,\n",
       " 0.010937604548650677,\n",
       " 0.010814717889235345,\n",
       " 0.01069478459767037,\n",
       " 0.010577687126650215,\n",
       " 0.010463314654457987,\n",
       " 0.010351562598671097,\n",
       " 0.0102423322716286,\n",
       " 0.010135530197822769,\n",
       " 0.010031068058345067,\n",
       " 0.009928862175226224,\n",
       " 0.00982883314047457,\n",
       " 0.009730905575342785,\n",
       " 0.009635008006800245,\n",
       " 0.009541072383829084,\n",
       " 0.00944903410775196,\n",
       " 0.009358831593865736,\n",
       " 0.009270406063176318,\n",
       " 0.009183701634356788,\n",
       " 0.009098664847716164,\n",
       " 0.00901524460930032,\n",
       " 0.00893339223371508,\n",
       " 0.008853061110137962,\n",
       " 0.008774206587993261,\n",
       " 0.008696785967191896,\n",
       " 0.008620758209950483,\n",
       " 0.008546083960825285,\n",
       " 0.008472725564742756,\n",
       " 0.008400646803830271,\n",
       " 0.008329812893394416,\n",
       " 0.008260190385169196,\n",
       " 0.008191747126805551,\n",
       " 0.008124452132279665,\n",
       " 0.008058275558392695,\n",
       " 0.007993188730186665,\n",
       " 0.007929163977554242,\n",
       " 0.007866174581319808,\n",
       " 0.007804194817844676,\n",
       " 0.0077431998129072616,\n",
       " 0.007683165549893633,\n",
       " 0.007624068830602874,\n",
       " 0.007565887286538044,\n",
       " 0.007508599249369645,\n",
       " 0.007452183774051139,\n",
       " 0.007396620566490653,\n",
       " 0.007341890011587766,\n",
       " 0.007287973105334366,\n",
       " 0.007234851486837677,\n",
       " 0.007182507280876469,\n",
       " 0.007130923235598414,\n",
       " 0.0070800825670509415,\n",
       " 0.007029969052089651,\n",
       " 0.00698056692573136,\n",
       " 0.006931860881904119,\n",
       " 0.006883836074122209,\n",
       " 0.006836478116093652,\n",
       " 0.0067897729891347085,\n",
       " 0.006743707098541307,\n",
       " 0.006698267184624952,\n",
       " 0.006653440475475683,\n",
       " 0.0066092144519197,\n",
       " 0.006565576961944137,\n",
       " 0.006522516183980973,\n",
       " 0.006480020640428666,\n",
       " 0.006438079163255516,\n",
       " 0.006396680863042674,\n",
       " 0.006355815194255115,\n",
       " 0.006315471781154872,\n",
       " 0.006275640587085844,\n",
       " 0.00623631183399948,\n",
       " 0.006197475976594024,\n",
       " 0.006159123725605965,\n",
       " 0.006121246022206211,\n",
       " 0.00608383406152278,\n",
       " 0.006046879220678953,\n",
       " 0.006010373133726013,\n",
       " 0.005974307572818183,\n",
       " 0.005938674574100718,\n",
       " 0.005903466318178546,\n",
       " 0.005868675255368684,\n",
       " 0.005834293939030648,\n",
       " 0.005800315108394457,\n",
       " 0.0057667316699735825,\n",
       " 0.00573353677396886,\n",
       " 0.005700723650201373,\n",
       " 0.0056682856932826755,\n",
       " 0.0056362164927018805,\n",
       " 0.005604509720205646,\n",
       " 0.005573159261270804,\n",
       " 0.005542159147165217,\n",
       " 0.005511503447236417,\n",
       " 0.005481186451368152,\n",
       " 0.005451202508228379,\n",
       " 0.00542154615908923,\n",
       " 0.005392212072000463,\n",
       " 0.005363194935979449,\n",
       " 0.005334489645178917,\n",
       " 0.005306091185241165,\n",
       " 0.005277994624149104,\n",
       " 0.005250195150558324,\n",
       " 0.005222688061729823,\n",
       " 0.005195468752669473,\n",
       " 0.005168532706353526,\n",
       " 0.005141875531497712,\n",
       " 0.005115492856851025,\n",
       " 0.00508938046889079,\n",
       " 0.005063534202917567,\n",
       " 0.005037950031305091,\n",
       " 0.005012623910094561,\n",
       " 0.004987552013458546,\n",
       " 0.004962730479057196,\n",
       " 0.004938155551387873,\n",
       " 0.004913823571101434,\n",
       " 0.004889730918820745,\n",
       " 0.004865874104275745,\n",
       " 0.004842249660260477,\n",
       " 0.004818854186892807,\n",
       " 0.00479568434488204,\n",
       " 0.004772736896035912,\n",
       " 0.004750008646584609,\n",
       " 0.004727496489304654,\n",
       " 0.004705197301732021,\n",
       " 0.004683108087384567,\n",
       " 0.004661225870031593,\n",
       " 0.004639547738234828,\n",
       " 0.004618070838869182,\n",
       " 0.00459679237129143,\n",
       " 0.004575709628658156,\n",
       " 0.004554819895413512,\n",
       " 0.004534120494726593,\n",
       " 0.004513608877751197,\n",
       " 0.0044932824711289695,\n",
       " 0.004473138772572878,\n",
       " 0.004453175343760075,\n",
       " 0.004433389710803231,\n",
       " 0.004413779554071486,\n",
       " 0.004394342506500298,\n",
       " 0.004375076344599319,\n",
       " 0.004355978741264336,\n",
       " 0.004337047555535429,\n",
       " 0.004318280581151897,\n",
       " 0.004299675646214593,\n",
       " 0.00428123070288203,\n",
       " 0.004262943675266222,\n",
       " 0.0042448125553696,\n",
       " 0.004226835349729834,\n",
       " 0.004209010124532442,\n",
       " 0.004191334906513745,\n",
       " 0.0041738078266041765,\n",
       " 0.004156427062942739,\n",
       " 0.0041391907430238925,\n",
       " 0.004122097088460392,\n",
       " 0.004105144312439203,\n",
       " 0.004088330667130208,\n",
       " 0.004071654486354043,\n",
       " 0.004055114037718639,\n",
       " 0.0040387076922219385,\n",
       " 0.00402243382078064,\n",
       " 0.0040062908175213865,\n",
       " 0.003990277120742834,\n",
       " 0.003974391161932322,\n",
       " 0.0039586314362961995,\n",
       " 0.003942996426538731,\n",
       " 0.003927484673961495,\n",
       " 0.003912094702754463,\n",
       " 0.003896825091556351,\n",
       " 0.003881674421443618,\n",
       " 0.003866641298969755,\n",
       " 0.0038517243769006456,\n",
       " 0.0038369223030272,\n",
       " 0.00382223376722898,\n",
       " 0.0038076574506992035,\n",
       " 0.0037931920733795107,\n",
       " 0.0037788363668020556,\n",
       " 0.003764589096213519,\n",
       " 0.0037504490339205915,\n",
       " 0.003736414958583974,\n",
       " 0.003722485701149103,\n",
       " 0.003708660069768486,\n",
       " 0.003694936921930186,\n",
       " 0.003681315112958138,\n",
       " 0.0036677935195116268,\n",
       " 0.0036543710607348148,\n",
       " 0.003641046624159062,\n",
       " 0.0036278191619964636,\n",
       " 0.0036146875915395197,\n",
       " 0.003601650891785352,\n",
       " 0.003588708027416046,\n",
       " 0.0035758579967962883,\n",
       " 0.0035630997820389736,\n",
       " 0.0035504324204795783,\n",
       " 0.0035378549293047096,\n",
       " 0.0035253663541331207,\n",
       " 0.003512965766172497,\n",
       " 0.003500652236377497,\n",
       " 0.0034884248587581214,\n",
       " 0.003476282701508048,\n",
       " 0.0034642249027185225,\n",
       " 0.003452250570256345,\n",
       " 0.0034403588546355066,\n",
       " 0.0034285488749032766,\n",
       " 0.0034168198149191337,\n",
       " 0.003405170847024351,\n",
       " 0.0033936011331938175,\n",
       " 0.003382109872638804,\n",
       " 0.003370696274800261,\n",
       " 0.0033593595350427867,\n",
       " 0.0033480989059114555,\n",
       " 0.003336913621564576,\n",
       " 0.003325802899612369,\n",
       " 0.0033147660126209696,\n",
       " 0.003303802236050705,\n",
       " 0.0032929108246836122,\n",
       " 0.0032820910845404614,\n",
       " 0.003271342297907688,\n",
       " 0.003260663772276954,\n",
       " 0.003250054814541563,\n",
       " 0.003239514731056295,\n",
       " 0.0032290428742573875,\n",
       " 0.003218638591488262,\n",
       " 0.0032083012022257385,\n",
       " 0.003198030093998957,\n",
       " 0.003187824599168821,\n",
       " 0.003177684116860143,\n",
       " 0.0031676080131529975,\n",
       " 0.003157595670953322,\n",
       " 0.0031476465115933975,\n",
       " 0.0031377598978569532,\n",
       " 0.003127935279532411,\n",
       " 0.0031181720450140287,\n",
       " 0.0031084696438566373,\n",
       " 0.003098827487527328,\n",
       " 0.0030892450463464835,\n",
       " 0.0030797217504701896,\n",
       " 0.003070257040472795,\n",
       " 0.00306085038958815,\n",
       " 0.0030515012538775256,\n",
       " 0.003042209120513003,\n",
       " 0.00303297345810026,\n",
       " 0.0030237937651013437,\n",
       " 0.0030146695202829038,\n",
       " 0.0030056002312518594,\n",
       " 0.0029965854082883094,\n",
       " 0.00298762454079515,\n",
       " 0.0029787171692349872,\n",
       " 0.00296986278689191,\n",
       " 0.002961060937721597,\n",
       " 0.002952311164718032,\n",
       " 0.0029436130100096705,\n",
       " 0.002934965991662929,\n",
       " 0.002926369675937584,\n",
       " 0.0029178236259013034,\n",
       " 0.002909327378465797,\n",
       " 0.002900880516851603,\n",
       " 0.002892482596108013,\n",
       " 0.002884133215779388,\n",
       " 0.0028758319456064618,\n",
       " 0.0028675783517897653,\n",
       " 0.0028593720439097753,\n",
       " 0.002851212624022794,\n",
       " 0.0028430996641302993,\n",
       " 0.0028350328023166857,\n",
       " 0.0028270116197256505,\n",
       " 0.002819035739386522,\n",
       " 0.002811104775459567,\n",
       " 0.0028032183341228957,\n",
       " 0.0027953760609368086,\n",
       " 0.002787577590339446,\n",
       " 0.0027798225467590074,\n",
       " 0.002772110568897806,\n",
       " 0.002764441285021796,\n",
       " 0.002756814360570336,\n",
       " 0.0027492294478727204,\n",
       " 0.002741686187459185,\n",
       " 0.002734184232523877,\n",
       " 0.002726723247658466,\n",
       " 0.0027193029077123895,\n",
       " 0.002711922873484012,\n",
       " 0.0027045828396918626,\n",
       " 0.0026972824617334233,\n",
       " 0.002690021406183364,\n",
       " 0.0026827993729588886,\n",
       " 0.0026756160454193506,\n",
       " 0.0026684711153051035,\n",
       " 0.0026613642586163382,\n",
       " 0.0026542951837532254,\n",
       " 0.0026472636049928544,\n",
       " 0.0026402691953354134,\n",
       " 0.0026333116837641344,\n",
       " 0.0026263907797977994,\n",
       " 0.0026195061754371823,\n",
       " 0.00261265759348298,\n",
       " 0.002605844761172756,\n",
       " 0.0025990673864541864,\n",
       " 0.0025923252064801827,\n",
       " 0.0025856179381222345,\n",
       " 0.0025789453032816186,\n",
       " 0.0025723070516694845,\n",
       " 0.0025657029114597375,\n",
       " 0.0025591326147258284,\n",
       " 0.0025525959203338625,\n",
       " 0.002546092541414142,\n",
       " 0.0025396222430670023,\n",
       " 0.0025331847673166178,\n",
       " 0.002526779881984745,\n",
       " 0.002520407308261771,\n",
       " 0.002514066818502109,\n",
       " 0.0025077581845416672,\n",
       " 0.0025014811544666333,\n",
       " 0.002495235478271512,\n",
       " 0.002489020930951358,\n",
       " 0.0024828372867186557,\n",
       " 0.0024766843190815765,\n",
       " 0.002470561777631347,\n",
       " 0.0024644694602831345,\n",
       " 0.002458407138594461,\n",
       " 0.0024523745836840323,\n",
       " 0.002446371566275619,\n",
       " 0.0024403979033036792,\n",
       " 0.002434453360160032,\n",
       " 0.002428537725697314,\n",
       " 0.002422650786599835,\n",
       " 0.002416792327600408,\n",
       " 0.0024109621549585655,\n",
       " 0.0024051600477417584,\n",
       " 0.0023993858303937563,\n",
       " 0.0023936392750647587,\n",
       " 0.00238792019997301,\n",
       " 0.0023822284182318655,\n",
       " 0.002376563715077216,\n",
       " 0.002370925897221364,\n",
       " 0.0023653147907053804,\n",
       " 0.002359730192400102,\n",
       " 0.0023541719194892802,\n",
       " 0.002348639784155228,\n",
       " 0.0023431335940789625,\n",
       " 0.002337653176173399,\n",
       " 0.002332198351377097,\n",
       " 0.0023267689585347597,\n",
       " 0.0023213647827574277,\n",
       " 0.002315985677211165,\n",
       " 0.002310631463179302,\n",
       " 0.00230530195653377,\n",
       " 0.0022999969915593094,\n",
       " 0.0022947163958291195,\n",
       " 0.0022894600141590816,\n",
       " 0.00228422766031736,\n",
       " 0.0022790191899783657,\n",
       " 0.0022738344266829413,\n",
       " 0.002268673211617844,\n",
       " 0.0022635353785680924,\n",
       " 0.002258420777940201,\n",
       " 0.002253329251816971,\n",
       " 0.002248260634789857,\n",
       " 0.002243214801274234,\n",
       " 0.002238191568394745,\n",
       " 0.0022331907948466346,\n",
       " 0.0022282123301725575,\n",
       " 0.002223256015677838,\n",
       " 0.0022183217085372706,\n",
       " 0.0022134092802081674,\n",
       " 0.002208518568435982,\n",
       " 0.0022036494271916226,\n",
       " 0.0021988017250489066,\n",
       " 0.0021939753204412047,\n",
       " 0.002189170062675487,\n",
       " 0.0021843858161280255,\n",
       " 0.0021796224587374655,\n",
       " 0.00217487983408246,\n",
       " 0.0021701578013837955,\n",
       " 0.0021654562572232445,\n",
       " 0.002160775038675209,\n",
       " 0.002156114022389716,\n",
       " 0.0021514730740687248,\n",
       " 0.0021468520728439985,\n",
       " 0.002142250909934124,\n",
       " 0.002137669417586637,\n",
       " 0.0021331074913904483,\n",
       " 0.002128565014092512,\n",
       " 0.0021240418568820242,\n",
       " 0.0021195378805461958,\n",
       " 0.0021150529830765816,\n",
       " 0.0021105870260994517,\n",
       " 0.0021061398850784497,\n",
       " 0.0021017114712139186,\n",
       " 0.0020973016347369746,\n",
       " 0.0020929102874217504,\n",
       " 0.0020885372800157733,\n",
       " 0.0020841825247579454,\n",
       " 0.0020798458960971508,\n",
       " 0.0020755273043204492,\n",
       " 0.0020712265988370015,\n",
       " 0.0020669436906811806,\n",
       " 0.002062678476500858,\n",
       " 0.002058430839996054,\n",
       " 0.0020542006532137226,\n",
       " 0.002049987824279186,\n",
       " 0.0020457922472221705,\n",
       " 0.0020416138266694294,\n",
       " 0.002037452453501977,\n",
       " 0.0020333080062296606,\n",
       " 0.0020291803987944083,\n",
       " 0.0020250695071778258,\n",
       " 0.0020209752663294875,\n",
       " 0.002016897547854817,\n",
       " 0.002012836259481761,\n",
       " 0.0020087912948824043,\n",
       " 0.0020047625583616262,\n",
       " 0.002000749963793817,\n",
       " 0.0019967534103828635,\n",
       " 0.0019927727841291994,\n",
       " 0.0019888080057162793,\n",
       " 0.0019848589804761486,\n",
       " 0.001980925611566115,\n",
       " 0.001977007811827757,\n",
       " 0.001973105479535429,\n",
       " 0.0019692185347775833,\n",
       " 0.00196534687070923,\n",
       " 0.001961490414452943,\n",
       " 0.0019576490654944416,\n",
       " 0.0019538227333708736,\n",
       " 0.001950011336665672,\n",
       " 0.0019462147904623945,\n",
       " 0.0019424330066947102,\n",
       " 0.0019386658944613897,\n",
       " 0.0019349133719513264,\n",
       " 0.0019311753538929922,\n",
       " 0.0019274517635420117,\n",
       " 0.0019237425085453824,\n",
       " 0.001920047505785403,\n",
       " 0.001916366668814609,\n",
       " 0.0019126999314718154,\n",
       " 0.0019090471992873586,\n",
       " 0.0019054083988800739,\n",
       " 0.0019017834525653806,\n",
       " 0.0018981722671440916,\n",
       " 0.0018945747820200037,\n",
       " 0.0018909909077319362,\n",
       " 0.00188742057540636,\n",
       " 0.0018838636997740336,\n",
       " 0.001880320215734172,\n",
       " 0.0018767900414130047,\n",
       " 0.0018732731031241377,\n",
       " 0.0018697693229082846,\n",
       " 0.0018662786306020878,\n",
       " 0.0018628009514169927,\n",
       " 0.0018593362180433007,\n",
       " 0.0018558843466192185,\n",
       " 0.0018524452733106641,\n",
       " 0.0018490189290254321,\n",
       " 0.0018456052399390048,\n",
       " 0.0018422041396093166,\n",
       " 0.0018388155565969761,\n",
       " 0.0018354394149649993,\n",
       " 0.0018320756580116322,\n",
       " 0.0018287242114222323,\n",
       " 0.0018253850083136207,\n",
       " 0.0018220579768494054,\n",
       " 0.0018187430523768323,\n",
       " 0.0018154401767084233,\n",
       " 0.0018121492741923837,\n",
       " 0.0018088702883836307,\n",
       " 0.0018056031568400572,\n",
       " 0.0018023478000807028,\n",
       " 0.001799104169855767,\n",
       " 0.0017958721994573656,\n",
       " 0.00179265181720687,\n",
       " 0.0017894429702350473,\n",
       " 0.0017862455993180551,\n",
       " 0.0017830596395129036,\n",
       " 0.0017798850207293695,\n",
       " 0.001776721691527784,\n",
       " 0.0017735695939709128,\n",
       " 0.0017704286642737134,\n",
       " 0.001767298845029647,\n",
       " 0.001764180072931297,\n",
       " 0.0017610722910019882,\n",
       " 0.0017579754479627127,\n",
       " 0.0017548894743792994,\n",
       " 0.0017518143194025265,\n",
       " 0.0017487499256265617,\n",
       " 0.0017456962413861564,\n",
       " 0.0017426532085410545,\n",
       " 0.0017396207631234935,\n",
       " 0.0017365988592040191,\n",
       " 0.0017335874321630581,\n",
       " 0.0017305864354845272,\n",
       " 0.0017275958156624189,\n",
       " 0.001724615524541326,\n",
       " 0.0017216454954983183,\n",
       " 0.0017186856802142907,\n",
       " 0.0017157360352020486,\n",
       " 0.0017127964980400523,\n",
       " 0.0017098670125489164,\n",
       " 0.001706947539808726,\n",
       " 0.001704038021508493,\n",
       " 0.001701138405168329,\n",
       " 0.0016982486435563335,\n",
       " 0.0016953686825222644,\n",
       " 0.0016924984849724359,\n",
       " 0.001689637982597934,\n",
       " 0.0016867871372038008,\n",
       " 0.0016839459027730417,\n",
       " 0.0016811142262488298,\n",
       " 0.0016782920598800207,\n",
       " 0.001675479360690584,\n",
       " 0.0016726760667190288,\n",
       " 0.0016698821454830767,\n",
       " 0.0016670975444656136,\n",
       " 0.0016643222164012363,\n",
       " 0.0016615561187510823,\n",
       " 0.0016587992015886428,\n",
       " 0.00165605141998006,\n",
       " 0.00165331272184333,\n",
       " 0.0016505830719461812,\n",
       " 0.0016478624269380367,\n",
       " 0.0016451507361618456,\n",
       " 0.001642447965667794,\n",
       " 0.0016397540616179852,\n",
       " 0.0016370689755583123,\n",
       " 0.0016343926755216108,\n",
       " 0.0016317251094543684,\n",
       " 0.0016290662421499565,\n",
       " 0.0016264160302808769,\n",
       " 0.0016237744232108493,\n",
       " 0.0016211413870087534,\n",
       " 0.0016185168794950488,\n",
       " 0.0016159008627081493,\n",
       " 0.001613293290841095,\n",
       " 0.0016106941226676215,\n",
       " 0.0016081033210840919,\n",
       " 0.0016055208410557002,\n",
       " 0.0016029466420511207,\n",
       " 0.001600380687592161,\n",
       " 0.0015978229332069148,\n",
       " 0.0015952733388706673,\n",
       " 0.0015927318802027074,\n",
       " 0.0015901985003357977,\n",
       " 0.0015876731713724895,\n",
       " 0.001585155856563547,\n",
       " 0.0015826465111931254,\n",
       " 0.0015801450950169644,\n",
       " 0.0015776515834567619,\n",
       " 0.001575165931108982,\n",
       " 0.001572688097110442,\n",
       " 0.0015702180446842775,\n",
       " 0.0015677557407313108,\n",
       " 0.0015653011554622818,\n",
       " 0.001562854250425324,\n",
       " 0.0015604149793722259,\n",
       " 0.001557983312321129,\n",
       " 0.0015555592106468283,\n",
       " 0.0015531426512281718,\n",
       " 0.0015507335899730586,\n",
       " 0.001548331987198597,\n",
       " 0.0015459378071901852,\n",
       " 0.0015435510294462135,\n",
       " 0.0015411716122321686,\n",
       " 0.0015387995179869892,\n",
       " 0.0015364347129057194,\n",
       " 0.0015340771665638994,\n",
       " 0.001531726851579515,\n",
       " 0.0015293837316672216,\n",
       " 0.0015270477625286768,\n",
       " 0.0015247189275784366,\n",
       " 0.0015223971886065368,\n",
       " 0.0015200825112240105,\n",
       " 0.0015177748644807866,\n",
       " 0.0015154742088802691,\n",
       " 0.0015131805205170546,\n",
       " 0.0015108937662347466,\n",
       " 0.001508613916192589,\n",
       " 0.0015063409318923694,\n",
       " 0.001504074790327228,\n",
       " 0.0015018154591494592,\n",
       " 0.0014995629092461281,\n",
       " 0.001497317102774061,\n",
       " 0.0014950780173159336,\n",
       " 0.001492845621054622,\n",
       " 0.0014906198853547148,\n",
       " 0.0014884007728028096,\n",
       " 0.001486188261368376,\n",
       " 0.0014839823195824044,\n",
       " 0.0014817829191227868,\n",
       " 0.0014795900228580935,\n",
       " 0.001477403620653102,\n",
       " 0.0014752236684615159,\n",
       " 0.0014730501382832,\n",
       " 0.0014708830049180333,\n",
       " 0.001468722245685906,\n",
       " 0.001466567816891656,\n",
       " 0.0014644197024927012,\n",
       " 0.0014622778764091856,\n",
       " 0.0014601423035277062,\n",
       " 0.0014580129638877334,\n",
       " 0.0014558898278832575,\n",
       " 0.0014537728688688704,\n",
       " 0.001451662062863704,\n",
       " 0.001449557376643446,\n",
       " 0.0014474587803062144,\n",
       " 0.0014453662585818478,\n",
       " 0.0014432797744441369,\n",
       " 0.001441199306211025,\n",
       " 0.0014391248343686616,\n",
       " 0.0014370563180715163,\n",
       " 0.0014349937438416767,\n",
       " 0.001432937087907489,\n",
       " 0.001430886317232933,\n",
       " 0.0014288414137271224,\n",
       " 0.0014268023378251951,\n",
       " 0.00142476907720184,\n",
       " 0.0014227416091227442,\n",
       " 0.0014207199014854947,\n",
       " 0.0014187039370394516,\n",
       " 0.0014166936886175067,\n",
       " 0.0014146891201277307,\n",
       " 0.0014126902223704516,\n",
       " 0.0014106969637828998,\n",
       " 0.0014087093276001156,\n",
       " 0.0014067272870921022,\n",
       " 0.0014047508182018632,\n",
       " 0.0014027798876365693,\n",
       " 0.0014008144887157385,\n",
       " 0.0013988545921438727,\n",
       " 0.0013969001715550238,\n",
       " 0.0013949512032198383,\n",
       " 0.0013930076657818979,\n",
       " 0.0013910695400204264,\n",
       " 0.001389136796995193,\n",
       " 0.0013872094223015222,\n",
       " 0.0013852873913336736,\n",
       " 0.001383370670304949,\n",
       " 0.001381459252090384,\n",
       " 0.0013795531069944467,\n",
       " 0.0013776522199326902,\n",
       " 0.0013757565656875802,\n",
       " 0.0013738661215633363,\n",
       " 0.0013719808671337555,\n",
       " 0.001370100772373723,\n",
       " 0.0013682258335437005,\n",
       " 0.001366356012353506,\n",
       " 0.0013644912976250407,\n",
       " 0.0013626316676564835,\n",
       " 0.001360777102916195,\n",
       " 0.0013589275858257002,\n",
       " 0.0013570830889228394,\n",
       " 0.0013552435874916683,\n",
       " 0.0013534090709293697,\n",
       " 0.001351579518051876,\n",
       " 0.0013497548981519939,\n",
       " 0.0013479351952347817,\n",
       " 0.001346120394904792,\n",
       " 0.0013443104842061218,\n",
       " 0.0013425054281953946,\n",
       " 0.0013407052187066355,\n",
       " 0.0013389098251074026,\n",
       " 0.0013371192431115631,\n",
       " 0.0013353334455785326,\n",
       " 0.0013335524080817852,\n",
       " 0.0013317761202789801,\n",
       " 0.0013300045495789462,\n",
       " 0.0013282376899326933,\n",
       " 0.0013264755242545986,\n",
       " 0.0013247180255260692,\n",
       " 0.0013229651810719037,\n",
       " 0.0013212169678428888,\n",
       " 0.001319473365094732,\n",
       " 0.001317734365799103,\n",
       " 0.0013159999403474394,\n",
       " 0.001314270073733568,\n",
       " 0.0013125447524519323,\n",
       " 0.0013108239527059996,\n",
       " 0.0013091076647204225,\n",
       " 0.001307395868055856,\n",
       " 0.00130568854431689,\n",
       " 0.001303985665306123,\n",
       " 0.001302287228928958,\n",
       " 0.0013005932100171936,\n",
       " 0.0012989035975609802,\n",
       " 0.0012972183700099213,\n",
       " 0.0012955375079686602,\n",
       " 0.0012938609939813757,\n",
       " 0.0012921888123378286,\n",
       " 0.001290520948898804,\n",
       " 0.0012888573909390078,\n",
       " 0.0012871981153641438,\n",
       " 0.0012855431013893454,\n",
       " 0.001283892341949764,\n",
       " 0.0012822458190455048,\n",
       " 0.0012806035164766617,\n",
       " 0.0012789654196633197,\n",
       " 0.0012773315038420226,\n",
       " 0.0012757017583671928,\n",
       " 0.0012740761620162772,\n",
       " 0.0012724547073305113,\n",
       " 0.0012708373759554741,\n",
       " 0.001269224151372187,\n",
       " 0.001267615018713569,\n",
       " 0.0012660099645992475,\n",
       " 0.0012644089653453552,\n",
       " 0.001262812011277943,\n",
       " 0.0012612190936904571,\n",
       " 0.001259630181463935,\n",
       " 0.0012580452698743745,\n",
       " 0.0012564643430286412,\n",
       " 0.0012548873866229143,\n",
       " 0.0012533143877837556,\n",
       " 0.0012517453232835378,\n",
       " 0.0012501801838589292,\n",
       " 0.001248618961172931,\n",
       " 0.0012470616244391792,\n",
       " 0.0012455081692330115,\n",
       " 0.0012439585799306995,\n",
       " 0.0012424128424708874,\n",
       " 0.0012408709441983547,\n",
       " 0.0012393328737234038,\n",
       " 0.0012377986091537739,\n",
       " 0.001236268142427963,\n",
       " 0.0012347414546490867,\n",
       " 0.001233218528809949,\n",
       " 0.0012316993612456063,\n",
       " 0.001230183925374544,\n",
       " 0.0012286722205564647,\n",
       " 0.001227164222932035,\n",
       " 0.0012256599226693878,\n",
       " 0.0012241593109198422,\n",
       " 0.001222662368078054,\n",
       " 0.0012211690764992126,\n",
       " 0.0012196794319445214,\n",
       " 0.0012181934189575304,\n",
       " 0.0012167110236274352,\n",
       " 0.0012152322334345116,\n",
       " 0.0012137570254694761,\n",
       " 0.0012122854023970371,\n",
       " 0.0012108173433323687,\n",
       " 0.0012093528294791268,\n",
       " 0.0012078918555621344,\n",
       " 0.001206434416833732,\n",
       " 0.0012049804857379616,\n",
       " 0.0012035300607575257,\n",
       " 0.001202083117243792,\n",
       " 0.0012006396562960556,\n",
       " 0.0011991996556206177,\n",
       " 0.001197763106794682,\n",
       " 0.0011963299905961994,\n",
       " 0.0011949003013668575,\n",
       " 0.0011934740223726438,\n",
       " 0.0011920511501944786,\n",
       " 0.0011906316701136583,\n",
       " 0.0011892155572418171,\n",
       " 0.0011878028124624907,\n",
       " 0.0011863934249293285,\n",
       " 0.001184987373239083,\n",
       " 0.001183584649770364,\n",
       " 0.0011821852476639194,\n",
       " 0.0011807891491048912,\n",
       " 0.0011793963497014385,\n",
       " 0.001178006833859373,\n",
       " 0.0011766205875439247,\n",
       " 0.001175237598123801,\n",
       " 0.001173857854230838,\n",
       " 0.0011724813456336887,\n",
       " 0.001171108063124141,\n",
       " 0.0011697379984148034,\n",
       " 0.0011683711324054916,\n",
       " 0.0011670074595475722,\n",
       " 0.0011656469632057424,\n",
       " 0.0011642896400497625,\n",
       " 0.001162935475440884,\n",
       " 0.0011615844562042337,\n",
       " 0.0011602365704824252,\n",
       " 0.0011588918076038113,\n",
       " 0.0011575501696054415,\n",
       " 0.0011562116233960638,\n",
       " 0.0011548761741181478,\n",
       " 0.001153543803116852,\n",
       " 0.001152214505244351,\n",
       " 0.0011508882758676023,\n",
       " 0.0011495650875338029,\n",
       " 0.0011482449388184946,\n",
       " 0.001146927828439665,\n",
       " 0.0011456137319604386,\n",
       " 0.00114430263902916,\n",
       " 0.0011429945519708705,\n",
       " 0.0011416894495950441,\n",
       " 0.001140387324471739,\n",
       " 0.0011390881582724755,\n",
       " 0.001137791957784686,\n",
       " 0.0011364987058339955,\n",
       " 0.0011352083869634664,\n",
       " 0.0011339209989033876,\n",
       " 0.0011326365279694859,\n",
       " 0.0011313549618459163,\n",
       " 0.001130076289448418,\n",
       " 0.0011288005008011559,\n",
       " 0.0011275275985674113,\n",
       " 0.0011262575618610725,\n",
       " 0.0011249903835261713,\n",
       " 0.0011237260454808043,\n",
       " 0.0011224645547343224,\n",
       " 0.0011212058943120774,\n",
       " 0.001119950048936886,\n",
       " 0.0011186970165008151,\n",
       " 0.0011174467834651927,\n",
       " 0.0011161993376452143,\n",
       " 0.001114954679716088,\n",
       " 0.0011137127986439682,\n",
       " 0.001112473672856863,\n",
       " 0.001111237306223043,\n",
       " 0.0011100036905824198,\n",
       " 0.001108772806949382,\n",
       " 0.001107544649878411,\n",
       " 0.0011063192144685405,\n",
       " 0.0011050964846673684,\n",
       " 0.0011038764576692648,\n",
       " 0.0011026591193076315,\n",
       " 0.0011014444568325289,\n",
       " 0.0011002324704105443,\n",
       " 0.0010990231485500741,\n",
       " 0.0010978164809086748,\n",
       " 0.0010966124698196795,\n",
       " 0.0010954110941000235,\n",
       " 0.0010942123463264812,\n",
       " 0.0010930162198181487,\n",
       " 0.0010918227085622136,\n",
       " 0.0010906317955056116,\n",
       " 0.0010894434769421176,\n",
       " 0.001088257749536129,\n",
       " 0.0010870745986440707,\n",
       " 0.001085894022728258,\n",
       " 0.0010847160087632422,\n",
       " 0.001083540545026119,\n",
       " 0.0010823676209662755,\n",
       " 0.0010811972387296915,\n",
       " 0.001080029388606217,\n",
       " 0.0010788640618567167,\n",
       " 0.001077701238974437,\n",
       " 0.0010765409256863637,\n",
       " 0.00107538310386372,\n",
       " 0.0010742277688321387,\n",
       " 0.0010730749163846893,\n",
       " 0.0010719245427351345,\n",
       " 0.0010707766328343286,\n",
       " 0.0010696311731380403,\n",
       " 0.0010684881630979937,\n",
       " 0.0010673475905792083,\n",
       " 0.0010662094563017338,\n",
       " 0.0010650737376305505,\n",
       " 0.0010639404374772053,\n",
       " 0.0010628095468205579,\n",
       " 0.001061681057541582,\n",
       " 0.0010605549739746867,\n",
       " 0.001059431265096243,\n",
       " 0.0010583099379095815,\n",
       " 0.0010571909870761677,\n",
       " 0.0010560743961497883,\n",
       " 0.0010549601619703835,\n",
       " 0.0010538482816938996,\n",
       " 0.0010527387411191557,\n",
       " 0.001051631539106622,\n",
       " 0.0010505266629891885,\n",
       " 0.0010494241013664567,\n",
       " 0.0010483238556196,\n",
       " 0.0010472259153501426,\n",
       " 0.001046130271199456,\n",
       " 0.0010450369147447746,\n",
       " 0.0010439458384056092,\n",
       " 0.0010428570353595197,\n",
       " 0.0010417705111078423,\n",
       " 0.001040686247318715,\n",
       " 0.0010396042391349943,\n",
       " 0.0010385244705437189,\n",
       " 0.0010374469504162876,\n",
       " 0.0010363716634538955,\n",
       " 0.001035298595887618,\n",
       " 0.001034227746966955,\n",
       " 0.0010331591160164567,\n",
       " 0.001032092690786686,\n",
       " 0.0010310284718946813,\n",
       " 0.0010299664366127132,\n",
       " 0.0010289065877688896,\n",
       " 0.0010278489279085075,\n",
       " 0.0010267934360392696,\n",
       " 0.0010257401049096411,\n",
       " 0.0010246889396347725,\n",
       " 0.0010236399215352349,\n",
       " 0.0010225930570826065,\n",
       " 0.0010215483288182436,\n",
       " 0.0010205057326708989,\n",
       " 0.0010194652649764499,\n",
       " 0.0010184269107956542,\n",
       " 0.0010173906683247256,\n",
       " 0.001016356535940257,\n",
       " 0.001015324500539649,\n",
       " 0.0010142945619720952,\n",
       " 0.0010132667084603371,\n",
       " 0.0010122409410463747,\n",
       " 0.0010112172490264752,\n",
       " 0.0010101956227672798,\n",
       " 0.001009176065240297]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(1000, dtype=torch.float32)\n",
    "x += 1\n",
    "y = 1 / x\n",
    "y = y.tolist()\n",
    "ma = []\n",
    "for i in y:\n",
    "    if ma:\n",
    "        ma.append(ma[-1]*0.9+i*0.1)\n",
    "    else :\n",
    "        ma.append(i)\n",
    "ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q学习算法实现**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体在训练中只做两件事：\n",
    "- 选择动作\n",
    "- 更新策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, cfg):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.n\n",
    "        self.action_size = env.action_space.n\n",
    "        self.Q = np.random.rand(self.state_size, self.action_size)\n",
    "        self.gamma = cfg.gamma\n",
    "        self.alpha = cfg.alpha\n",
    "        self.epsilon_min = cfg.epsilon_min\n",
    "        self.epsilon_max = cfg.epsilon_max\n",
    "        self.epsilon = cfg.epsilon\n",
    "        self.sample_count = 0\n",
    "        self.epsilon_decay = cfg.epsilon_decay\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        self.sample_count += 1\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-1. * self.sample_count / self.epsilon_decay) # decrease epsilon\n",
    "        # epsilon-greedy strategy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state, :])\n",
    "        \n",
    "    def qlearn(self, state, action, reward, next_state, done):\n",
    "        q_now = self.Q[state, action]\n",
    "        if done:\n",
    "            q_next = reward\n",
    "        else:\n",
    "            q_next = reward + self.gamma * np.max(self.Q[next_state, :])\n",
    "        self.Q[state, action] += self.alpha * (q_next - q_now)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.seed = 1 # 随机种子\n",
    "        self.epsilon = 0.95 #  e-greedy策略中epsilon的初始值\n",
    "        self.epsilon_max = 0.95 #  e-greedy策略中epsilon的初始值\n",
    "        self.epsilon_min = 0.01 #  e-greedy策略中epsilon的最终值\n",
    "        self.epsilon_decay = 300 #  e-greedy策略中epsilon的衰减率\n",
    "        self.gamma = 0.9 # 折扣因子\n",
    "        self.alpha = 0.1 # 学习率\n",
    "\n",
    "cfg = Config()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env,cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, reward: -3386, epsilon: 0.17888418913003543, ma: -3386\n",
      "Episode: 1, reward: -206, epsilon: 0.1282199109253413, ma: -3068.0\n",
      "Episode: 2, reward: -377, epsilon: 0.07509709179078522, ma: -2798.9\n",
      "Episode: 3, reward: -113, epsilon: 0.05466602781795325, ma: -2530.3100000000004\n",
      "Episode: 4, reward: -157, epsilon: 0.036466502459482096, ma: -2292.9790000000003\n",
      "Episode: 5, reward: -255, epsilon: 0.025734879545065793, ma: -2089.1811000000002\n",
      "Episode: 6, reward: -316, epsilon: 0.017633500658128767, ma: -1911.86299\n",
      "Episode: 7, reward: -339, epsilon: 0.012465874575496746, ma: -1754.5766910000002\n",
      "Episode: 8, reward: -79, epsilon: 0.011894989331299403, ma: -1587.0190219000003\n",
      "Episode: 9, reward: -203, epsilon: 0.010963239247691907, ma: -1448.6171197100002\n",
      "Episode: 10, reward: -301, epsilon: 0.010491257528167241, ma: -1333.8554077390002\n",
      "Episode: 11, reward: -183, epsilon: 0.01026692520486906, ma: -1218.7698669651002\n",
      "Episode: 12, reward: -202, epsilon: 0.010136133381881726, ma: -1117.0928802685903\n",
      "Episode: 13, reward: -55, epsilon: 0.010113329762479608, ma: -1010.8835922417313\n",
      "Episode: 14, reward: -284, epsilon: 0.01006116867146269, ma: -938.1952330175582\n",
      "Episode: 15, reward: -134, epsilon: 0.010039133093401875, ma: -857.7757097158023\n",
      "Episode: 16, reward: -167, epsilon: 0.010022427812562169, ma: -788.6981387442222\n",
      "Episode: 17, reward: -111, epsilon: 0.01001549166009779, ma: -720.9283248698\n",
      "Episode: 18, reward: -268, epsilon: 0.010008819529179454, ma: -675.63549238282\n",
      "Episode: 19, reward: -60, epsilon: 0.010007220819766887, ma: -614.071943144538\n",
      "Episode: 20, reward: -187, epsilon: 0.010003871473385084, ma: -571.3647488300842\n",
      "Episode: 21, reward: -125, epsilon: 0.010002552232554187, ma: -526.7282739470758\n",
      "Episode: 22, reward: -87, epsilon: 0.010001909742636286, ma: -482.7554465523682\n",
      "Episode: 23, reward: -118, epsilon: 0.0100012887015412, ma: -446.2799018971314\n",
      "Episode: 24, reward: -96, epsilon: 0.01000093578938322, ma: -411.2519117074183\n",
      "Episode: 25, reward: -90, epsilon: 0.010000693249825809, ma: -379.1267205366765\n",
      "Episode: 26, reward: -117, epsilon: 0.010000469369560309, ma: -352.91404848300886\n",
      "Episode: 27, reward: -108, epsilon: 0.010000327468030406, ma: -328.422643634708\n",
      "Episode: 28, reward: -112, epsilon: 0.010000225440687986, ma: -306.78037927123717\n",
      "Episode: 29, reward: -99, epsilon: 0.010000162074661074, ma: -286.00234134411346\n",
      "Episode: 30, reward: -96, epsilon: 0.010000117690359073, ma: -267.00210720970216\n",
      "Episode: 31, reward: -61, epsilon: 0.010000096036061982, ma: -246.40189648873195\n",
      "Episode: 32, reward: -75, epsilon: 0.010000074792960273, ma: -229.26170683985876\n",
      "Episode: 33, reward: -81, epsilon: 0.010000057095412193, ma: -214.43553615587288\n",
      "Episode: 34, reward: -49, epsilon: 0.01000004849159254, ma: -197.8919825402856\n",
      "Episode: 35, reward: -140, epsilon: 0.01000003040854841, ma: -192.10278428625705\n",
      "Episode: 36, reward: -87, epsilon: 0.010000022753608917, ma: -181.59250585763132\n",
      "Episode: 37, reward: -57, epsilon: 0.010000018816304725, ma: -169.1332552718682\n",
      "Episode: 38, reward: -122, epsilon: 0.010000012529139606, ma: -164.41992974468135\n",
      "Episode: 39, reward: -50, epsilon: 0.010000010605687707, ma: -152.97793677021323\n",
      "Episode: 40, reward: -85, epsilon: 0.010000007988932129, ma: -146.1801430931919\n",
      "Episode: 41, reward: -45, epsilon: 0.010000006876137607, ma: -136.0621287838727\n",
      "Episode: 42, reward: -109, epsilon: 0.010000004781353983, ma: -133.35591590548543\n",
      "Episode: 43, reward: -29, epsilon: 0.010000004340793215, ma: -122.92032431493689\n",
      "Episode: 44, reward: -83, epsilon: 0.010000003291654853, ma: -118.92829188344321\n",
      "Episode: 45, reward: -72, epsilon: 0.010000002589307416, ma: -114.23546269509889\n",
      "Episode: 46, reward: -78, epsilon: 0.01000000199648959, ma: -110.611916425589\n",
      "Episode: 47, reward: -61, epsilon: 0.010000001629147872, ma: -105.6507247830301\n",
      "Episode: 48, reward: -67, epsilon: 0.010000001303070974, ma: -101.7856523047271\n",
      "Episode: 49, reward: -79, epsilon: 0.0100000010013914, ma: -99.50708707425439\n",
      "Episode: 50, reward: -57, epsilon: 0.010000000828109765, ma: -95.25637836682895\n",
      "Episode: 51, reward: -59, epsilon: 0.010000000680262699, ma: -91.63074053014607\n",
      "Episode: 52, reward: -58, epsilon: 0.01000000056067741, ma: -88.26766647713146\n",
      "Episode: 53, reward: -74, epsilon: 0.010000000438113955, ma: -86.84089982941832\n",
      "Episode: 54, reward: -62, epsilon: 0.010000000356314005, ma: -84.3568098464765\n",
      "Episode: 55, reward: -73, epsilon: 0.01000000027935379, ma: -83.22112886182884\n",
      "Episode: 56, reward: -49, epsilon: 0.01000000023725742, ma: -79.79901597564597\n",
      "Episode: 57, reward: -75, epsilon: 0.010000000184776264, ma: -79.31911437808138\n",
      "Episode: 58, reward: -49, epsilon: 0.010000000156931968, ma: -76.28720294027325\n",
      "Episode: 59, reward: -129, epsilon: 0.010000000141997915, ma: -81.55848264624593\n",
      "Episode: 60, reward: -71, epsilon: 0.01000000011207247, ma: -80.50263438162133\n",
      "Episode: 61, reward: -70, epsilon: 0.010000000088749019, ma: -79.4523709434592\n",
      "Episode: 62, reward: -57, epsilon: 0.010000000073391812, ma: -77.20713384911329\n",
      "Episode: 63, reward: -62, epsilon: 0.010000000059688879, ma: -75.68642046420197\n",
      "Episode: 64, reward: -31, epsilon: 0.010000000053829002, ma: -71.21777841778177\n",
      "Episode: 65, reward: -38, epsilon: 0.01000000004742482, ma: -67.8960005760036\n",
      "Episode: 66, reward: -167, epsilon: 0.010000000037806425, ma: -77.80640051840324\n",
      "Episode: 67, reward: -71, epsilon: 0.010000000029838885, ma: -77.12576046656291\n",
      "Episode: 68, reward: -45, epsilon: 0.010000000025682566, ma: -73.91318441990661\n",
      "Episode: 69, reward: -35, epsilon: 0.010000000022854448, ma: -70.02186597791595\n",
      "Episode: 70, reward: -44, epsilon: 0.010000000019736685, ma: -67.41967938012436\n",
      "Episode: 71, reward: -47, epsilon: 0.010000000016874648, ma: -65.37771144211193\n",
      "Episode: 72, reward: -67, epsilon: 0.010000000013497157, ma: -65.53994029790074\n",
      "Episode: 73, reward: -50, epsilon: 0.010000000011425097, ma: -63.985946268110666\n",
      "Episode: 74, reward: -60, epsilon: 0.010000000009354078, ma: -63.5873516412996\n",
      "Episode: 75, reward: -38, epsilon: 0.010000000008241198, ma: -61.028616477169635\n",
      "Episode: 76, reward: -25, epsilon: 0.010000000007582268, ma: -57.42575482945267\n",
      "Episode: 77, reward: -67, epsilon: 0.010000000006064663, ma: -58.38317934650741\n",
      "Episode: 78, reward: -51, epsilon: 0.010000000005116543, ma: -57.644861411856674\n",
      "Episode: 79, reward: -62, epsilon: 0.010000000004161238, ma: -58.08037527067101\n",
      "Episode: 80, reward: -35, epsilon: 0.010000000003703009, ma: -55.77233774360391\n",
      "Episode: 81, reward: -41, epsilon: 0.01000000000322999, ma: -54.29510396924353\n",
      "Episode: 82, reward: -67, epsilon: 0.010000000002583501, ma: -55.56559357231918\n",
      "Episode: 83, reward: -40, epsilon: 0.010000000002261013, ma: -54.00903421508726\n",
      "Episode: 84, reward: -58, epsilon: 0.010000000001863543, ma: -54.408130793578536\n",
      "Episode: 85, reward: -31, epsilon: 0.010000000001680592, ma: -52.067317714220685\n",
      "Episode: 86, reward: -23, epsilon: 0.01000000000155656, ma: -49.16058594279861\n",
      "Episode: 87, reward: -60, epsilon: 0.010000000001274404, ma: -50.24452734851875\n",
      "Episode: 88, reward: -176, epsilon: 0.010000000000985913, ma: -62.820074613666876\n",
      "Episode: 89, reward: -30, epsilon: 0.01000000000089209, ma: -59.53806715230019\n",
      "Episode: 90, reward: -42, epsilon: 0.010000000000775547, ma: -57.784260437070174\n",
      "Episode: 91, reward: -49, epsilon: 0.010000000000658678, ma: -56.90583439336316\n",
      "Episode: 92, reward: -73, epsilon: 0.01000000000051641, ma: -58.51525095402684\n",
      "Episode: 93, reward: -30, epsilon: 0.010000000000467267, ma: -55.663725858624154\n",
      "Episode: 94, reward: -125, epsilon: 0.010000000000428475, ma: -62.59735327276174\n",
      "Episode: 95, reward: -38, epsilon: 0.010000000000377499, ma: -60.13761794548556\n",
      "Episode: 96, reward: -60, epsilon: 0.01000000000030907, ma: -60.123856150937\n",
      "Episode: 97, reward: -27, epsilon: 0.010000000000282469, ma: -56.811470535843306\n",
      "Episode: 98, reward: -32, epsilon: 0.01000000000025389, ma: -54.330323482258976\n",
      "Episode: 99, reward: -42, epsilon: 0.010000000000220721, ma: -53.097291134033085\n",
      "Episode: 100, reward: -58, epsilon: 0.01000000000018192, ma: -53.587562020629775\n",
      "Episode: 101, reward: -18, epsilon: 0.010000000000171327, ma: -50.028805818566795\n",
      "Episode: 102, reward: -35, epsilon: 0.01000000000015246, ma: -48.525925236710115\n",
      "Episode: 103, reward: -38, epsilon: 0.010000000000134322, ma: -47.4733327130391\n",
      "Episode: 104, reward: -40, epsilon: 0.010000000000117554, ma: -46.72599944173519\n",
      "Episode: 105, reward: -27, epsilon: 0.010000000000107437, ma: -44.753399497561674\n",
      "Episode: 106, reward: -49, epsilon: 0.010000000000091247, ma: -45.1780595478055\n",
      "Episode: 107, reward: -29, epsilon: 0.01000000000008284, ma: -43.56025359302495\n",
      "Episode: 108, reward: -31, epsilon: 0.010000000000074706, ma: -42.30422823372246\n",
      "Episode: 109, reward: -75, epsilon: 0.010000000000058181, ma: -45.57380541035022\n",
      "Episode: 110, reward: -23, epsilon: 0.010000000000053888, ma: -43.31642486931519\n",
      "Episode: 111, reward: -31, epsilon: 0.010000000000048597, ma: -42.08478238238367\n",
      "Episode: 112, reward: -29, epsilon: 0.01000000000004412, ma: -40.77630414414531\n",
      "Episode: 113, reward: -43, epsilon: 0.010000000000038228, ma: -40.998673729730776\n",
      "Episode: 114, reward: -166, epsilon: 0.010000000000030576, ma: -53.4988063567577\n",
      "Episode: 115, reward: -18, epsilon: 0.010000000000028797, ma: -49.94892572108193\n",
      "Episode: 116, reward: -24, epsilon: 0.010000000000026581, ma: -47.354033148973734\n",
      "Episode: 117, reward: -54, epsilon: 0.010000000000022203, ma: -48.01862983407636\n",
      "Episode: 118, reward: -30, epsilon: 0.01000000000002009, ma: -46.21676685066872\n",
      "Episode: 119, reward: -33, epsilon: 0.010000000000017998, ma: -44.89509016560184\n",
      "Episode: 120, reward: -40, epsilon: 0.010000000000015751, ma: -44.40558114904166\n",
      "Episode: 121, reward: -35, epsilon: 0.010000000000014017, ma: -43.465023034137495\n",
      "Episode: 122, reward: -28, epsilon: 0.010000000000012768, ma: -41.91852073072374\n",
      "Episode: 123, reward: -62, epsilon: 0.010000000000010384, ma: -43.92666865765137\n",
      "Episode: 124, reward: -15, epsilon: 0.010000000000009878, ma: -41.03400179188623\n",
      "Episode: 125, reward: -26, epsilon: 0.010000000000009057, ma: -39.53060161269761\n",
      "Episode: 126, reward: -22, epsilon: 0.010000000000008417, ma: -37.77754145142785\n",
      "Episode: 127, reward: -41, epsilon: 0.010000000000007342, ma: -38.099787306285066\n",
      "Episode: 128, reward: -38, epsilon: 0.010000000000006469, ma: -38.089808575656555\n",
      "Episode: 129, reward: -120, epsilon: 0.010000000000006032, ma: -46.2808277180909\n",
      "Episode: 130, reward: -55, epsilon: 0.01000000000000502, ma: -47.15274494628181\n",
      "Episode: 131, reward: -21, epsilon: 0.010000000000004682, ma: -44.537470451653626\n",
      "Episode: 132, reward: -34, epsilon: 0.01000000000000418, ma: -43.48372340648826\n",
      "Episode: 133, reward: -35, epsilon: 0.01000000000000372, ma: -42.63535106583944\n",
      "Episode: 134, reward: -16, epsilon: 0.010000000000003527, ma: -39.971815959255494\n",
      "Episode: 135, reward: -27, epsilon: 0.010000000000003223, ma: -38.67463436332995\n",
      "Episode: 136, reward: -54, epsilon: 0.010000000000002692, ma: -40.20717092699695\n",
      "Episode: 137, reward: -31, epsilon: 0.010000000000002427, ma: -39.28645383429726\n",
      "Episode: 138, reward: -19, epsilon: 0.01000000000000228, ma: -37.257808450867536\n",
      "Episode: 139, reward: -31, epsilon: 0.010000000000002056, ma: -36.632027605780785\n",
      "Episode: 140, reward: -29, epsilon: 0.010000000000001865, ma: -35.8688248452027\n",
      "Episode: 141, reward: -22, epsilon: 0.010000000000001733, ma: -34.48194236068244\n",
      "Episode: 142, reward: -41, epsilon: 0.010000000000001513, ma: -35.13374812461419\n",
      "Episode: 143, reward: -34, epsilon: 0.01000000000000135, ma: -35.02037331215278\n",
      "Episode: 144, reward: -30, epsilon: 0.010000000000001221, ma: -34.5183359809375\n",
      "Episode: 145, reward: -34, epsilon: 0.010000000000001091, ma: -34.46650238284375\n",
      "Episode: 146, reward: -21, epsilon: 0.010000000000001017, ma: -33.119852144559374\n",
      "Episode: 147, reward: -22, epsilon: 0.010000000000000946, ma: -32.00786693010344\n",
      "Episode: 148, reward: -31, epsilon: 0.010000000000000852, ma: -31.907080237093098\n",
      "Episode: 149, reward: -30, epsilon: 0.010000000000000772, ma: -31.71637221338379\n",
      "Episode: 150, reward: -17, epsilon: 0.010000000000000729, ma: -30.24473499204541\n",
      "Episode: 151, reward: -51, epsilon: 0.010000000000000614, ma: -32.32026149284087\n",
      "Episode: 152, reward: -26, epsilon: 0.010000000000000564, ma: -31.688235343556787\n",
      "Episode: 153, reward: -17, epsilon: 0.010000000000000533, ma: -30.21941180920111\n",
      "Episode: 154, reward: -38, epsilon: 0.01000000000000047, ma: -30.997470628281\n",
      "Episode: 155, reward: -27, epsilon: 0.010000000000000429, ma: -30.5977235654529\n",
      "Episode: 156, reward: -31, epsilon: 0.010000000000000387, ma: -30.637951208907612\n",
      "Episode: 157, reward: -16, epsilon: 0.010000000000000366, ma: -29.174156088016854\n",
      "Episode: 158, reward: -24, epsilon: 0.010000000000000338, ma: -28.656740479215166\n",
      "Episode: 159, reward: -32, epsilon: 0.010000000000000304, ma: -28.99106643129365\n",
      "Episode: 160, reward: -23, epsilon: 0.010000000000000281, ma: -28.391959788164286\n",
      "Episode: 161, reward: -32, epsilon: 0.010000000000000253, ma: -28.752763809347858\n",
      "Episode: 162, reward: -30, epsilon: 0.01000000000000023, ma: -28.877487428413072\n",
      "Episode: 163, reward: -16, epsilon: 0.010000000000000217, ma: -27.589738685571767\n",
      "Episode: 164, reward: -34, epsilon: 0.010000000000000194, ma: -28.230764817014588\n",
      "Episode: 165, reward: -23, epsilon: 0.01000000000000018, ma: -27.70768833531313\n",
      "Episode: 166, reward: -34, epsilon: 0.01000000000000016, ma: -28.336919501781814\n",
      "Episode: 167, reward: -24, epsilon: 0.010000000000000148, ma: -27.903227551603635\n",
      "Episode: 168, reward: -21, epsilon: 0.010000000000000139, ma: -27.212904796443272\n",
      "Episode: 169, reward: -22, epsilon: 0.010000000000000129, ma: -26.691614316798944\n",
      "Episode: 170, reward: -15, epsilon: 0.010000000000000122, ma: -25.52245288511905\n",
      "Episode: 171, reward: -35, epsilon: 0.01000000000000011, ma: -26.470207596607146\n",
      "Episode: 172, reward: -24, epsilon: 0.0100000000000001, ma: -26.223186836946432\n",
      "Episode: 173, reward: -29, epsilon: 0.01000000000000009, ma: -26.500868153251787\n",
      "Episode: 174, reward: -39, epsilon: 0.01000000000000008, ma: -27.75078133792661\n",
      "Episode: 175, reward: -37, epsilon: 0.010000000000000071, ma: -28.67570320413395\n",
      "Episode: 176, reward: -21, epsilon: 0.010000000000000066, ma: -27.908132883720555\n",
      "Episode: 177, reward: -17, epsilon: 0.010000000000000063, ma: -26.8173195953485\n",
      "Episode: 178, reward: -29, epsilon: 0.010000000000000057, ma: -27.03558763581365\n",
      "Episode: 179, reward: -19, epsilon: 0.010000000000000054, ma: -26.232028872232284\n",
      "Episode: 180, reward: -25, epsilon: 0.010000000000000049, ma: -26.108825985009055\n",
      "Episode: 181, reward: -21, epsilon: 0.010000000000000045, ma: -25.597943386508152\n",
      "Episode: 182, reward: -25, epsilon: 0.010000000000000042, ma: -25.53814904785734\n",
      "Episode: 183, reward: -25, epsilon: 0.010000000000000038, ma: -25.484334143071607\n",
      "Episode: 184, reward: -29, epsilon: 0.010000000000000035, ma: -25.83590072876445\n",
      "Episode: 185, reward: -22, epsilon: 0.010000000000000033, ma: -25.452310655888006\n",
      "Episode: 186, reward: -18, epsilon: 0.010000000000000031, ma: -24.707079590299205\n",
      "Episode: 187, reward: -17, epsilon: 0.01000000000000003, ma: -23.936371631269285\n",
      "Episode: 188, reward: -24, epsilon: 0.010000000000000026, ma: -23.942734468142355\n",
      "Episode: 189, reward: -34, epsilon: 0.010000000000000024, ma: -24.94846102132812\n",
      "Episode: 190, reward: -35, epsilon: 0.010000000000000021, ma: -25.95361491919531\n",
      "Episode: 191, reward: -17, epsilon: 0.010000000000000021, ma: -25.05825342727578\n",
      "Episode: 192, reward: -28, epsilon: 0.01000000000000002, ma: -25.352428084548205\n",
      "Episode: 193, reward: -14, epsilon: 0.010000000000000018, ma: -24.21718527609338\n",
      "Episode: 194, reward: -18, epsilon: 0.010000000000000016, ma: -23.595466748484046\n",
      "Episode: 195, reward: -27, epsilon: 0.010000000000000016, ma: -23.935920073635643\n",
      "Episode: 196, reward: -28, epsilon: 0.010000000000000014, ma: -24.34232806627208\n",
      "Episode: 197, reward: -17, epsilon: 0.010000000000000012, ma: -23.608095259644873\n",
      "Episode: 198, reward: -16, epsilon: 0.010000000000000012, ma: -22.847285733680387\n",
      "Episode: 199, reward: -32, epsilon: 0.01000000000000001, ma: -23.762557160312348\n",
      "Episode: 200, reward: -134, epsilon: 0.01000000000000001, ma: -34.786301444281115\n",
      "Episode: 201, reward: -17, epsilon: 0.010000000000000009, ma: -33.00767129985301\n",
      "Episode: 202, reward: -16, epsilon: 0.010000000000000009, ma: -31.30690416986771\n",
      "Episode: 203, reward: -15, epsilon: 0.010000000000000009, ma: -29.67621375288094\n",
      "Episode: 204, reward: -19, epsilon: 0.010000000000000009, ma: -28.608592377592846\n",
      "Episode: 205, reward: -25, epsilon: 0.010000000000000007, ma: -28.24773313983356\n",
      "Episode: 206, reward: -14, epsilon: 0.010000000000000007, ma: -26.8229598258502\n",
      "Episode: 207, reward: -20, epsilon: 0.010000000000000007, ma: -26.14066384326518\n",
      "Episode: 208, reward: -31, epsilon: 0.010000000000000005, ma: -26.626597458938665\n",
      "Episode: 209, reward: -156, epsilon: 0.010000000000000005, ma: -39.5639377130448\n",
      "Episode: 210, reward: -15, epsilon: 0.010000000000000005, ma: -37.107543941740325\n",
      "Episode: 211, reward: -30, epsilon: 0.010000000000000004, ma: -36.39678954756629\n",
      "Episode: 212, reward: -25, epsilon: 0.010000000000000004, ma: -35.25711059280967\n",
      "Episode: 213, reward: -21, epsilon: 0.010000000000000004, ma: -33.8313995335287\n",
      "Episode: 214, reward: -16, epsilon: 0.010000000000000004, ma: -32.04825958017583\n",
      "Episode: 215, reward: -24, epsilon: 0.010000000000000004, ma: -31.24343362215825\n",
      "Episode: 216, reward: -18, epsilon: 0.010000000000000004, ma: -29.919090259942426\n",
      "Episode: 217, reward: -141, epsilon: 0.010000000000000002, ma: -41.027181233948184\n",
      "Episode: 218, reward: -18, epsilon: 0.010000000000000002, ma: -38.724463110553366\n",
      "Episode: 219, reward: -17, epsilon: 0.010000000000000002, ma: -36.552016799498034\n",
      "Episode: 220, reward: -16, epsilon: 0.010000000000000002, ma: -34.49681511954823\n",
      "Episode: 221, reward: -14, epsilon: 0.010000000000000002, ma: -32.44713360759341\n",
      "Episode: 222, reward: -23, epsilon: 0.010000000000000002, ma: -31.502420246834067\n",
      "Episode: 223, reward: -29, epsilon: 0.010000000000000002, ma: -31.25217822215066\n",
      "Episode: 224, reward: -20, epsilon: 0.010000000000000002, ma: -30.126960399935594\n",
      "Episode: 225, reward: -27, epsilon: 0.010000000000000002, ma: -29.814264359942033\n",
      "Episode: 226, reward: -15, epsilon: 0.010000000000000002, ma: -28.33283792394783\n",
      "Episode: 227, reward: -31, epsilon: 0.010000000000000002, ma: -28.59955413155305\n",
      "Episode: 228, reward: -15, epsilon: 0.010000000000000002, ma: -27.239598718397744\n",
      "Episode: 229, reward: -13, epsilon: 0.010000000000000002, ma: -25.81563884655797\n",
      "Episode: 230, reward: -126, epsilon: 0.010000000000000002, ma: -35.834074961902175\n",
      "Episode: 231, reward: -19, epsilon: 0.010000000000000002, ma: -34.150667465711955\n",
      "Episode: 232, reward: -25, epsilon: 0.010000000000000002, ma: -33.23560071914076\n",
      "Episode: 233, reward: -17, epsilon: 0.01, ma: -31.612040647226685\n",
      "Episode: 234, reward: -15, epsilon: 0.01, ma: -29.950836582504017\n",
      "Episode: 235, reward: -13, epsilon: 0.01, ma: -28.255752924253617\n",
      "Episode: 236, reward: -16, epsilon: 0.01, ma: -27.030177631828256\n",
      "Episode: 237, reward: -23, epsilon: 0.01, ma: -26.627159868645432\n",
      "Episode: 238, reward: -34, epsilon: 0.01, ma: -27.364443881780886\n",
      "Episode: 239, reward: -18, epsilon: 0.01, ma: -26.427999493602798\n",
      "Episode: 240, reward: -16, epsilon: 0.01, ma: -25.38519954424252\n",
      "Episode: 241, reward: -13, epsilon: 0.01, ma: -24.14667958981827\n",
      "Episode: 242, reward: -19, epsilon: 0.01, ma: -23.63201163083644\n",
      "Episode: 243, reward: -22, epsilon: 0.01, ma: -23.468810467752796\n",
      "Episode: 244, reward: -24, epsilon: 0.01, ma: -23.521929420977514\n",
      "Episode: 245, reward: -16, epsilon: 0.01, ma: -22.769736478879764\n",
      "Episode: 246, reward: -13, epsilon: 0.01, ma: -21.79276283099179\n",
      "Episode: 247, reward: -18, epsilon: 0.01, ma: -21.413486547892614\n",
      "Episode: 248, reward: -19, epsilon: 0.01, ma: -21.17213789310335\n",
      "Episode: 249, reward: -16, epsilon: 0.01, ma: -20.654924103793018\n",
      "Episode: 250, reward: -23, epsilon: 0.01, ma: -20.889431693413716\n",
      "Episode: 251, reward: -126, epsilon: 0.01, ma: -31.400488524072347\n",
      "Episode: 252, reward: -16, epsilon: 0.01, ma: -29.860439671665116\n",
      "Episode: 253, reward: -24, epsilon: 0.01, ma: -29.274395704498602\n",
      "Episode: 254, reward: -29, epsilon: 0.01, ma: -29.246956134048745\n",
      "Episode: 255, reward: -13, epsilon: 0.01, ma: -27.62226052064387\n",
      "Episode: 256, reward: -14, epsilon: 0.01, ma: -26.260034468579484\n",
      "Episode: 257, reward: -22, epsilon: 0.01, ma: -25.834031021721536\n",
      "Episode: 258, reward: -127, epsilon: 0.01, ma: -35.95062791954938\n",
      "Episode: 259, reward: -21, epsilon: 0.01, ma: -34.45556512759445\n",
      "Episode: 260, reward: -18, epsilon: 0.01, ma: -32.810008614835\n",
      "Episode: 261, reward: -14, epsilon: 0.01, ma: -30.9290077533515\n",
      "Episode: 262, reward: -20, epsilon: 0.01, ma: -29.83610697801635\n",
      "Episode: 263, reward: -14, epsilon: 0.01, ma: -28.252496280214714\n",
      "Episode: 264, reward: -15, epsilon: 0.01, ma: -26.927246652193244\n",
      "Episode: 265, reward: -17, epsilon: 0.01, ma: -25.93452198697392\n",
      "Episode: 266, reward: -23, epsilon: 0.01, ma: -25.64106978827653\n",
      "Episode: 267, reward: -13, epsilon: 0.01, ma: -24.376962809448877\n",
      "Episode: 268, reward: -13, epsilon: 0.01, ma: -23.23926652850399\n",
      "Episode: 269, reward: -21, epsilon: 0.01, ma: -23.01533987565359\n",
      "Episode: 270, reward: -13, epsilon: 0.01, ma: -22.013805888088232\n",
      "Episode: 271, reward: -15, epsilon: 0.01, ma: -21.312425299279408\n",
      "Episode: 272, reward: -31, epsilon: 0.01, ma: -22.281182769351467\n",
      "Episode: 273, reward: -13, epsilon: 0.01, ma: -21.353064492416323\n",
      "Episode: 274, reward: -13, epsilon: 0.01, ma: -20.51775804317469\n",
      "Episode: 275, reward: -21, epsilon: 0.01, ma: -20.565982238857224\n",
      "Episode: 276, reward: -14, epsilon: 0.01, ma: -19.9093840149715\n",
      "Episode: 277, reward: -13, epsilon: 0.01, ma: -19.21844561347435\n",
      "Episode: 278, reward: -13, epsilon: 0.01, ma: -18.596601052126918\n",
      "Episode: 279, reward: -26, epsilon: 0.01, ma: -19.33694094691423\n",
      "Episode: 280, reward: -13, epsilon: 0.01, ma: -18.703246852222808\n",
      "Episode: 281, reward: -13, epsilon: 0.01, ma: -18.13292216700053\n",
      "Episode: 282, reward: -14, epsilon: 0.01, ma: -17.719629950300476\n",
      "Episode: 283, reward: -25, epsilon: 0.01, ma: -18.447666955270428\n",
      "Episode: 284, reward: -17, epsilon: 0.01, ma: -18.302900259743385\n",
      "Episode: 285, reward: -13, epsilon: 0.01, ma: -17.772610233769047\n",
      "Episode: 286, reward: -18, epsilon: 0.01, ma: -17.795349210392143\n",
      "Episode: 287, reward: -18, epsilon: 0.01, ma: -17.81581428935293\n",
      "Episode: 288, reward: -13, epsilon: 0.01, ma: -17.334232860417636\n",
      "Episode: 289, reward: -15, epsilon: 0.01, ma: -17.100809574375873\n",
      "Episode: 290, reward: -13, epsilon: 0.01, ma: -16.690728616938287\n",
      "Episode: 291, reward: -15, epsilon: 0.01, ma: -16.52165575524446\n",
      "Episode: 292, reward: -15, epsilon: 0.01, ma: -16.369490179720014\n",
      "Episode: 293, reward: -16, epsilon: 0.01, ma: -16.332541161748015\n",
      "Episode: 294, reward: -13, epsilon: 0.01, ma: -15.999287045573215\n",
      "Episode: 295, reward: -15, epsilon: 0.01, ma: -15.899358341015894\n",
      "Episode: 296, reward: -22, epsilon: 0.01, ma: -16.509422506914305\n",
      "Episode: 297, reward: -13, epsilon: 0.01, ma: -16.158480256222873\n",
      "Episode: 298, reward: -14, epsilon: 0.01, ma: -15.942632230600587\n",
      "Episode: 299, reward: -14, epsilon: 0.01, ma: -15.748369007540528\n",
      "Episode: 300, reward: -15, epsilon: 0.01, ma: -15.673532106786476\n",
      "Episode: 301, reward: -13, epsilon: 0.01, ma: -15.40617889610783\n",
      "Episode: 302, reward: -17, epsilon: 0.01, ma: -15.565561006497049\n",
      "Episode: 303, reward: -13, epsilon: 0.01, ma: -15.309004905847345\n",
      "Episode: 304, reward: -13, epsilon: 0.01, ma: -15.078104415262612\n",
      "Episode: 305, reward: -13, epsilon: 0.01, ma: -14.870293973736352\n",
      "Episode: 306, reward: -13, epsilon: 0.01, ma: -14.683264576362719\n",
      "Episode: 307, reward: -13, epsilon: 0.01, ma: -14.514938118726448\n",
      "Episode: 308, reward: -14, epsilon: 0.01, ma: -14.463444306853804\n",
      "Episode: 309, reward: -13, epsilon: 0.01, ma: -14.317099876168424\n",
      "Episode: 310, reward: -30, epsilon: 0.01, ma: -15.885389888551583\n",
      "Episode: 311, reward: -13, epsilon: 0.01, ma: -15.596850899696426\n",
      "Episode: 312, reward: -13, epsilon: 0.01, ma: -15.337165809726784\n",
      "Episode: 313, reward: -13, epsilon: 0.01, ma: -15.103449228754107\n",
      "Episode: 314, reward: -13, epsilon: 0.01, ma: -14.893104305878698\n",
      "Episode: 315, reward: -13, epsilon: 0.01, ma: -14.70379387529083\n",
      "Episode: 316, reward: -13, epsilon: 0.01, ma: -14.533414487761748\n",
      "Episode: 317, reward: -13, epsilon: 0.01, ma: -14.380073038985573\n",
      "Episode: 318, reward: -14, epsilon: 0.01, ma: -14.342065735087017\n",
      "Episode: 319, reward: -13, epsilon: 0.01, ma: -14.207859161578316\n",
      "Episode: 320, reward: -18, epsilon: 0.01, ma: -14.587073245420486\n",
      "Episode: 321, reward: -19, epsilon: 0.01, ma: -15.028365920878437\n",
      "Episode: 322, reward: -15, epsilon: 0.01, ma: -15.025529328790594\n",
      "Episode: 323, reward: -13, epsilon: 0.01, ma: -14.822976395911535\n",
      "Episode: 324, reward: -13, epsilon: 0.01, ma: -14.640678756320384\n",
      "Episode: 325, reward: -13, epsilon: 0.01, ma: -14.476610880688346\n",
      "Episode: 326, reward: -14, epsilon: 0.01, ma: -14.428949792619512\n",
      "Episode: 327, reward: -13, epsilon: 0.01, ma: -14.286054813357563\n",
      "Episode: 328, reward: -17, epsilon: 0.01, ma: -14.557449332021807\n",
      "Episode: 329, reward: -13, epsilon: 0.01, ma: -14.401704398819628\n",
      "Episode: 330, reward: -13, epsilon: 0.01, ma: -14.261533958937667\n",
      "Episode: 331, reward: -13, epsilon: 0.01, ma: -14.135380563043901\n",
      "Episode: 332, reward: -13, epsilon: 0.01, ma: -14.021842506739512\n",
      "Episode: 333, reward: -13, epsilon: 0.01, ma: -13.919658256065562\n",
      "Episode: 334, reward: -13, epsilon: 0.01, ma: -13.827692430459006\n",
      "Episode: 335, reward: -22, epsilon: 0.01, ma: -14.644923187413106\n",
      "Episode: 336, reward: -13, epsilon: 0.01, ma: -14.480430868671796\n",
      "Episode: 337, reward: -13, epsilon: 0.01, ma: -14.332387781804618\n",
      "Episode: 338, reward: -15, epsilon: 0.01, ma: -14.399149003624157\n",
      "Episode: 339, reward: -13, epsilon: 0.01, ma: -14.259234103261742\n",
      "Episode: 340, reward: -15, epsilon: 0.01, ma: -14.333310692935568\n",
      "Episode: 341, reward: -13, epsilon: 0.01, ma: -14.199979623642012\n",
      "Episode: 342, reward: -17, epsilon: 0.01, ma: -14.479981661277812\n",
      "Episode: 343, reward: -13, epsilon: 0.01, ma: -14.331983495150032\n",
      "Episode: 344, reward: -13, epsilon: 0.01, ma: -14.19878514563503\n",
      "Episode: 345, reward: -13, epsilon: 0.01, ma: -14.078906631071527\n",
      "Episode: 346, reward: -13, epsilon: 0.01, ma: -13.971015967964375\n",
      "Episode: 347, reward: -13, epsilon: 0.01, ma: -13.873914371167938\n",
      "Episode: 348, reward: -13, epsilon: 0.01, ma: -13.786522934051145\n",
      "Episode: 349, reward: -13, epsilon: 0.01, ma: -13.707870640646032\n",
      "Episode: 350, reward: -13, epsilon: 0.01, ma: -13.63708357658143\n",
      "Episode: 351, reward: -17, epsilon: 0.01, ma: -13.973375218923287\n",
      "Episode: 352, reward: -13, epsilon: 0.01, ma: -13.87603769703096\n",
      "Episode: 353, reward: -13, epsilon: 0.01, ma: -13.788433927327864\n",
      "Episode: 354, reward: -13, epsilon: 0.01, ma: -13.709590534595078\n",
      "Episode: 355, reward: -13, epsilon: 0.01, ma: -13.63863148113557\n",
      "Episode: 356, reward: -13, epsilon: 0.01, ma: -13.574768333022014\n",
      "Episode: 357, reward: -15, epsilon: 0.01, ma: -13.717291499719813\n",
      "Episode: 358, reward: -13, epsilon: 0.01, ma: -13.645562349747832\n",
      "Episode: 359, reward: -13, epsilon: 0.01, ma: -13.58100611477305\n",
      "Episode: 360, reward: -13, epsilon: 0.01, ma: -13.522905503295746\n",
      "Episode: 361, reward: -13, epsilon: 0.01, ma: -13.470614952966173\n",
      "Episode: 362, reward: -13, epsilon: 0.01, ma: -13.423553457669557\n",
      "Episode: 363, reward: -13, epsilon: 0.01, ma: -13.381198111902602\n",
      "Episode: 364, reward: -13, epsilon: 0.01, ma: -13.343078300712342\n",
      "Episode: 365, reward: -13, epsilon: 0.01, ma: -13.30877047064111\n",
      "Episode: 366, reward: -13, epsilon: 0.01, ma: -13.277893423577\n",
      "Episode: 367, reward: -13, epsilon: 0.01, ma: -13.2501040812193\n",
      "Episode: 368, reward: -13, epsilon: 0.01, ma: -13.225093673097371\n",
      "Episode: 369, reward: -13, epsilon: 0.01, ma: -13.202584305787635\n",
      "Episode: 370, reward: -13, epsilon: 0.01, ma: -13.182325875208873\n",
      "Episode: 371, reward: -13, epsilon: 0.01, ma: -13.164093287687987\n",
      "Episode: 372, reward: -13, epsilon: 0.01, ma: -13.147683958919188\n",
      "Episode: 373, reward: -13, epsilon: 0.01, ma: -13.13291556302727\n",
      "Episode: 374, reward: -14, epsilon: 0.01, ma: -13.219624006724544\n",
      "Episode: 375, reward: -13, epsilon: 0.01, ma: -13.19766160605209\n",
      "Episode: 376, reward: -13, epsilon: 0.01, ma: -13.177895445446882\n",
      "Episode: 377, reward: -13, epsilon: 0.01, ma: -13.160105900902195\n",
      "Episode: 378, reward: -13, epsilon: 0.01, ma: -13.144095310811977\n",
      "Episode: 379, reward: -13, epsilon: 0.01, ma: -13.12968577973078\n",
      "Episode: 380, reward: -13, epsilon: 0.01, ma: -13.116717201757703\n",
      "Episode: 381, reward: -13, epsilon: 0.01, ma: -13.105045481581934\n",
      "Episode: 382, reward: -13, epsilon: 0.01, ma: -13.094540933423742\n",
      "Episode: 383, reward: -13, epsilon: 0.01, ma: -13.085086840081368\n",
      "Episode: 384, reward: -19, epsilon: 0.01, ma: -13.676578156073232\n",
      "Episode: 385, reward: -13, epsilon: 0.01, ma: -13.60892034046591\n",
      "Episode: 386, reward: -13, epsilon: 0.01, ma: -13.548028306419319\n",
      "Episode: 387, reward: -13, epsilon: 0.01, ma: -13.493225475777388\n",
      "Episode: 388, reward: -120, epsilon: 0.01, ma: -24.14390292819965\n",
      "Episode: 389, reward: -13, epsilon: 0.01, ma: -23.02951263537969\n",
      "Episode: 390, reward: -13, epsilon: 0.01, ma: -22.026561371841723\n",
      "Episode: 391, reward: -13, epsilon: 0.01, ma: -21.123905234657553\n",
      "Episode: 392, reward: -13, epsilon: 0.01, ma: -20.311514711191798\n",
      "Episode: 393, reward: -13, epsilon: 0.01, ma: -19.580363240072618\n",
      "Episode: 394, reward: -13, epsilon: 0.01, ma: -18.92232691606536\n",
      "Episode: 395, reward: -13, epsilon: 0.01, ma: -18.330094224458826\n",
      "Episode: 396, reward: -119, epsilon: 0.01, ma: -28.39708480201294\n",
      "Episode: 397, reward: -13, epsilon: 0.01, ma: -26.85737632181165\n",
      "Episode: 398, reward: -13, epsilon: 0.01, ma: -25.471638689630485\n",
      "Episode: 399, reward: -13, epsilon: 0.01, ma: -24.22447482066744\n"
     ]
    }
   ],
   "source": [
    "episode = 400\n",
    "rewards = []\n",
    "ma_rewards = []\n",
    "for i in range(episode):\n",
    "    ep_reward = 0\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        agent.qlearn(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(ep_reward)\n",
    "    if ma_rewards:\n",
    "        ma_rewards.append(ma_rewards[-1]*0.9+ep_reward*0.1)\n",
    "    else :\n",
    "        ma_rewards.append(ep_reward)\n",
    "    print(\"Episode: {}, reward: {}, epsilon: {}, ma: {}\".format(i,ep_reward,agent.epsilon,ma_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.81727263e+00, -6.82303976e+00, -6.84089944e+00,\n",
       "        -6.83580475e+00],\n",
       "       [-6.67696820e+00, -6.64951068e+00, -6.68688604e+00,\n",
       "        -6.65207030e+00],\n",
       "       [-6.41489390e+00, -6.41361422e+00, -6.43292449e+00,\n",
       "        -6.42680430e+00],\n",
       "       [-6.15451645e+00, -6.13697066e+00, -6.16192968e+00,\n",
       "        -6.15802587e+00],\n",
       "       [-5.83962153e+00, -5.82556983e+00, -5.82538417e+00,\n",
       "        -5.86570196e+00],\n",
       "       [-5.47772527e+00, -5.48240972e+00, -5.47904396e+00,\n",
       "        -5.48963860e+00],\n",
       "       [-5.08144824e+00, -5.08498776e+00, -5.11963128e+00,\n",
       "        -5.15465335e+00],\n",
       "       [-4.69958578e+00, -4.66589370e+00, -4.65973157e+00,\n",
       "        -4.69612964e+00],\n",
       "       [-4.19172998e+00, -4.20218981e+00, -4.21274136e+00,\n",
       "        -4.23822864e+00],\n",
       "       [-3.71162809e+00, -3.70573302e+00, -3.69913480e+00,\n",
       "        -3.72432403e+00],\n",
       "       [-3.21000421e+00, -3.16447482e+00, -3.17091674e+00,\n",
       "        -3.23551514e+00],\n",
       "       [-2.65362525e+00, -2.65688320e+00, -2.62482864e+00,\n",
       "        -2.63267828e+00],\n",
       "       [-6.96937553e+00, -6.95269578e+00, -7.00059729e+00,\n",
       "        -6.97847676e+00],\n",
       "       [-6.74846805e+00, -6.73492435e+00, -6.73493035e+00,\n",
       "        -6.76891357e+00],\n",
       "       [-6.45965202e+00, -6.46442715e+00, -6.48869571e+00,\n",
       "        -6.48427258e+00],\n",
       "       [-6.16243768e+00, -6.15422448e+00, -6.14725024e+00,\n",
       "        -6.14988735e+00],\n",
       "       [-5.84157571e+00, -5.79836953e+00, -5.79933987e+00,\n",
       "        -5.81803449e+00],\n",
       "       [-5.44189914e+00, -5.40869158e+00, -5.43011605e+00,\n",
       "        -5.40645253e+00],\n",
       "       [-5.02911999e+00, -4.98585705e+00, -4.98326975e+00,\n",
       "        -5.05679032e+00],\n",
       "       [-4.55744368e+00, -4.49216427e+00, -4.51014607e+00,\n",
       "        -4.57194811e+00],\n",
       "       [-4.03228311e+00, -3.95213970e+00, -3.96211197e+00,\n",
       "        -4.03541862e+00],\n",
       "       [-3.42213233e+00, -3.35225825e+00, -3.35390498e+00,\n",
       "        -3.37318403e+00],\n",
       "       [-2.76675041e+00, -2.67306839e+00, -2.67081621e+00,\n",
       "        -2.79167304e+00],\n",
       "       [-2.03780624e+00, -1.94378775e+00, -1.89907250e+00,\n",
       "        -1.90111096e+00],\n",
       "       [-7.18191186e+00, -7.17564727e+00, -7.29356401e+00,\n",
       "        -7.19287158e+00],\n",
       "       [-6.87252430e+00, -6.86187568e+00, -1.85320554e+01,\n",
       "        -6.94066360e+00],\n",
       "       [-6.53605259e+00, -6.51321075e+00, -4.24638446e+01,\n",
       "        -6.55801845e+00],\n",
       "       [-6.12691864e+00, -6.12579407e+00, -4.17708196e+01,\n",
       "        -6.13624444e+00],\n",
       "       [-5.73788696e+00, -5.69532772e+00, -2.75608292e+01,\n",
       "        -5.74790438e+00],\n",
       "       [-5.23497770e+00, -5.21703098e+00, -4.12078007e+01,\n",
       "        -5.24092573e+00],\n",
       "       [-4.88118948e+00, -4.68559000e+00, -3.51592383e+01,\n",
       "        -4.78078009e+00],\n",
       "       [-4.09862506e+00, -4.09510000e+00, -2.79338044e+01,\n",
       "        -4.10018908e+00],\n",
       "       [-3.55956707e+00, -3.43900000e+00, -4.29187503e+01,\n",
       "        -3.45592436e+00],\n",
       "       [-2.86106676e+00, -2.71000000e+00, -1.95252509e+01,\n",
       "        -2.79158709e+00],\n",
       "       [-2.03832614e+00, -1.90000000e+00, -9.56424097e+00,\n",
       "        -2.08249746e+00],\n",
       "       [-1.36748631e+00, -1.08402813e+00, -1.00000000e+00,\n",
       "        -1.19489671e+00],\n",
       "       [-7.45798300e+00, -6.60931911e+01, -7.47061033e+00,\n",
       "        -7.48833657e+00],\n",
       "       [ 4.31906636e-01,  7.12656397e-01,  8.39782829e-01,\n",
       "         7.57123199e-01],\n",
       "       [ 8.48992172e-01,  6.64893983e-01,  6.36923438e-01,\n",
       "         6.93678039e-01],\n",
       "       [ 3.28333388e-01,  5.46316824e-01,  5.69974043e-01,\n",
       "         9.67241202e-01],\n",
       "       [ 3.31801822e-01,  1.21230923e-01,  2.39923141e-01,\n",
       "         3.64274094e-01],\n",
       "       [ 1.24355801e-03,  3.33353418e-01,  6.63302260e-01,\n",
       "         4.04719753e-01],\n",
       "       [ 9.82155680e-01,  9.57378039e-01,  3.44811008e-03,\n",
       "         4.40453184e-01],\n",
       "       [ 5.90458696e-01,  6.96732757e-01,  7.13335930e-01,\n",
       "         3.11132957e-01],\n",
       "       [ 7.99729691e-01,  3.28875694e-01,  3.66644906e-01,\n",
       "         6.67337707e-01],\n",
       "       [ 4.05482175e-01,  6.46708318e-01,  1.22139541e-01,\n",
       "         3.82709363e-01],\n",
       "       [ 6.83360350e-01,  7.75452350e-01,  7.60409820e-01,\n",
       "         6.66505942e-01],\n",
       "       [ 3.83396513e-01,  6.33883437e-01,  3.46390905e-01,\n",
       "         7.68218136e-01]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看路径："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 up\n",
      "25 right\n",
      "26 right\n",
      "27 right\n",
      "28 right\n",
      "29 right\n",
      "30 right\n",
      "31 right\n",
      "32 right\n",
      "33 right\n",
      "34 right\n",
      "35 down\n"
     ]
    }
   ],
   "source": [
    "action_lang=[\"up\",\"right\",\"down\",\"left\"]\n",
    "print(36,action_lang[np.argmax(agent.Q[36,:])])\n",
    "for i in range(25,36):\n",
    "    print(i,action_lang[np.argmax(agent.Q[i,:])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 习题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 构成马尔可夫决策过程的四元组**\n",
    "\n",
    "$(state,action,reward,P)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 学习**\n",
    "\n",
    "exploration & exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Sarsa 学习过程**\n",
    "\n",
    "$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha ( R_{t+1} + \\gamma Q(s_{t+1},a_{t+1} - Q(s,a)) )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. QLearning 与 Sarsa 区别**\n",
    "\n",
    "QL：异策略，大胆探索，不需要知道下一步的策略（直接贪心）\n",
    "Sa：同策略，保守探索，需要知道下一步的策略\n",
    "\n",
    "一般来说 Sarsa 显得更加保守和安全，而 QLearning 更能找到最佳策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 策略梯度\n",
    "\n",
    "> 9/12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 策略梯度算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习具有三个组成部分：\n",
    "- 演员 actor\n",
    "- 环境 env\n",
    "- 奖励函数 reward\n",
    "\n",
    "环境 与 奖励函数 都不是我们可以控制的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 策略函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**策略** 一般记作 $\\pi$，在基于模型的学习中 策略是一个函数\n",
    "\n",
    "若是采用深度学习方法来做强化学习，策略就是一个网络，网络的参数记作 $\\theta$\n",
    "\n",
    "**例如：**\n",
    "\n",
    "给网络输入一个游戏画面；输出是可以执行的动作的概率\n",
    "\n",
    "有三个动作就有三个输出神经元\n",
    "\n",
    "经过 softmax 操作后就得到了概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 轨迹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**轨迹** 就是一个回合内环境输出的状态s和演员输出的动作a的组合\n",
    "\n",
    "$\\tau = \\{ s_1,a_1,s_2,a_2...,s_t,a_t \\}$\n",
    "\n",
    "根据给定的参数 $\\theta$ 可以计算某个轨迹的发生概率：\n",
    "\n",
    "$p_\\theta(\\tau) = p(s_1)p_\\theta(a_1|s_1)p(s_2|s_1,a_1)p_\\theta(a_2|s_2)...$\n",
    "\n",
    "即：\n",
    "\n",
    "![](./img/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个式子中：\n",
    "\n",
    "$p(s_t+1|s_t,a_t)$ 涉及到状态转移，因此是环境决定的\n",
    "\n",
    "$p_\\theta(a_t|s_t)$ 是可以控制的，即：**给定参数 $\\theta$,在 $s_t$ 条件下选择 $a_t$ 的概率**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 奖励函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奖励函数：$reward_t = r(s_t,a_t)$\n",
    "\n",
    "将一条轨迹上所有s-a的奖励加起来，就是轨迹的回报：$R(\\tau)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**学习的目的就是通过调整 $\\theta$，使 $R(\\tau)$ 的值取到最大**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R(\\tau)$ 是一个随机变量，其所有轨迹的概率加权和就是期望奖励\n",
    "\n",
    "$R_\\theta=\\sum R(\\tau)p_\\theta(\\tau)$\n",
    "\n",
    "使用 **梯度上升** 最大化期望奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际期望值难以计算，利用蒙特卡洛思想 采用采样的方式：\n",
    "\n",
    "**采样 N 个轨迹并计算得到的回报，加起来就得到了梯度**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/16.png)\n",
    "\n",
    "n 指第n回合，t 指第t时间步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 采样，得到轨迹\n",
    "- 代入轨迹，得到梯度\n",
    "- 通过梯度来更新参数 $\\theta$ : $\\theta \\leftarrow \\theta + \\eta \\nabla R_\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$\\eta$ 是学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新一遍后，重新采样数据再更新模型，一次采样只使用一次（一次采样多个回合）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以认为强化学习是一个分类问题\n",
    "\n",
    "一般分类问题 目标函数写成最小化交叉熵，即最大化:\n",
    "\n",
    "![](./img/17.png)\n",
    "\n",
    "在强化学习中，还要在前面乘一个权重 轨迹回报 $R(\\tau)$\n",
    "\n",
    "![](./img/18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 添加基线\n",
    "- 分配合适的分数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **添加基线**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：**\n",
    "\n",
    "理想情况下，目标函数的值来自于对大量轨迹采样的期望值；然而现实情况下的采样只是一小部分。\n",
    "\n",
    "如果没有采样到动作a，即使a是一个不错的动作，它也会被忽略（改率降低）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解决：**\n",
    "\n",
    "对 $R(\\tau)$ 添加一条基线，选取某个奖励值 $b$，在计算 $\\nabla R_\\theta$ 时，将所有奖励值都减去 $b$。得到 **相对优势**\n",
    "\n",
    "即：![](./img/19.png)\n",
    "\n",
    "这样使得权重值有正有负，从而使得梯度下降方向更加合理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b$ 通常是由一个网络 (评论员网络) 估计出来的，$R-b$ 称为 **优势函数**\n",
    "\n",
    "$A^\\theta (s_t,a_t) = R-b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **分配合适分数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：**\n",
    "\n",
    "我们需要的是评价某个状态下的策略的好坏，然而根据轨迹的回报来计算是不公正的：\n",
    "\n",
    "<u>一条好的轨迹不代表其中的动作都是好的；而一条坏的轨迹却可能包含很多好的动作。</u>\n",
    "\n",
    "**解决：**\n",
    "\n",
    "在计算某个 s-a 的奖励时，不把整个轨迹的回报代入，而是计算这个动作执行之后得到的的所有奖励\n",
    "\n",
    "并且引入折扣因子 $\\gamma$ 用于调整对长短期收益的关注程度： $r_{a_t}=r_1 + \\gamma  r_2 + \\gamma ^2  r_3 + ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE 蒙特卡洛策略梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 蒙特卡洛方法：使用采样到的某个s-a的回报期望来近似其策略q值：$q_\\pi(s_t,a_t) = q_t(s_t,a_t) = r_t + \\gamma  r_{t+1} + \\gamma ^2  r_{t+2} + ...$\n",
    "- 策略梯度算法：使用 $\\pi (\\theta)$ 策略函数代替 Q 表格，利用梯度更新参数：$\\theta \\leftarrow \\theta + \\alpha  \\nabla_\\theta J(\\pi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 策略模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用深度神经网络作为策略模型 $\\pi_\\theta (s_t,a_t)$ 输入状态 $s_t,a_t$ 输出动作概率 $p_\\theta(a | s_t)$\n",
    "\n",
    "在神经网络中：\n",
    "\n",
    "- 输入层：状态 $s_t$， `num_inputs = num_states`\n",
    "- 隐藏层：神经元\n",
    "- 输出层：动作概率 $p_\\theta(a | s_t)$ `num_outputs = num_actions`\n",
    "\n",
    "最后，还需要加上一层 softmax 层，将输出转换为概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128,lr = 1e-3):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.num_actions = action_size\n",
    "        self.num_states = state_size\n",
    "        self.lr = lr\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 智能体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 选择动作：概率采样\n",
    "- 计算收益：\n",
    "$$\n",
    "G_t=\\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}\n",
    "$$\n",
    "- 更新参数：策略梯度算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import autograd\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, cfg):\n",
    "        self.gamma = cfg.gamma\n",
    "        self.name = cfg.name\n",
    "        self.env = env\n",
    "        self.state_size = cfg.state_size\n",
    "        self.action_size = cfg.action_size\n",
    "        self.net = PolicyNet(self.state_size, self.action_size,lr=cfg.lr)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        # 使用网络得到概率分布\n",
    "        # print(\">>> \",type(state),state.size())\n",
    "        probs = self.net(state)\n",
    "        # 预测时不参与计算梯度\n",
    "        choice = np.random.choice(self.action_size, p=probs[0].detach().numpy())\n",
    "        # 对数化\n",
    "        log_prob = torch.log(probs.squeeze(0)[choice])\n",
    "        return choice, log_prob\n",
    "    \n",
    "    def get_action(self, state): # for test\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        return np.argmax(self.net(state)[0].detach().numpy())\n",
    "    \n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = []\n",
    "        # 计算每条轨迹的折扣奖励\n",
    "        for t in range(len(rewards)):\n",
    "            Gt = 0\n",
    "            pw = 0\n",
    "            for reward in rewards[t:]:\n",
    "                Gt += reward * (self.gamma ** pw)\n",
    "                pw += 1\n",
    "            discounted_rewards.append(Gt)\n",
    "        return discounted_rewards\n",
    "    \n",
    "    def update_policy(self, log_probs, rewards):\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "        discounted_rewards = torch.tensor(discounted_rewards,dtype=torch.float)\n",
    "        # 对回报作归一化\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "        # 计算梯度\n",
    "        policy_grad = []\n",
    "        for log_prob, discounted_reward in zip(log_probs, discounted_rewards):\n",
    "            policy_grad.append(-discounted_reward * log_prob)\n",
    "        # 优化\n",
    "        self.net.optimizer.zero_grad()\n",
    "        pg = torch.stack(policy_grad).sum()\n",
    "        pg.backward()\n",
    "        self.net.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`CarPole-v0` 环境：**\n",
    "\n",
    "控制一个小车向左或向右移动，目的是使小车上的摆件保持在竖直位置上。每一步使得摆件竖直都会得到 1 单位奖励。\n",
    "\n",
    "动作空间：0，1 表示向左和向右\n",
    "\n",
    "观察空间：一个四维连续空间，包含小车位置、速度和摆件角度、角速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n, env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.num_episodes = 2000\n",
    "        self.gamma = 0.9\n",
    "        self.name = 'DeepPolicyGradient'\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.max_steps = 500\n",
    "        self.lr = 0.0005\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**每得到一条轨迹，就对参数进行更新**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env,agent,cfg):\n",
    "    print('Training Starts with LR {}'.format(cfg.lr))\n",
    "    num_steps = []\n",
    "    avg_num_steps = []\n",
    "    rewards = []\n",
    "    for ep in range(cfg.num_episodes): # 回合开始\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        ep_rewards = []\n",
    "        for step in range(cfg.max_steps): # 每个回合的步数\n",
    "            action, log_prob = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            log_probs.append(log_prob)\n",
    "            ep_rewards.append(reward)\n",
    "            if done:\n",
    "                # 得到一条轨迹\n",
    "                agent.update_policy(log_probs,ep_rewards)\n",
    "                num_steps.append(step)\n",
    "                avg_num_steps.append(np.mean(num_steps[-10:]))\n",
    "                rewards.append(sum(ep_rewards))\n",
    "                if ep % 100 == 0:\n",
    "                    print('Episode: {}/{}, Steps: {}, avg_R:{}'.format(ep, cfg.num_episodes, step,np.mean(avg_num_steps)))\n",
    "\n",
    "                break\n",
    "            state = next_state\n",
    "    return num_steps, avg_num_steps, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_plot(data1, data2, data3, label1, label2, label3, xlabel):\n",
    "    plt.plot(data1)\n",
    "    plt.plot(data2)\n",
    "    plt.plot(data3)\n",
    "    plt.legend([label1, label2,label3])\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**开始训练：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Starts with LR 0.0005\n",
      "Episode: 0/2000, Steps: 27, avg_R:27.0\n",
      "Episode: 100/2000, Steps: 51, avg_R:17.705174446016027\n",
      "Episode: 200/2000, Steps: 42, avg_R:24.176231935560292\n",
      "Episode: 300/2000, Steps: 60, avg_R:31.76652032906186\n",
      "Episode: 400/2000, Steps: 135, avg_R:45.50230079562997\n",
      "Episode: 500/2000, Steps: 140, avg_R:65.96930662484554\n",
      "Episode: 600/2000, Steps: 199, avg_R:83.87774146264164\n",
      "Episode: 700/2000, Steps: 199, avg_R:96.85695095441883\n",
      "Episode: 800/2000, Steps: 188, avg_R:108.18716931216933\n",
      "Episode: 900/2000, Steps: 199, avg_R:117.17494186353787\n",
      "Episode: 1000/2000, Steps: 168, avg_R:124.59842419485275\n",
      "Episode: 1100/2000, Steps: 129, avg_R:128.76359910903508\n",
      "Episode: 1200/2000, Steps: 199, avg_R:132.0468964355101\n",
      "Episode: 1300/2000, Steps: 199, avg_R:136.93353006844552\n",
      "Episode: 1400/2000, Steps: 199, avg_R:141.07239301859215\n",
      "Episode: 1500/2000, Steps: 199, avg_R:144.6146053424701\n",
      "Episode: 1600/2000, Steps: 199, avg_R:147.89351818803723\n",
      "Episode: 1700/2000, Steps: 199, avg_R:150.62464586657708\n",
      "Episode: 1800/2000, Steps: 199, avg_R:153.06575381401868\n",
      "Episode: 1900/2000, Steps: 199, avg_R:155.21805503369154\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env,cfg)\n",
    "num_steps, avg_num_steps, rewards = train(env,agent,cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_plot(num_steps,avg_num_steps,[],'num_steps','avg_num_steps', 'reward','episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNet(\n",
       "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent,env,episodes=10,max_step=500,render=False):\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    for episode in range(episodes):\n",
    "        env.render() if render else None\n",
    "        episode_reward = 0\n",
    "        state = env.reset()\n",
    "        for step in range(max_step):\n",
    "            next_state,reward,done,_ = env.step(agent.get_action(state))\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                rewards.append(episode_reward)\n",
    "                steps.append(step+1)\n",
    "                print('Episode: {}, Steps: {}, R:{}'.format(episode, step+1,episode_reward))\n",
    "                break\n",
    "            state = next_state\n",
    "    return rewards,steps\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Steps: 200, R:200.0\n",
      "Episode: 1, Steps: 200, R:200.0\n",
      "Episode: 2, Steps: 200, R:200.0\n",
      "Episode: 3, Steps: 200, R:200.0\n",
      "Episode: 4, Steps: 200, R:200.0\n",
      "Episode: 5, Steps: 200, R:200.0\n",
      "Episode: 6, Steps: 200, R:200.0\n",
      "Episode: 7, Steps: 200, R:200.0\n",
      "Episode: 8, Steps: 200, R:200.0\n",
      "Episode: 9, Steps: 200, R:200.0\n"
     ]
    }
   ],
   "source": [
    "rewards,steps = test(agent,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Steps: 200, R:200.0\n"
     ]
    }
   ],
   "source": [
    "test(agent,env,1,render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 习题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 一个具体的轨迹 s1,a1,s2,a2 出现的概率取决于什么？**\n",
    "\n",
    "环境模型的状态转移概率 $p(s_{t+1}|s_t,a_t)$ 和智能体模型的动作选择概率 $p_\\theta(a_t|s_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 最大化期望奖励时 应该使用什么方法**\n",
    "\n",
    "梯度上升法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 理解策略梯度公式**\n",
    "\n",
    "![](./img/pg.png)\n",
    "\n",
    "如果在 st 时刻执行动作 at 使得最后的回报为正，就加大 at 的概率，反之则减小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. 梯度提升计算的方法**\n",
    "\n",
    "深度学习 学习器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. 优化技巧**\n",
    "\n",
    "添加基线 分配合适分数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. 蒙特卡洛强化学习 和 时序差分强化学习的区别**\n",
    "\n",
    "蒙特卡洛需要取得一条完整的轨迹再进行学习\n",
    "\n",
    "时序差分法不需要完整轨迹，只需要一个状态和下一个状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REINFORCE 算法的计算过程**\n",
    "\n",
    "- 确定策略模型\n",
    "- 采样轨迹\n",
    "- 优化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 近端策略优化\n",
    "\n",
    "> 9/15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "近端策略优化（proximal policy optimization，PPO）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**同策略训练方法：** 与环境交互的智能体和学习的智能体是相同的，如策略梯度、sarsa\n",
    "\n",
    "**异策略训练方法：** 学习的智能体和环境交互的智能体是不同的，如 q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略梯度是同策略的算法，算法中需要：\n",
    "\n",
    "- 智能体\n",
    "- 策略\n",
    "- 演员\n",
    "\n",
    "演员的作用是与环境交互获得采样数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：**\n",
    "\n",
    "由于是同策略算法，使用待学习的模型来与环境交互，那么每次更新完参数后都会得到一个新的模型，原有的采样数据不能使用了，这样就非常花费时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解决：**\n",
    "\n",
    "变成异策略的：设计使得 $\\theta$ 能够 使用演员 $\\theta '$ 交互采样到的数据来训练\n",
    "\n",
    "这样： $\\theta '$ 只需要采样一次就可以让 $\\theta $ 多次训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重要性采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**背景：**\n",
    "- 函数 $f(x)$\n",
    "- 需要计算从分布 $p$ 采样数据，代入函数 $f(x)$ \n",
    "- 但是不能直接从 分布 $p$ 采样\n",
    "- 只能从另一个分布 $q$ 采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算期望就是积分求平均，因此：\n",
    "\n",
    "$\\int f(x)p(x)dx = \\int f(x) \\frac{p(x)}{q(x)} q(x)dx = \\int \\frac{p(x)}{q(x)} f(x)q(x)dx$\n",
    "\n",
    "即：\n",
    "\n",
    "![](./img/20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 $q$ 采样出来的数据，都需要乘以 **重要性权重** $\\frac{p(x)}{q(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q$ 可以是任意分布，只要在 $q=0$ 时 $p \\neq 0$。 但在实现上，二者差距不能太大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO 实现了对两个分布差距太大的避免"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 近端策略优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dailly38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
