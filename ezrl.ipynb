{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EazyRL_v1.0.6 笔记\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 绪论\n",
    "> 9/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强化学习 RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 机器学习的一个分支\n",
    "- 研究智能主体在环境中如何采取行动以最大化累积奖励\n",
    "- 基本思想是通过智能体（Agent）与环境（Environment）的**交互**来**学习最优策略**，使得智能体能够采取一系列**动作**以**获取最大的长期奖励**\n",
    "- 目标是寻找最优策略\n",
    "\n",
    "![](./img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 产生 action 作用于 env\n",
    "\n",
    "env 根据 action 产生 reward 和 next_state\n",
    "\n",
    "agent 根据 reward 和 next_state 进行更新\n",
    "\n",
    "循环上述过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**与监督学习对比**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督学习：\n",
    "- 样本互不相关，是独立同分布的\n",
    "- 根据监督者给出的 label 确定正误\n",
    "- 通过反向传播更新模型参数，实现学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习：\n",
    "- 输入的是序列数据，数据之间有很强的关联\n",
    "- 不给出正确的动作，需要学习器不断尝试\n",
    "- 延迟奖励：奖励信号不是立即给出的，而是延迟一段时间\n",
    "- 动作会影响后面的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度强化学习：省去特征提取，实现端到端"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 序列决策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在与环境的交互过程中，智能体会获得很多观测\n",
    "\n",
    "每一个观测 - 采取一个动作 - 得到一个奖励\n",
    "\n",
    "**历史是观测 动作 奖励的序列**\n",
    "\n",
    "智能体采取**当前动作** 依赖于 **得到的历史**\n",
    "\n",
    "**状态是历史的函数**\n",
    "\n",
    "基本问题：**学习与规划**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体能够观察环境所有状态（上帝视角） - 完全可观测的环境 - 马尔可夫决策过程 MDP\n",
    "\n",
    "智能体只能看到部分的观测状态（有限信息） - 部分可观测的环境 - 部分可观测马尔可夫决策过程 POMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动作空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定环境中 有效动作的集合\n",
    "\n",
    "- 离散动作空间：动作数量有限，如围棋\n",
    "- 连续动作空间：动作是实值的向量，如智能驾驶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 智能体组成\n",
    "\n",
    "> 策略，价值函数，模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **策略**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入状态到动作的函数\n",
    "\n",
    "- 随机性策略：概率分布采样\n",
    "- 确定性策略：直接选择最有可能的动作（可能性最大的动作）\n",
    "\n",
    "通常采用随机性策略，避免出现局部最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **价值函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数值是对**未来奖励**的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**折扣因子**\n",
    "\n",
    "折扣因子γ（gamma） 反映了决策者对未来收益相对于当前收益的重视程度。\n",
    "\n",
    "折扣因子越大，表示决策者越看重未来的收益；\n",
    "\n",
    "折扣因子越小，表示决策者越倾向于即时满足，即更看重当前的收益。\n",
    "\n",
    "\n",
    "\n",
    "![](./img/2.png)\n",
    "\n",
    "pi策略 gamma折扣因子 r回报 s状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 函数**\n",
    "\n",
    "包含 状态 和 动作 两个变量，未来可以获得奖励的期望取决于当前的状态和当前的动作。\n",
    "\n",
    "![](./img/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 状态转移概率\n",
    "- 奖励函数：当前状态采取某个动作可以得到的奖励（价值函数是预测）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 智能体类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **基于价值 / 基于策略**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于价值：学习价值函数 （QL,Sarsa）\n",
    "- 基于策略：学习策略，通过状态给出动作概率 (PG)\n",
    "- 结合两者：演员（策略）- 评论员（价值）智能体\n",
    "\n",
    "基于价值迭代的方法只能应用在不连续的、离散的环境下，而基于策略迭代的方法可以应用在连续的环境中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **有模型 / 免模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是否需要对真实环境进行建模：\n",
    "\n",
    "- 有模型：学习状态转移来采取动作，如状态转移和奖励函数都是**已知**的，能知道某一状态采取某一决策后的奖励和状态（动规问题）\n",
    "- 免模型：学习价值函数和策略函数进行决策，没有环境转移函数和奖励函数。没有对真实环境进行建模，智能体只能在真实环境中通过一定的策略来执行动作，**等待**奖励和状态迁移，然后根据这些反馈信息来**更新**动作策略，这样反复迭代直到学习到最优策略。\n",
    "\n",
    "有模型同时在真实环境与虚拟世界学习\n",
    "\n",
    "免模型直接与真实环境交互来学习 一般需要大量数据样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索与利用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单步强化学习任务 - K-臂赌博机\n",
    "\n",
    "- 仅探索：机会平均分配，最后计算期望\n",
    "- 仅利用：贪心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CartPole-v0 环境**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info, others = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态信息 observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.2427716, 3.7990518, 6.626667 , 4.7200627], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奖励 reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "游戏是否完成了 done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调试信息 info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`step()` 完成了一个完整的 S-A-R-S' 迭代过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看 gym 库中的环境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CartPole-v0',\n",
       " 'CartPole-v1',\n",
       " 'MountainCar-v0',\n",
       " 'MountainCarContinuous-v0',\n",
       " 'Pendulum-v1',\n",
       " 'Acrobot-v1',\n",
       " 'LunarLander-v2',\n",
       " 'LunarLanderContinuous-v2',\n",
       " 'BipedalWalker-v3',\n",
       " 'BipedalWalkerHardcore-v3',\n",
       " 'CarRacing-v2',\n",
       " 'Blackjack-v1',\n",
       " 'FrozenLake-v1',\n",
       " 'FrozenLake8x8-v1',\n",
       " 'CliffWalking-v0',\n",
       " 'Taxi-v3',\n",
       " 'Reacher-v2',\n",
       " 'Reacher-v4',\n",
       " 'Pusher-v2',\n",
       " 'Pusher-v4',\n",
       " 'InvertedPendulum-v2',\n",
       " 'InvertedPendulum-v4',\n",
       " 'InvertedDoublePendulum-v2',\n",
       " 'InvertedDoublePendulum-v4',\n",
       " 'HalfCheetah-v2',\n",
       " 'HalfCheetah-v3',\n",
       " 'HalfCheetah-v4',\n",
       " 'Hopper-v2',\n",
       " 'Hopper-v3',\n",
       " 'Hopper-v4',\n",
       " 'Swimmer-v2',\n",
       " 'Swimmer-v3',\n",
       " 'Swimmer-v4',\n",
       " 'Walker2d-v2',\n",
       " 'Walker2d-v3',\n",
       " 'Walker2d-v4',\n",
       " 'Ant-v2',\n",
       " 'Ant-v3',\n",
       " 'Ant-v4',\n",
       " 'Humanoid-v2',\n",
       " 'Humanoid-v3',\n",
       " 'Humanoid-v4',\n",
       " 'HumanoidStandup-v2',\n",
       " 'HumanoidStandup-v4']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym import envs\n",
    "\n",
    "specs = envs.registry.values() # 原书代码有误\n",
    "ids = [spec.id for spec in specs]\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 与 `gym` 库交互"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例：小车上山"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<MountainCarEnv<MountainCar-v0>>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观测空间：连续空间用 `gym.spaces.Box` 类 表示范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作空间：离散空间用 `gym.spaces.Discrete` 类 表示可取的数量，即动作数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现智能体，控制小车移动："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "    def decide(self, observation):\n",
    "        pos,vel = observation\n",
    "        lb = min( -0.09 * (pos + 0.25) ** 2 + 0.03,\n",
    "                0.3 * (pos + 0.9) ** 4 - 0.008 )\n",
    "        ub = -0.07 * (pos + 0.38) ** 2 + 0.07\n",
    "        if lb < vel < ub:\n",
    "            return  2\n",
    "        else: return 0\n",
    "    def learn(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SimpleAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与环境交互："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env,agent,render=False,train=False):\n",
    "    episode_reward = 0 # 回合总奖励\n",
    "    observation = env.reset() \n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.decide(observation)\n",
    "        next_obs,reward,done,_ = env.step(action) \n",
    "        episode_reward += reward\n",
    "        if train:\n",
    "            agent.learn(observation,action,reward,done)\n",
    "        if done:\n",
    "            break\n",
    "        observation = next_obs\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交互一回合：\n",
    "> !TODO notebook 中无法显示 gym 窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-105.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.seed(7) # 随机种子 v0.26 已经去掉了这个方法\n",
    "episode_reward= play(env,agent)\n",
    "episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估智能体的性能，需要计算出连续交互 100 回合的平均回合奖励："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-106.13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_rewards = [ play(env,agent) for _ in range(100) ]\n",
    "import numpy as np\n",
    "np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **总结**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('') # 取环境\n",
    "obs = env.reset()  # 初始化环境\n",
    "action = 1         # 随机动作\n",
    "next_obs,reward,done,info = env.step(action)   # 执行动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**习题**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 强化学习基本结构？**\n",
    "\n",
    "agent, environment, reward, value function, policy/action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 比监督学习训练过程困难的原因**\n",
    "\n",
    "延迟奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 基本特征**\n",
    "\n",
    "exploration, exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. 状态和观测？**\n",
    "\n",
    "状态：客观世界中事物的属性，是事物内部特征的集合，是全集\n",
    "\n",
    "观测：是状态的一种表现，是外界对状态的一种观察，是子集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. 智能体组成**\n",
    "\n",
    "policy, value function, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. 分类？**\n",
    "\n",
    "policy_based, value_based\n",
    "\n",
    "model_free, model_based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. 基于策略迭代和基于价值迭代的强化学习方法有什么区别**\n",
    "\n",
    "策略：通过策略给出概率，连续的环境\n",
    "\n",
    "价值：学习价值函数，离散的环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. 有模型学习和免模型学习有什么区别**\n",
    "\n",
    "有：奖励函数 / 价值函数 都是明确的，可以建立清晰的模型\n",
    "\n",
    "无：现实情况，无需模型。通过智能体与环境的交互来学习。需要较大的数据样本量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. 强化学习？**\n",
    "\n",
    "通过智能体与环境的不断交互，优化调整策略，以期达到最优奖励\n",
    "\n",
    "输入是一串序列，样本之间具有相关性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dailly38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
